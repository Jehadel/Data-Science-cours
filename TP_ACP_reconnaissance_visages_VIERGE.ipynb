{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Reconnaissance de visages avec l'Analyse en Composantes Principales (ACP)\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "Dans ce TP, vous allez :\n",
    "- Découvrir le problème de dimensionnalité en apprentissage automatique\n",
    "- Appliquer l'ACP pour réduire la dimension d'un jeu de données d'images\n",
    "- Interpréter les composantes principales comme des \"eigenfaces\"\n",
    "- Utiliser l'ACP pour la compression d'images\n",
    "- Intégrer l'ACP dans un pipeline de machine learning pour la reconnaissance de visages\n",
    "- Évaluer l'impact de différents prétraitements (scaling, balancing) sur les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliothèques et chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pour importer le dataset\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# pour la PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pour la catégorisation (machine learning)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# divers\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour des graphiques plus lisibles\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset LFW (Labeled Faces in the Wild)\n",
    "\n",
    "Le dataset LFW contient des photos de visages de personnalités publiques. Pour ce TP, nous allons filtrer pour ne garder que les personnes ayant au moins 70 photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "# min_faces_per_person=70 : on garde seulement les personnes avec au moins 70 photos\n",
    "# resize=0.4 : on réduit la taille des images pour accélérer les calculs\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "print(\"Dataset chargé avec succès !\")\n",
    "print(f\"Nombre total d'images : {lfw_people.images.shape[0]}\")\n",
    "print(f\"Taille de chaque image : {lfw_people.images.shape[1]} x {lfw_people.images.shape[2]} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration de la structure du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des données\n",
    "X = lfw_people.data  # Images aplaties en vecteurs\n",
    "y = lfw_people.target  # Labels (identité de la personne)\n",
    "target_names = lfw_people.target_names  # Noms des personnes\n",
    "n_samples, n_features = X.shape\n",
    "h, w = lfw_people.images.shape[1:]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STRUCTURE DU DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nombre d'observations (images) : {n_samples}\")\n",
    "print(f\"Nombre de features (pixels) par image : {n_features}\")\n",
    "print(f\"Dimensions d'une image : {h} x {w} = {h*w} pixels\")\n",
    "print(f\"\\nNombre de personnes différentes : {len(target_names)}\")\n",
    "print(f\"\\nNoms des personnes : {target_names}\")\n",
    "\n",
    "# Distribution des images par personne\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DISTRIBUTION DES IMAGES PAR PERSONNE\")\n",
    "print(\"=\" * 60)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for name, count in zip(target_names[unique], counts):\n",
    "    print(f\"{name:30s} : {count:3d} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage d'un échantillon d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Fonction utilitaire pour afficher une galerie d'images\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()\n",
    "\n",
    "# Sélection aléatoire d'images pour l'affichage\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(n_samples, 12, replace=False)\n",
    "sample_images = X[sample_indices]\n",
    "sample_titles = [target_names[y[i]] for i in sample_indices]\n",
    "\n",
    "print(\"Échantillon aléatoire de 12 visages du dataset :\")\n",
    "plot_gallery(sample_images, sample_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Le problème de dimensionnalité\n",
    "\n",
    "### Analyse du ratio observations/features\n",
    "\n",
    "En machine learning, il existe une règle empirique importante : **le nombre de features devrait être inférieur à la racine carrée du nombre d'observations** ($N_{features} < \\sqrt{N_{observations}}$).\n",
    "\n",
    "Cette règle permet d'éviter le **surapprentissage** (overfitting) et assure qu'on a suffisamment de données pour estimer correctement les paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANALYSE DE LA DIMENSIONNALITÉ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nombre d'observations : {n_samples}\")\n",
    "print(f\"Nombre de features : {n_features}\")\n",
    "print(f\"\\nRatio features/observations : {n_features/n_samples:.2f}\")\n",
    "print(f\"Racine carrée du nombre d'observations : {np.sqrt(n_samples):.2f}\")\n",
    "print(f\"\\nRègle empirique : N_features < sqrt(N_observations)\")\n",
    "print(f\"Vérification : {n_features} < {np.sqrt(n_samples):.2f} ?\")\n",
    "\n",
    "if n_features > np.sqrt(n_samples):\n",
    "    print(\"NON - Nous avons un problème de haute dimensionnalité !\")\n",
    "    print(f\"\\nLe ratio est de {n_features/np.sqrt(n_samples):.2f} fois trop élevé.\")\n",
    "    print(\"\\nSolution : Utiliser l'ACP pour réduire la dimensionnalité.\")\n",
    "else:\n",
    "    print(\"OUI - La dimensionnalité est acceptable.\")\n",
    "\n",
    "# Choix du nombre de composantes\n",
    "n_components_rule = int(np.sqrt(n_samples))  # Selon la règle empirique\n",
    "n_components_large = 150  # Valeur plus élevée pour comparaison\n",
    "\n",
    "print(f\"\\nSelon la règle empirique (sqrt(N_observations)) :\")\n",
    "print(f\"  Nombre maximal recommandé : {n_components_rule} composantes\")\n",
    "print(f\"\\nPour ce TP, nous allons comparer DEUX approches :\")\n",
    "print(f\"  1. ACP 'conservatrice' : {n_components_rule} composantes (respecte la règle)\")\n",
    "print(f\"  2. ACP 'large' : {n_components_large} composantes (pour plus d'information)\")\n",
    "print(f\"\\nCette comparaison nous aidera à comprendre le trade-off entre\")\n",
    "print(f\"respect de la règle et conservation de l'information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application de l'ACP pour réduire la dimensionnalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et ajustement des deux modèles PCA (instancier pca_rule et pca_large)\n",
    "# Ne pas oublier d’afficher la variance expliquée et comparer\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse des résultats de l'ACP\n",
    "\n",
    "### Nature des composantes principales\n",
    "\n",
    "Les composantes principales extraites de visages sont souvent appelées **\"eigenfaces\"**. Chaque composante représente un \"mode de variation\" dans l'ensemble des visages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INFORMATIONS SUR L'ACP (LARGE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Expliquer en quoi consiste les composantes principales ou eigenfaces : dimension, nature, type, etc.\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification avec une transformée inverse\n",
    "\n",
    "Nous allons :\n",
    "1. Choisir une image aléatoire\n",
    "2. La transformer dans l'espace ACP (réduction de dimension)\n",
    "3. Appliquer la transformée inverse pour reconstruire l'image\n",
    "4. Comparer l'image originale et les images reconstruites avec les deux ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection d'une image aléatoire (appelée original_image)\n",
    "\n",
    "# Transformation et reconstruction avec les DEUX ACP (appelées réspectivement :\n",
    "# - transformed_rule (image transformée par l’ACP conservative 35 paramétes) -> mettre suffixe _rule à toutes les variables liées à cette ACP\n",
    "# - reconstructed_rule (image reonconstruite - ACP conservative)\n",
    "# - transformed_large (image transformée par l’ACP large 150 paramétres) -> mettre suffixe _large à toutes les variables liées à cette ACP\n",
    "# - reconstructed_large (image reonconstruite - ACP large)\n",
    "\n",
    "# Affichage comparatif\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "# Différences\n",
    "# pour calculer la différence entre l’image originale et l’image reconstruite, on peut calculer :\n",
    "difference_rule = np.abs(original_image - reconstructed_rule.flatten())\n",
    "\n",
    "# afficher et comparer les différences originale/reconstruite pour les deux ACP\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "# Calcul des erreurs\n",
    "# En vous inspirant du calcul des différences, calculer les MSE : « la moyenne du carré des écarts » pour chaque ACP\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES RECONSTRUCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nACP {n_components_rule} composantes (règle empirique) :\")\n",
    "print(f\"  - Variance expliquée : {pca_rule.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"  - Erreur (MSE) : {mse_rule:.4f}\")\n",
    "print(f\"\\nACP {n_components_large} composantes (large) :\")\n",
    "print(f\"  - Variance expliquée : {pca_large.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"  - Erreur (MSE) : {mse_large:.4f}\")\n",
    "print(f\"\\nRéduction d'erreur avec {n_components_large} composantes : {(mse_rule - mse_large)/mse_rule*100:.1f}%\")\n",
    "print(\"\\nObservation : Plus de composantes = meilleure reconstruction mais plus de risque de surapprentissage !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction de l'échantillon initial d'images\n",
    "\n",
    "Comparons maintenant la qualité de reconstruction sur plusieurs images avec les deux ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction des images de l'échantillon initial de 12 images sample_images avec les DEUX ACP\n",
    "sample_transformed_rule = # VOTRE CODE\n",
    "sample_reconstructed_rule = # VOTRE CODE\n",
    "\n",
    "sample_transformed_large = # VOTRE CODE\n",
    "sample_reconstructed_large = # VOTRE CODE\n",
    "\n",
    "# Affichage comparatif : \n",
    "# Afficher sur une ligne l’échantillon initial de 12 images (original)\n",
    "# Afficher sur une ligne en dessous (milieu) les 12 images tranformées par l’ACP conservative\n",
    "# Afficher sur la dernière ligne (bas) les 12 images transformées par l’ACP large\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calcul des MSE moyennes\n",
    "mse_mean_rule = # VOTRE CODE\n",
    "mse_mean_large = # VOTRE CODE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MSE MOYENNE SUR L'ÉCHANTILLON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ACP {n_components_rule} composantes : {mse_mean_rule:.4f}\")\n",
    "print(f\"ACP {n_components_large} composantes : {mse_mean_large:.4f}\")\n",
    "print(f\"\\nAmélioration de la reconstruction : {(mse_mean_rule - mse_mean_large)/mse_mean_rule*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation des composantes principales (\"Eigenfaces\")\n",
    "\n",
    "### Image moyenne du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'image moyenne représente le \"visage moyen\" de tous les visages du dataset. vous pouvez y accéder par pca.mean_\n",
    "# bien sûr on n’oublie pas le .reshape()\n",
    "# Affichage comparatif des images moyenne de chaque ACP\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"L'image moyenne représente le 'visage typique' moyen de toutes les personnes du dataset.\")\n",
    "print(\"Les composantes principales capturent les variations autour de cette moyenne.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les 5 premières composantes principales\n",
    "\n",
    "Les premières composantes capturent les plus grandes variations dans les données (ex: éclairage, orientation du visage, expressions).\n",
    "\n",
    "**Comparaison des deux ACP :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des 5 premières eigenfaces des deux ACP\n",
    "# la liste des composantes d’une pca peut-être trouvée à pca.components_\n",
    "# afficher les 5 premières eigenfaces poru chaque ACP\n",
    "\n",
    "eigenfaces_5_first_rule = # VOTRE CODE\n",
    "eigenfaces_5_first_large = # VOTRE CODE\n",
    "\n",
    "# Création d'une figure comparative\n",
    "# afficher les 5 premières eigenfaces de ACP rule sur une ligne\n",
    "# afficher les 5 premières eigenfaces de ACP large sur une autre ligne\n",
    "# (pour comparer 2 à 2)\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VARIANCE EXPLIQUÉE PAR LES 5 PREMIÈRES COMPOSANTES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les 5 dernières composantes principales\n",
    "\n",
    "Les dernières composantes capturent des variations plus fines et du bruit par rapport aux premières composantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des 5 dernières eigenfaces des deux ACP (donc n°144, 145… pour l’une, et n°31, 32,… pour l’autre)\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "# Création d'une figure comparative\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VARIANCE EXPLIQUÉE PAR LES 5 DERNIÈRES COMPOSANTES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 composantes principales aléatoires de l’ACP à 150 composantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection aléatoire de 15 indices\n",
    "# sélectionner et afficher les 15 eigenfaces correspondantes, avec la variance expliquée\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compression d'images avec l'ACP\n",
    "\n",
    "### Synthèse des comparaisons précédentes\n",
    "\n",
    "Avant de passer à la compression, faisons un bilan des comparaisons entre les deux ACP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SYNTHÈSE : ACP {0} vs ACP {1} COMPOSANTES\".format(n_components_rule, n_components_large))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. RESPECT DE LA RÈGLE EMPIRIQUE :\")\n",
    "print(f\"   ACP {n_components_rule} : Respecte sqrt(N) = {np.sqrt(n_samples):.1f}\")\n",
    "print(f\"   ACP {n_components_large} : Dépasse la règle ({n_components_large/np.sqrt(n_samples):.2f}x)\")\n",
    "\n",
    "print(f\"\\n2. VARIANCE EXPLIQUÉE :\")\n",
    "print(f\"   ACP {n_components_rule} : {pca_rule.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"   ACP {n_components_large} : {pca_large.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"   Gain : +{(pca_large.explained_variance_ratio_.sum() - pca_rule.explained_variance_ratio_.sum())*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. QUALITÉ DE RECONSTRUCTION (MSE moyenne) :\")\n",
    "print(f\"   ACP {n_components_rule} : MSE plus élevée (moins précis)\")\n",
    "print(f\"   ACP {n_components_large} : MSE plus faible (plus précis)\")\n",
    "\n",
    "print(f\"\\n4. RISQUE DE SURAPPRENTISSAGE :\")\n",
    "print(f\"   ACP {n_components_rule} : Risque faible (suit la règle)\")\n",
    "print(f\"   ACP {n_components_large} : Risque potentiellement plus élevé\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nCONCLUSION INTERMÉDIAIRE :\")\n",
    "print(\"\\nNous avons vu deux approches :\")\n",
    "print(f\"  • ACP {n_components_rule} : CONSERVATRICE - respecte les règles, mais perd de l'info\")\n",
    "print(f\"  • ACP {n_components_large} : GOURMANDE - capture plus d'info, mais risque accru\")\n",
    "print(\"\\nQuelle approche est la meilleure ? Cela dépend de l'application !\")\n",
    "print(\"\\nC'est pourquoi la SECTION 6 va nous montrer comment OPTIMISER\")\n",
    "print(\"   le nombre de composantes de manière SYSTÉMATIQUE et DATA-DRIVEN.\")\n",
    "print(\"\\nNous allons chercher le 'sweet spot' qui maximise les performances\")\n",
    "print(\"tout en minimisant le risque de surapprentissage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction progressive d'une image\n",
    "\n",
    "Nous allons montrer comment une image peut être reconstruite progressivement en ajoutant de plus en plus de composantes principales. C'est le principe de la **compression d'images**. Pour cela nous allons refaire des ACP avec un nombre croissants de composantes à tester (de 1 à 200) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection d'une image pour la démonstration\n",
    "# On prend la première image de notre échantillon\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "# Liste des nombres de composantes à tester\n",
    "n_components_list = [1, 2, 10, 20, 30, 40, 50, 75, 100, 150, 200]\n",
    "\n",
    "# Création de la figure\n",
    "\n",
    "\n",
    "# Affichage de l'image originale\n",
    "\n",
    "\n",
    "# Reconstruction avec différents nombres de composantes\n",
    "for idx, n_comp in enumerate(n_components_list):\n",
    "    # Créer une PCA temporaire avec n_comp composantes\n",
    "\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Transformer et reconstruire\n",
    "    \n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Calcul du taux de compression et de la variance expliquée\n",
    "    \n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Affichage\n",
    "    \n",
    "    # VOTRE CODE\n",
    "\n",
    "# Masquer les axes non utilisés\n",
    "for idx in range(len(n_components_list) + 1, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Reconstruction progressive d'une image avec un nombre croissant de composantes principales\",\n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations :\")\n",
    "print(\"- Avec 1-2 composantes : vos observations \") # compléter\n",
    "print(\"- Avec 10-20 composantes : vos observations \") # compléter\n",
    "print(\"- Avec 50+ composantes : vos observations \") # compléter \n",
    "print(\"- Avec 150+ composantes : vos observations\") # compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choix du nombre optimal de composantes\n",
    "\n",
    "Dans les sections précédentes, nous avons comparé deux approches :\n",
    "- **ACP conservatrice** avec {n_components_rule} composantes (selon la règle empirique)\n",
    "- **ACP large** avec {n_components} composantes (pour plus d'information)\n",
    "\n",
    "Mais comment choisir le **nombre optimal** de composantes de manière rigoureuse ?\n",
    "\n",
    "### Courbe de variance expliquée cumulée\n",
    "\n",
    "Cette courbe nous aide à déterminer combien de composantes sont nécessaires pour capturer une certaine proportion de la variance totale. C'est un outil essentiel pour prendre une décision **data-driven** plutôt que de se fier uniquement aux règles empiriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la variance expliquée cumulée\n",
    "cumulative_variance = # VOTRE CODE\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, n_components_large + 1), cumulative_variance, 'b-', linewidth=2)\n",
    "plt.xlabel('Nombre de composantes principales', fontsize=12)\n",
    "plt.ylabel('Variance expliquée cumulée', fontsize=12)\n",
    "plt.title('Variance expliquée cumulée en fonction du nombre de composantes principales', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout de lignes horizontales pour différents seuils\n",
    "thresholds = [0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue']\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    plt.axhline(y=threshold, color=color, linestyle='--', alpha=0.5, \n",
    "                label=f'{threshold:.0%} variance')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0, n_components_large)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance totale expliquée par {n_components_large} composantes : {cumulative_variance[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de composantes nécessaires pour différents seuils de variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components_for_variance(cumulative_variance, threshold):\n",
    "    \"\"\"Trouve le nombre minimum de composantes pour atteindre un seuil de variance\"\"\"\n",
    "    # Vérifier si le seuil est atteignable\n",
    "    if threshold > cumulative_variance[-1]:\n",
    "        # Retourner le nombre total de composantes si le seuil est trop élevé\n",
    "        return len(cumulative_variance)\n",
    "    return np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOMBRE DE COMPOSANTES NÉCESSAIRES POUR DIFFÉRENTS SEUILS DE VARIANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "variance_thresholds = [0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "results = []\n",
    "\n",
    "for threshold in variance_thresholds:\n",
    "    n_comp_needed = find_n_components_for_variance(cumulative_variance, threshold)\n",
    "    variance_reached = cumulative_variance[n_comp_needed - 1]\n",
    "    compression = n_features / n_comp_needed\n",
    "    results.append((threshold, n_comp_needed, variance_reached, compression))\n",
    "    \n",
    "    print(f\"\\n{threshold:.0%} de variance :\")\n",
    "    print(f\"  → {n_comp_needed} composantes nécessaires\")\n",
    "    print(f\"  → Variance réellement atteinte : {variance_reached:.2%}\")\n",
    "    \n",
    "    # Indiquer si le seuil n'a pas été atteint\n",
    "    if variance_reached < threshold:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!! Seuil non atteint ! Maximum disponible : {cumulative_variance[-1]:.2%}\")\n",
    "    \n",
    "    print(f\"  → Taux de compression : {compression:.1f}x\")\n",
    "    print(f\"  → Réduction de dimensionnalité : {n_features} → {n_comp_needed} \"\n",
    "          f\"(-{100*(1-n_comp_needed/n_features):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nInterprétation :\")\n",
    "print(\"- Plus on veut conserver de variance, plus il faut de composantes\")\n",
    "print(\"- Un bon compromis se situe souvent entre 80% et 95% de variance\")\n",
    "print(\"- Au-delà de 95%, on ajoute surtout du bruit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Introduction au Machine Learning : Classification de visages\n",
    "\n",
    "### Préparation des données\n",
    "\n",
    "Nous allons maintenant utiliser l'ACP comme étape de prétraitement pour un problème de **classification supervisée** : reconnaître l'identité d'une personne à partir de son visage.\n",
    "\n",
    "Nous reprendrons ce code en détail en cours, pour le moment contentez vous d’exécuter les cellules, et essayer de comprendre ce qu’il se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PRÉPARATION DES ENSEMBLES D'ENTRAÎNEMENT ET DE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Division en ensembles d'entraînement (80%) et de test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nEnsemble d'entraînement : {X_train.shape[0]} images\")\n",
    "print(f\"Ensemble de test : {X_test.shape[0]} images\")\n",
    "print(f\"\\nNombre de features : {X_train.shape[1]}\")\n",
    "print(f\"Nombre de classes (personnes) : {len(np.unique(y))}\")\n",
    "\n",
    "# Vérification de la distribution des classes\n",
    "print(\"\\nDistribution des classes dans l'ensemble de test :\")\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "for name, count in zip(target_names[unique_test], counts_test):\n",
    "    print(f\"  {name:30s} : {count:2d} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement sur les images originales (sans ACP)\n",
    "\n",
    "Commençons par entraîner un modèle directement sur les images originales pour avoir une référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ENTRAÎNEMENT SUR LES IMAGES ORIGINALES (SANS ACP)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Création et entraînement du modèle SVM\n",
    "print(\"\\nEntraînement en cours...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clf_original = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "clf_original.fit(X_train, y_train)\n",
    "\n",
    "train_time_original = time.time() - start_time\n",
    "\n",
    "# Prédiction et évaluation\n",
    "y_pred_original = clf_original.predict(X_test)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "print(f\"✓ Entraînement terminé en {train_time_original:.2f} secondes\")\n",
    "print(f\"\\nPerformances sur l'ensemble de test :\")\n",
    "print(f\"  Accuracy : {accuracy_original:.2%}\")\n",
    "print(f\"\\nRapport de classification détaillé :\")\n",
    "print(classification_report(y_test, y_pred_original, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement sur les images transformées par ACP\n",
    "\n",
    "Maintenant, appliquons l'ACP avant l'entraînement pour réduire la dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(f\"ENTRAÎNEMENT AVEC ACP ({n_components_large} COMPOSANTES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Application de l'ACP\n",
    "print(\"\\nApplication de l'ACP...\")\n",
    "pca_ml = PCA(n_components=n_components_large, whiten=False, svd_solver='randomized')\n",
    "X_train_pca = pca_ml.fit_transform(X_train)\n",
    "X_test_pca = pca_ml.transform(X_test)\n",
    "\n",
    "print(f\"Dimensions après ACP :\")\n",
    "print(f\"  Train : {X_train_pca.shape}\")\n",
    "print(f\"  Test : {X_test_pca.shape}\")\n",
    "print(f\"  Variance expliquée : {pca_ml.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Entraînement du modèle\n",
    "print(\"\\nEntraînement en cours...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clf_pca = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "train_time_pca = time.time() - start_time\n",
    "\n",
    "# Prédiction et évaluation\n",
    "y_pred_pca = clf_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"✓ Entraînement terminé en {train_time_pca:.2f} secondes\")\n",
    "print(f\"\\nPerformances sur l'ensemble de test :\")\n",
    "print(f\"  Accuracy : {accuracy_pca:.2%}\")\n",
    "print(f\"\\nRapport de classification détaillé :\")\n",
    "print(classification_report(y_test, y_pred_pca, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison : Avec vs Sans ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON : AVEC VS SANS ACP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Métrique':<30} {'Sans ACP':>15} {'Avec ACP':>15} {'Différence':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"{'Nombre de features':<30} {X_train.shape[1]:>15} {X_train_pca.shape[1]:>15} \"\n",
    "      f\"{X_train_pca.shape[1] - X_train.shape[1]:>15}\")\n",
    "\n",
    "print(f\"{'Temps d’entraînement (s)':<30} {train_time_original:>15.2f} {train_time_pca:>15.2f} \" \n",
    "      f\"{train_time_pca - train_time_original:>15.2f}\")\n",
    "\n",
    "speedup = train_time_original / train_time_pca\n",
    "print(f\"{'Accélération':<30} {'':<15} {f'{speedup:.2f}x':>15} {'':>15}\")\n",
    "\n",
    "print(f\"{'Accuracy':<30} {accuracy_original:>14.2%} {accuracy_pca:>14.2%} \"\n",
    "      f\"{accuracy_pca - accuracy_original:>+14.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nObservations :\")\n",
    "print(f\"- Réduction de dimensionnalité : {n_features} → {n_components_large} \"\n",
    "      f\"({100*(1-n_components_large/n_features):.1f}% de réduction)\")\n",
    "print(f\"- L'entraînement est {speedup:.1f}x plus rapide avec l'ACP\")\n",
    "\n",
    "if accuracy_pca > accuracy_original:\n",
    "    print(f\"- L'accuracy est MEILLEURE avec l'ACP (+{100*(accuracy_pca-accuracy_original):.2f}%)\")\n",
    "    print(\"- L'ACP a permis d'éliminer du bruit et d'améliorer la généralisation\")\n",
    "elif accuracy_pca < accuracy_original:\n",
    "    print(f\"- L'accuracy est légèrement INFÉRIEURE avec l'ACP ({100*(accuracy_pca-accuracy_original):.2f}%)\")\n",
    "    print(\"- Un compromis acceptable pour un gain important en vitesse\")\n",
    "else:\n",
    "    print(\"- L'accuracy est IDENTIQUE avec et sans ACP\")\n",
    "    print(\"- L'ACP n'a pas fait perdre d'information utile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimisation du nombre de composantes principales\n",
    "\n",
    "### Recherche du nombre optimal de composantes\n",
    "\n",
    "Testons différents nombres de composantes pour trouver le meilleur compromis entre vitesse et performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RECHERCHE DU NOMBRE OPTIMAL DE COMPOSANTES PRINCIPALES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Différentes valeurs à tester\n",
    "n_components_to_test = [10, 25, 50, 75, 100, 125, 150, 175, 200]\n",
    "\n",
    "results_optimization = []\n",
    "\n",
    "print(\"\\nEntraînement en cours pour différentes valeurs de n_components...\\n\")\n",
    "\n",
    "for n_comp in n_components_to_test:\n",
    "    # ACP\n",
    "    pca_temp = PCA(n_components=n_comp, whiten=False, svd_solver='randomized')\n",
    "    X_train_temp = pca_temp.fit_transform(X_train)\n",
    "    X_test_temp = pca_temp.transform(X_test)\n",
    "    \n",
    "    # Entraînement\n",
    "    start = time.time()\n",
    "    clf_temp = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "    clf_temp.fit(X_train_temp, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Évaluation\n",
    "    y_pred_temp = clf_temp.predict(X_test_temp)\n",
    "    acc = accuracy_score(y_test, y_pred_temp)\n",
    "    var_explained = pca_temp.explained_variance_ratio_.sum()\n",
    "    \n",
    "    results_optimization.append({\n",
    "        'n_components': n_comp,\n",
    "        'accuracy': acc,\n",
    "        'train_time': train_time,\n",
    "        'variance_explained': var_explained\n",
    "    })\n",
    "    \n",
    "    print(f\"n_components={n_comp:3d} | Accuracy={acc:.2%} | \"\n",
    "          f\"Time={train_time:.2f}s | Variance={var_explained:.2%}\")\n",
    "\n",
    "print(\"\\n✓ Optimisation terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des résultats d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des résultats\n",
    "n_comps = [r['n_components'] for r in results_optimization]\n",
    "accuracies = [r['accuracy'] for r in results_optimization]\n",
    "train_times = [r['train_time'] for r in results_optimization]\n",
    "variances = [r['variance_explained'] for r in results_optimization]\n",
    "\n",
    "# Création des graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Accuracy vs n_components\n",
    "axes[0, 0].plot(n_comps, accuracies, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "axes[0, 0].set_xlabel('Nombre de composantes principales', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Performance du modèle', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "best_idx = np.argmax(accuracies)\n",
    "axes[0, 0].axvline(x=n_comps[best_idx], color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].annotate(f'Max: {accuracies[best_idx]:.2%}\\n@ {n_comps[best_idx]} comp.',\n",
    "                    xy=(n_comps[best_idx], accuracies[best_idx]),\n",
    "                    xytext=(10, -20), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# 2. Train time vs n_components\n",
    "axes[0, 1].plot(n_comps, train_times, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Nombre de composantes principales', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Temps d\\'entraînement (s)', fontsize=11)\n",
    "axes[0, 1].set_title('Temps d\\'entraînement', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Variance expliquée vs n_components\n",
    "axes[1, 0].plot(n_comps, variances, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('Nombre de composantes principales', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Variance expliquée', fontsize=11)\n",
    "axes[1, 0].set_title('Variance capturée', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90%')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Trade-off Accuracy vs Train time\n",
    "scatter = axes[1, 1].scatter(train_times, accuracies, c=n_comps, s=200, \n",
    "                             cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "axes[1, 1].set_xlabel('Temps d\\'entraînement (s)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1, 1].set_title('Trade-off Performance vs Vitesse', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=axes[1, 1])\n",
    "cbar.set_label('Nombre de composantes', fontsize=10)\n",
    "\n",
    "# Annotation du point optimal\n",
    "axes[1, 1].scatter([train_times[best_idx]], [accuracies[best_idx]], \n",
    "                   s=300, facecolors='none', edgecolors='red', linewidths=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMeilleur résultat :\")\n",
    "print(f\"  - {n_comps[best_idx]} composantes principales\")\n",
    "print(f\"  - Accuracy : {accuracies[best_idx]:.2%}\")\n",
    "print(f\"  - Temps d'entraînement : {train_times[best_idx]:.2f}s\")\n",
    "print(f\"  p Variance expliquée : {variances[best_idx]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Impact du scaling et du balancing\n",
    "\n",
    "### Effet du scaling (standardisation)\n",
    "\n",
    "Le **scaling** consiste à centrer et réduire les données (moyenne=0, écart-type=1). C'est important pour de nombreux algorithmes de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EFFET DU SCALING SUR LES PERFORMANCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sans scaling (déjà fait précédemment)\n",
    "print(\"\\nRésultats SANS scaling (référence) :\")\n",
    "print(f\"  Accuracy : {accuracy_pca:.2%}\")\n",
    "\n",
    "# Avec scaling\n",
    "print(\"\\nEntraînement AVEC scaling...\")\n",
    "\n",
    "# Standardisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ACP sur données standardisées\n",
    "pca_scaled = PCA(n_components=n_components_large, whiten=False, svd_solver='randomized')\n",
    "X_train_pca_scaled = pca_scaled.fit_transform(X_train_scaled)\n",
    "X_test_pca_scaled = pca_scaled.transform(X_test_scaled)\n",
    "\n",
    "# Entraînement\n",
    "start_time = time.time()\n",
    "clf_scaled = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "clf_scaled.fit(X_train_pca_scaled, y_train)\n",
    "train_time_scaled = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "y_pred_scaled = clf_scaled.predict(X_test_pca_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"Entraînement terminé en {train_time_scaled:.2f} secondes\")\n",
    "print(f\"\\nRésultats AVEC scaling :\")\n",
    "print(f\"  Accuracy : {accuracy_scaled:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"COMPARAISON :\")\n",
    "print(f\"  Différence d'accuracy : {accuracy_scaled - accuracy_pca:+.2%}\")\n",
    "print(f\"  Différence de temps : {train_time_scaled - train_time_pca:+.2f}s\")\n",
    "\n",
    "if accuracy_scaled > accuracy_pca:\n",
    "    print(\"\\nLe scaling AMÉLIORE les performances !\")\n",
    "elif accuracy_scaled < accuracy_pca:\n",
    "    print(\"\\nLe scaling DIMINUE légèrement les performances.\")\n",
    "else:\n",
    "    print(\"\\nLe scaling n'a PAS d'effet significatif.\")\n",
    "\n",
    "print(\"\\nRapport de classification avec scaling :\")\n",
    "print(classification_report(y_test, y_pred_scaled, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effet du balancing (équilibrage des classes)\n",
    "\n",
    "Le **balancing** vise à donner le même poids à chaque classe lors de l'entraînement, ce qui est utile quand certaines classes sont sous-représentées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ANALYSE DU DÉSÉQUILIBRE DES CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyse de la distribution\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"\\nDistribution des classes dans l'ensemble d'entraînement :\")\n",
    "for name, count in zip(target_names[unique_train], counts_train):\n",
    "    print(f\"  {name:30s} : {count:3d} images ({100*count/len(y_train):.1f}%)\")\n",
    "\n",
    "# Calcul du déséquilibre\n",
    "max_count = counts_train.max()\n",
    "min_count = counts_train.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\nRatio de déséquilibre : {imbalance_ratio:.2f}\")\n",
    "print(f\"Classe la plus représentée : {counts_train.max()} images\")\n",
    "print(f\"Classe la moins représentée : {counts_train.min()} images\")\n",
    "\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"\\nDéséquilibre significatif détecté ! Le balancing peut aider.\")\n",
    "else:\n",
    "    print(\"\\nDistribution relativement équilibrée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EFFET DU BALANCING SUR LES PERFORMANCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Avec balancing (class_weight='balanced')\n",
    "print(\"\\nEntraînement AVEC balancing...\")\n",
    "\n",
    "start_time = time.time()\n",
    "clf_balanced = SVC(kernel='rbf', gamma='scale', C=1.0, \n",
    "                   class_weight='balanced', random_state=42)\n",
    "clf_balanced.fit(X_train_pca_scaled, y_train)\n",
    "train_time_balanced = time.time() - start_time\n",
    "\n",
    "# Évaluation\n",
    "y_pred_balanced = clf_balanced.predict(X_test_pca_scaled)\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "\n",
    "print(f\"Entraînement terminé en {train_time_balanced:.2f} secondes\")\n",
    "print(f\"\\nRésultats AVEC balancing :\")\n",
    "print(f\"  Accuracy : {accuracy_balanced:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"COMPARAISON :\")\n",
    "print(f\"  Sans balancing : {accuracy_scaled:.2%}\")\n",
    "print(f\"  Avec balancing : {accuracy_balanced:.2%}\")\n",
    "print(f\"  Différence : {accuracy_balanced - accuracy_scaled:+.2%}\")\n",
    "\n",
    "if accuracy_balanced > accuracy_scaled:\n",
    "    print(\"\\nLe balancing AMÉLIORE les performances !\")\n",
    "    print(\"   Les classes minoritaires sont mieux reconnues.\")\n",
    "elif accuracy_balanced < accuracy_scaled:\n",
    "    print(\"\\nLe balancing DIMINUE légèrement les performances globales.\")\n",
    "    print(\"   Mais il peut améliorer les performances sur les classes minoritaires.\")\n",
    "else:\n",
    "    print(\"\\nLe balancing n'a PAS d'effet significatif sur l'accuracy globale.\")\n",
    "\n",
    "print(\"\\nRapport de classification avec balancing :\")\n",
    "print(classification_report(y_test, y_pred_balanced, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison finale de toutes les configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TABLEAU RÉCAPITULATIF DE TOUTES LES CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "configurations = [\n",
    "    ('Sans ACP (référence)', accuracy_original, train_time_original, n_features),\n",
    "    (f'Avec ACP ({n_components_large} comp.)', accuracy_pca, train_time_pca, n_components_large),\n",
    "    ('ACP + Scaling', accuracy_scaled, train_time_scaled, n_components_large),\n",
    "    ('ACP + Scaling + Balancing', accuracy_balanced, train_time_balanced, n_components_large),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Configuration':<30} {'Accuracy':>12} {'Temps (s)':>12} {'Features':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for config_name, acc, time_val, n_feat in configurations:\n",
    "    print(f\"{config_name:<30} {acc:>11.2%} {time_val:>11.2f} {n_feat:>12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Visualisation graphique\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "config_names = [c[0] for c in configurations]\n",
    "accuracies_all = [c[1] for c in configurations]\n",
    "times_all = [c[2] for c in configurations]\n",
    "\n",
    "# Graphique des accuracies\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "bars1 = ax1.bar(range(len(config_names)), accuracies_all, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(config_names)))\n",
    "ax1.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Comparaison des performances (Accuracy)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim([min(accuracies_all) * 0.95, max(accuracies_all) * 1.02])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar, acc in zip(bars1, accuracies_all):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.2%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Graphique des temps\n",
    "bars2 = ax2.bar(range(len(config_names)), times_all, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(config_names)))\n",
    "ax2.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Temps d\\'entraînement (s)', fontsize=12)\n",
    "ax2.set_title('Comparaison des temps d\\'entraînement', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar, time_val in zip(bars2, times_all):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusions\n",
    "best_config_idx = np.argmax(accuracies_all)\n",
    "fastest_config_idx = np.argmin(times_all)\n",
    "\n",
    "print(\"\\nCONCLUSIONS :\")\n",
    "print(f\"\\n1. Meilleure configuration (accuracy) : {config_names[best_config_idx]}\")\n",
    "print(f\"   Accuracy : {accuracies_all[best_config_idx]:.2%}\")\n",
    "print(f\"   Temps : {times_all[best_config_idx]:.2f}s\")\n",
    "\n",
    "print(f\"\\n2. Configuration la plus rapide : {config_names[fastest_config_idx]}\")\n",
    "print(f\"   Temps : {times_all[fastest_config_idx]:.2f}s\")\n",
    "print(f\"   Accuracy : {accuracies_all[fastest_config_idx]:.2%}\")\n",
    "\n",
    "speedup_final = times_all[0] / times_all[1]\n",
    "print(f\"\\n3. Gain de vitesse avec l'ACP : {speedup_final:.1f}x plus rapide\")\n",
    "\n",
    "print(\"\\n4. Impact des prétraitements :\")\n",
    "print(f\"   - Scaling : {accuracies_all[2] - accuracies_all[1]:+.2%}\")\n",
    "print(f\"   - Balancing : {accuracies_all[3] - accuracies_all[2]:+.2%}\")\n",
    "print(f\"   - Total (ACP+Scaling+Balancing) : {accuracies_all[3] - accuracies_all[0]:+.2%} vs référence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion du TP\n",
    "\n",
    "### Ce que nous avons appris :\n",
    "\n",
    "1. **Problème de dimensionnalité** : Trop de features par rapport au nombre d'observations peut nuire aux performances et ralentir l'entraînement.\n",
    "\n",
    "2. **L'ACP comme solution** : La réduction de dimensionnalité via ACP permet de :\n",
    "   - Accélérer significativement l'entraînement\n",
    "   - Réduire le surapprentissage\n",
    "   - Parfois même améliorer les performances\n",
    "\n",
    "3. **Eigenfaces** : Les composantes principales représentent des \"visages types\" qui capturent les variations principales dans les données.\n",
    "\n",
    "4. **Compression d'images** : L'ACP peut être utilisée pour compresser des images en ne gardant que les composantes les plus importantes.\n",
    "\n",
    "5. **Optimisation** : Le choix du nombre de composantes est un compromis entre :\n",
    "   - La quantité d'information conservée (variance expliquée)\n",
    "   - La vitesse de calcul\n",
    "   - Les performances du modèle\n",
    "\n",
    "6. **Prétraitements** :\n",
    "   - Le **scaling** peut améliorer les performances en normalisant les données\n",
    "   - Le **balancing** aide quand les classes sont déséquilibrées\n",
    "\n",
    "### Points clés à retenir :\n",
    "\n",
    "- L'ACP est une technique de **réduction de dimensionnalité linéaire**\n",
    "- Elle trouve les directions de variance maximale dans les données\n",
    "- C'est un outil puissant pour la **visualisation**, la **compression** et l'**accélération** des algorithmes de ML\n",
    "- Elle doit être appliquée sur l'ensemble d'entraînement et les mêmes transformations appliquées au test\n",
    "- Le nombre de composantes est un hyperparamètre à optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
