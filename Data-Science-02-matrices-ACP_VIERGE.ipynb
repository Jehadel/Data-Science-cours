{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a0f2c4-2b01-47cc-9705-b3f5871664c0",
   "metadata": {},
   "source": [
    "# Data Science 2 : suite rappel d’algèbre linéaire (matrices) et illustration avec la réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b961dcc-7353-472c-b8da-a6db3dd824f9",
   "metadata": {},
   "source": [
    "Enseignant : Jean Delpech\n",
    "\n",
    "Cours : Data Science\n",
    "\n",
    "Classe : M1 Data/IA\n",
    "\n",
    "Année scolaire : 2025/2026\n",
    "\n",
    "Dernière mise à jour : novembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d32a51-bbf4-4771-b51f-9a688932e0ae",
   "metadata": {},
   "source": [
    "Dans le chapitre précédent nous avons vu les notions élémentaires de l’algèbre linéaire : tout d’abord une présentation historique pour bien prendre conscience de la raison d’être de cette discipline, quels problèmes elle propose de résoudre et quelle perspective elle propose d’adopter, pour ensuite découvrir les notions de vecteurs, de scalaire, d’espace vectoriel, de norme, de barycentre, de variance et d’inertie. Ces outils nous ont permis d’élaborer un algorithm de clustering : k-means.\n",
    "\n",
    "Dans ce second chapitre nous découvrirons les notions de matrices, leurs types, les différentes opérations matricielles, les transformations linéaires, les projections et changement de base, les matrices de covariances, les valeurs propres, etc. \n",
    "\n",
    "Ces notions nous permettrons de nous attaquer à un problème essentiel pour le data scientist : la réduction de dimension. En effet il n’est pas rare en data science de devoir gérer des données avec de très nombreuses features, illisibles pour un humain et inefficace à exploiter dans leur totalité. On va donc chercher à « regrouper » ensemble les observations (clustering) selon certains critères, mais aussi les features, afin de ne retenir que les dimensions qui apportent de l’information par rapport au problème posé. C’est une technique que l’on va mettre en œuvre dans tous les domaines de la science des données (analyse, machine learning, deep learning). \n",
    "\n",
    "Après avoir vu les notions d’algèbre linéaire adéquates, nous implémenterons une technique de réduction de dimension : l’analyse en composante principale.\n",
    "\n",
    "Vous apprendrez (ou réviserez) :\n",
    "- la définition des matrices et les différents types de matrices\n",
    "- la définition des opérations sur les matrices\n",
    "- comment les matrices permettent de réaliser des transformatiosn linéaires (application géométrique : rotation, symétrie, etc.)\n",
    "- la notion de base et de changement de base, de projection\n",
    "- la matrice de variance/covariance\n",
    "- les vecteurs et les valeurs propres\n",
    "- l’algorithme ACP (ou PCA) pour réduire les dimensions\n",
    "- les limites de cet algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7891e-ed44-4099-824e-c32c580f3eb7",
   "metadata": {},
   "source": [
    "## Matrices et opérations matricielles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd5f11-d4ee-4058-a780-5f669db3e4ab",
   "metadata": {},
   "source": [
    "### Définition et notation\n",
    "\n",
    "L’approche la plus directe (pour ne pas dire « naïve ») de ce qu’est une **matrice** est de la définir comme un tableau rectangulaire de nombres organisés en lignes et colonnes. Par exemple :\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} \n",
    "    2.256 & 3.56 & 7.65 \\\\ \n",
    "    4.0 & 25.45 & 0.658 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Nous avons là une matrice M de nombres rééls à 2 lignes et 3 colonne, ou « matrice 2×3 »\n",
    "\n",
    "Une matrice $A$ de dimension $m \\times n$ (m lignes, n colonnes) s'écrit :\n",
    "\n",
    "$$A \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "Les éléments sont notés $A[i,j]$ ou $a_{ij}$ (élément à la ligne i, colonne j).\n",
    "\n",
    "Ainsi dans notre exemple, l’élément M[2,3] est le nombre 0,658.\n",
    "\n",
    "Par convention on va toujours faire référence à la ligne en premier, puis la colonne. Je ne le répéterai jamais assez :\n",
    "\n",
    "ligne, puis colonne\n",
    "ligne, puis colonne\n",
    "ligne, colonne…\n",
    "\n",
    "Mémorisez bien cette convention elle permettra de ne pas vous perdre quand on définira certaines opérations avec des matrices (comme la multiplication), car l’ordre dans lequel on va parcourir la matrice aura une importance cruciale.\n",
    "\n",
    "#### Lien avec les données\n",
    "\n",
    "En data science, un dataset est naturellement représenté par une matrice :\n",
    "- **Lignes** : enregistrement/observations/échantillons/individus\n",
    "- **Colonnes** : variables/features/caractéristiques\n",
    "\n",
    "**Exemple** : Dataset de clients :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d37cb-4ffa-4ccd-8c18-f410920a7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset comme matrice\n",
    "# Lignes = clients, Colonnes = [âge, revenu, nb_achats]\n",
    "X = np.array([\n",
    "    [25, 50000, 3],  # client 1\n",
    "    [34, 65000, 5],  # client 2\n",
    "    [28, 48000, 2],  # client 3\n",
    "    [45, 80000, 8],  # client 4\n",
    "    [23, 42000, 1],  # client 5\n",
    "])\n",
    "\n",
    "print(\"Matrice de données X :\")\n",
    "print(X)\n",
    "print(\"\\nLignes = clients, Colonnes = [âge, revenu, nb_achats]\")\n",
    "print(f\"Dimension : {X.shape[0]} clients × {X.shape[1]} features\")\n",
    "print(f\"X ∈ ℝ^{X.shape[0]}×{X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115b37e-57cc-457f-b012-31495de932f7",
   "metadata": {},
   "source": [
    "Nous allons voir dans la suite de ce cours que les matrices sont des outils bien plus sophistiquées que cela, qui peuvent modéliser un bon nombre de problèmes mathématiques et avoir bien des applications. Nous verrons notamment la place qu’elles occupent en algèbre linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6c220-25fb-4c83-a73a-222e90403ba7",
   "metadata": {},
   "source": [
    "#### Taxonomie des matrices\n",
    "\n",
    "Les matrices peuvent présenter une grande variété de motifs (*patterns*), ces formes vont refléter les propriétés mathématiques des objets, opérations ou autres problèmes qu’elles modélisent. Passons donc ces formes en revue !\n",
    "\n",
    "#### Matrice ligne (vecteur ligne)\n",
    "\n",
    "Une matrice peut très bien n’avoir qu’une seule ligne, dans ce cas le nombre d’éléments de la matrice sera égal au nombre de colonnes. On parlera aussi de vecteur ligne, par exemple pour représenter un enregistrement de *n* features dans un dataset.\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} 34.256 & 2.88 & 6.0 & -34.5761 & 2.546 & -79.8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dimension $1 \\times n$ : une seule ligne\n",
    "\n",
    "En fait on peut voir une matrice *n* × *m* comme l’empilement de *n* vecteurs lignes de dimension *m*\n",
    "\n",
    "#### Matrice colonne (vecteur colonne)\n",
    "\n",
    "De la même manière que précédemment, une matrice peut se limiter à une seule colonne. On parlera aussi de vecteur colonne, par exemple pour représenter toutes les valeurs d’une features pour chaque enregistrement (p. ex. : `pandas.Series.`)\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} 34.256 \\\\ \n",
    "                    2.88 \\\\\n",
    "                    6.0 \\\\\n",
    "                    -34.5761 \\\\\n",
    "                    2.546 \\\\\n",
    "                    79.8 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dimension $n \\times 1$ : une seule colonne\n",
    "\n",
    "Dans ce cas on pourrait voir une matrice *n* × *m* comme une succession de *m* vecteurs colonnes de dimension *n*.\n",
    "\n",
    "La disposition en ligne ou en colonne n’est pas qu’une simple fantaisie esthétique, nous verrons plus loin qu’elle peut avoir de graves conséquences lorsque l’on commencera à réaliser des opérations avec ces matrices, dont le résultat dépendra grandement de la manière dont sont disposés les éléments dans une matrice. \n",
    "\n",
    "#### Matrice nulle\n",
    "\n",
    "C’est une matrice dont tous les éléments sont nuls :\n",
    "\n",
    "$$\n",
    "0_{2, 3} = \\begin{bmatrix} 0 & 0 & 0 \\\\\n",
    "                            0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "ou encore :\n",
    "\n",
    "$$\n",
    "0_{2, 2} = \\begin{bmatrix} 0 & 0 \\\\\n",
    "                            0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notez la notation qui fait référence à ses dimensions.\n",
    "\n",
    "La matrice nulle est un élément neutre (ou élément identité – à ne pas confondre avec la *matrice* identité que nous verrons ci-dessous) pour certaines opérations sur les matrices. \n",
    "\n",
    "#### Matrice carrée\n",
    "\n",
    "La plupart des matrices sont rectangulaires (le nombre de lignes et de colonnes sont différents). Mais lorsque le nombre de lignes et de colonnes est identique, on parlera de matrice carré.\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} 34.256 & 2.88 & 6.0 \\\\\n",
    "                    -34.5761 & 2.546 & 79.8 \\\\\n",
    "                    10.342 & -0.257 & -3.82\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Dimension $n \\times n$ : même nombre de lignes et de colonnes\n",
    "\n",
    "Ces matrices possèdent des propriétés particulières (certaines opérations leurs seront spécifiques)\n",
    "\n",
    "#### Matrice identité $I_n$\n",
    "La matrice identité est une matrice carrée très particulière, avec des 1 sur la diagonale et des 0 ailleurs\n",
    "\n",
    "$$I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "Notez la notation qui fait référence à ses dimensions.\n",
    "\n",
    "Comme son nom le suggère, elle va jouer le rôle d’élément identité (ou élément neutre) pour certaines opérations sur les matrices.\n",
    "\n",
    "#### Matrice diagonale\n",
    "Matrice carrée avec des valeurs non nulles uniquement sur la diagonale (seuls les éléments dont les indices ligne et colonnes sont identiques sont non nuls) :\n",
    "\n",
    "$$\n",
    "M= \\begin{bmatrix} a_{1,1} & 0 & 0 \\\\\n",
    "                    0 & a_{2,2} & 0 \\\\\n",
    "                    0 & 0 & a_{3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ce motif est l’un des plus simples que l’on puisse rencontrer, et il a plusiers propriétés intéressantes :\n",
    "* c’est une matrice qui contient un grand nombre de zéro, et on sait exactement où ils sont. En informatique, il est inutile de représenter ces matrices dans leur totalité en mémoire (on peut juste retenir les éléments de la diagonale), ce qui fait gagner beaucoup de ressource. Ce principe se retrouve dans les *sparse matrixes*\n",
    "* Certaines opérations, comme l’inversion, sont très simples : il suffit d’inverser tous les éléments de la diagonale.\n",
    "$$\n",
    "M^{-1}= \\begin{bmatrix} 1/a_{1,1} & 0 & 0 \\\\\n",
    "                    0 & 1/a_{2,2} & 0 \\\\\n",
    "                    0 & 0 & 1/a_{3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  C’est beaucoup, beaucoup, beaucoup plus compliqué d’inverser une matrice quelconque.\n",
    "\n",
    "#### Matrice triangulaire\n",
    "\n",
    "Ce sont des matrices où toutes les valeurs au dessus (diagonale supérieure) ou en dessous (diagonale inférieure) de la diagonale sont nulles.\n",
    "\n",
    "$$\n",
    "M= \\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\\\\n",
    "                    0 & a_{2,2} & a_{2,1} \\\\\n",
    "                    0 & 0 & a_{3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "ou \n",
    "\n",
    "$$\n",
    "M= \\begin{bmatrix} a_{1,1} & 0 & 0 \\\\\n",
    "                    a_{2,1} & a_{2,2} & 0 \\\\\n",
    "                    a_{3,1} & a_{3,2} & a_{3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ce sont des matrices très intéressantes pour modéliser des problèmes comme les systèmes linéaires car elles permettent d’organiser efficacement les calculs (le premier coefficient unique sur une ligne permet de trouver la première valeur, et ainsi de suite). \n",
    "\n",
    "#### Exercice 1 : créer des matrices avec numpy\n",
    "\n",
    "Utilisez numpy pour créer différents types de matrices. Pour remplir, contentez vous d’utiliser des nombres entier (1, 2, 3…). N’hésitez pas à utiliser des méthodes qui permettent de générer certains types de matrices particuliers (explorez la doc de numpy si besoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf4767-c45c-4001-b799-3f090c131af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 1 : votre code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Vecteur ligne (1 × 3)\n",
    "\n",
    "\n",
    "# Vecteur colonne (3 × 1)\n",
    "\n",
    "\n",
    "# Matrice identité\n",
    "\n",
    "\n",
    "# Matrice diagonale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b88fea-5a93-4d6b-9850-1559dd695a17",
   "metadata": {},
   "source": [
    "## Opérations matricielles\n",
    "\n",
    "On peut réaliser différentes opérations sur les matrices, certaines triviales, comme l’addition, d’autres qui demandent d’être définie de manière plus réfléchies, comme la multiplication, et encore d’autres opérations qui leur sont spécifiques, comme la transposée.\n",
    "\n",
    "Afin d’illustrer la manière dont sont réalisées les opérations, considérons différents types d’exemples de matrices, qui codent un problème concret. Imaginons trois tables (matrices) :\n",
    "\n",
    "1. Une table où chaque ligne est affectée à un client, et chaque colonne à un produit commandé : chaque cellule de la table indique le nombre de produits commandés par un client donné :\n",
    "\n",
    "|Commandes| A | B | C |\n",
    "|---------|---|---|---|\n",
    "|Client 1 | 4 | 10| 2 |\n",
    "|Client 2 | 8 | 1 | 5 |\n",
    "|Client 3 | 3 | 2 | 0 |\n",
    "|Client 4 | 5 | 3 | 1 |\n",
    "\n",
    "Ainsi selon cette table le client 2 a commandé 5 fois le produit C.\n",
    "\n",
    "Cette table peut-être modélisée par la matrice suivante :\n",
    "$$\n",
    "M_{commandes}(3, 3) = \\begin{bmatrix} 4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0 \\\\\n",
    "                    5 & 3 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Une table des prix pour chaque produit :\n",
    "\n",
    "|Prix | (€) |\n",
    "|-----|-----|\n",
    "| A   |  5  |\n",
    "| B   |  1.5|\n",
    "| C   | 10  |\n",
    "\n",
    "Cette table peut être modélisée par un vecteur/matrice colonne :\n",
    "\n",
    "$$\n",
    "M_{prix}(3, 1) = \\begin{bmatrix} 5 \\\\\n",
    "                    1.5 \\\\\n",
    "                    10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Enfin considérons une table qui indique la quantité de matière première et le temps de travail nécessaire à la production de chaque produit :\n",
    "\n",
    "| Ressources| Kilo | Heures|\n",
    "|-----------|------|-------|\n",
    "| A         | 1.5  | 1     |\n",
    "| B         | 2    | 5     |\n",
    "| C         |0.5   | 0.5   |\n",
    "\n",
    "Cette fois-ci on utilisera une matrice rectangulaire (3, 2) pour la modélisation :\n",
    "\n",
    "$$\n",
    "M_{ressources}(3, 2) = \\begin{bmatrix} 1.5 & 1 \\\\\n",
    "                    2 & 5 \\\\\n",
    "                    0.5 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Addition \n",
    "\n",
    "Imaginons qu’après un mois d’activité, un commercial souhaite mettre à jour le nombre de produits vendus à ses clients. Imaginons les ventes du jour :\n",
    "\n",
    "|Vente du jour | A | B | C |\n",
    "|--------------|---|---|---|\n",
    "|Client 1      | 1 | 2 | 1 |\n",
    "|Client 2      | 3 | 0 | 4 |\n",
    "|Client 3      | 0 | 1 | 0 |\n",
    "|Client 4      | 2 | 1 | 3 |\n",
    "\n",
    "et la matrice correspondante :\n",
    "\n",
    "$$\n",
    "M_{jour}(4, 3) = \\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "                    3 & 0 & 4 \\\\\n",
    "                    0 & 1 & 0 \\\\\n",
    "                    2 & 1 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Si on veut mettre à jour les ventes réalisées, on voit que de manière tout à fait naturelle il suffit d’ajouter les deux matrices cellule par cellule :\n",
    "\n",
    "$$\n",
    "M_{total}(4, 3) = M_{commandes}(4, 3) +\n",
    "M_{jour}(4, 3) = \\\\\n",
    "\\begin{bmatrix} 4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0 \\\\\n",
    "                    5 & 3 & 1\n",
    "\\end{bmatrix}  + \\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "                    3 & 0 & 4 \\\\\n",
    "                    0 & 1 & 0 \\\\\n",
    "                    2 & 1 & 3\n",
    "\\end{bmatrix} = \\\\\n",
    "\\begin{bmatrix} 4+1 & 10+2 & 2+1 \\\\\n",
    "                    8+3 & 1+0 & 5+4 \\\\\n",
    "                    3+0 & 2+2 & 0+0 \\\\\n",
    "                    5+2 & 3+1 & 1+3\n",
    "\\end{bmatrix} = \\\\\n",
    "\\begin{bmatrix} 5 & 12 & 3 \\\\\n",
    "                11 & 1 & 9 \\\\\n",
    "                3 & 4 & 0 \\\\\n",
    "                7 & 4 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ainsi, pour définir **l’addition** : deux matrices de **même dimension** peuvent être additionnées élément par élément.\n",
    "  \n",
    "$$C = A + B \\quad \\Rightarrow \\quad c_{ij} = a_{ij} + b_{ij}$$\n",
    "\n",
    "Il vient immédiatement que les matrices nulles sont bien un élément neutre pour l’addition, pour des dimensions données.\n",
    "\n",
    "Nous avons vu dans le chapitre précédent, que ce qui caractérisait un espace vectoriel était que l’additon de deux vecteurs donnait également un vecteur. On voit ici que deux matrices qu’on additionne donne une matrice (à condition que les matrices aient des dimensions identiques).\n",
    "\n",
    "*Lorsque l’on réalise des opérations entre matrices, les dimensions des matrices va avoir une importance capitale : certaines dimensions doivent absolument être compatible pour que l’opération puisse être réalisée*\n",
    "\n",
    "### Multiplication scalaire\n",
    "\n",
    "Imaginons que pour une raison quelconque, les commandes de tous les clients doublent. On modèliserait cette évolution dans les commandes en multipliants toutes les commandes par deux :\n",
    "\n",
    "$$\n",
    "2 · M_{commandes} = 2 · \\begin{bmatrix} 4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0 \\\\\n",
    "                    5 & 3 & 1\n",
    "\\end{bmatrix} = \\\\\n",
    "\\begin{bmatrix} 2·4 & 2·10 & 2·2 \\\\\n",
    "                    2·8 & 2·1 & 2·5 \\\\\n",
    "                    2·3 & 2·2 & 2·0 \\\\\n",
    "                    2·5 & 2·3 & 2·1\n",
    "\\end{bmatrix} = \\\\\n",
    "\\begin{bmatrix} 8 & 20 & 4 \\\\\n",
    "                16 & 2 & 10 \\\\\n",
    "                6 & 2 & 0 \\\\\n",
    "                10 & 6 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ou, dans une hypothèse plus réaliste de multiplication des prix (inflation…), qu’il en serait de même pour la matrice des prix, d’une autre dimension :\n",
    "\n",
    "$$\n",
    "2 · M_{prix}(3, 1) = \\\\\n",
    "2 · \\begin{bmatrix} 5 \\\\\n",
    "                    1.5 \\\\\n",
    "                    10\n",
    "\\end{bmatrix} = \\\\\n",
    "\\begin{bmatrix} 10 \\\\\n",
    "                3 \\\\\n",
    "                20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dans notre exemple, nous avons multiplié une matrice par un nombre unique : nous reconnaissons là un *scalaire*.\n",
    "On peut donc définir ainsi la **multiplication scalaire** : multiplier chaque élément par le scalaire\n",
    "\n",
    "$$C = \\alpha A \\quad \\Rightarrow \\quad c_{ij} = \\alpha \\cdot a_{ij}$$\n",
    "\n",
    "Ici aussi on peut faire le rapprochement avec ce que nous avons vu pour les vecteurs et la multiplication par un scalaire. En fait nous constatons qu’un ensemble de matrices de dimensions données (m, n) forment un espace vectoriel.\n",
    "\n",
    "#### Exercice 2 : addition et multiplication scalaire\n",
    "\n",
    "Soit les matrices :\n",
    "$$A = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 4 \\\\ 2 & -1 \\end{bmatrix}$$\n",
    "\n",
    "Calculez :\n",
    "- a) $A + B$\n",
    "- b) $3A$\n",
    "- c) $2A - B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb37d2f-069f-44bf-89ff-7ab7d3e58ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2 : votre code !\n",
    "\n",
    "A = np.array([[2, -1], \n",
    "              [0, 3]])\n",
    "B = np.array([[1, 4], \n",
    "              [2, -1]])\n",
    "\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print(\"\\nB =\")\n",
    "print(B)\n",
    "\n",
    "# a) A + B\n",
    "\n",
    "\n",
    "# b) 3·A\n",
    "\n",
    "# c) 2A - B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d9e2f-ab57-4477-b683-934cd53ed1f8",
   "metadata": {},
   "source": [
    "### Transposée\n",
    "\n",
    "Voici une opération spécifique à l’univers des matrices qu’il est bon de connaître avant de s’attaquer à des opérations plus complexes : la transposition. On obtient la transposée d’une matrice quelconque par symétrie par rapport à la diagonale principale. Plus simplement, il s’agit d’échanger lignes et colonnes d’une matrice :\n",
    "$$\n",
    "\\text{Soit }M_{commandes} = \\begin{bmatrix} 4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0\n",
    "\\end{bmatrix}\\\\\n",
    "M_{commandes}^{T} = \\begin{bmatrix} 4 & 8 & 3 \\\\\n",
    "                    10 & 1 & 2 \\\\\n",
    "                    2 & 5 & 0\n",
    "\\end{bmatrix}\\\\\n",
    "\\text{Et bien sûr il vient immédiatement que la transposée d’une matrice transposée est la matrice d’origine :}\\\\\n",
    "(M_{commandes}^{T})^{T} = \\begin{bmatrix}  4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Exemple avec une matrice rectangulaire :\n",
    "\n",
    "$$\n",
    "M_{ressources} = \\begin{bmatrix} 1.5 & 1 \\\\\n",
    "                    2 & 5 \\\\\n",
    "                    0.5 & 0.5\n",
    "\\end{bmatrix}\\\\\n",
    "M_{ressources}^{T} = \\begin{bmatrix} 1.5 & 2 & 0.5 \\\\\n",
    "                    1 & 5 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$A^T[i,j] = A[j,i]$$\n",
    "\n",
    "Si $A$ est de dimension $m \\times n$, alors $A^T$ est de dimension $n \\times m$.\n",
    "\n",
    "* La transposition est linéaire (α scalaire) :\n",
    "    * $(A+B)^{\\mathsf {T}}=A^{\\mathsf {T}}+B^{\\mathsf {T}}$\n",
    "    * $(\\alpha A)^{\\mathsf {T}}=\\alpha A^{\\mathsf {T}}$\n",
    "* La transposition est distributive par rapport au produit, « en ordre inverse » : $(AB)^{\\mathsf {T}}=B^{\\mathsf {T}}\\,A^{\\mathsf {T}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a38e2-4592-4d14-9af6-dbd5886ae32e",
   "metadata": {},
   "source": [
    "### Produit matriciel\n",
    "\n",
    "Comment définir le produit (multiplication) entre deux matrices ?\n",
    "\n",
    "Naïvement on pourrait se dire qu’il suffit ici aussi de multiplier élément par élément, comme pour l’addition :\n",
    "\n",
    "$$C = A · B \\quad \\Rightarrow \\quad c_{ij} = a_{ij} · b_{ij}$$\n",
    "\n",
    "Par exemple :\n",
    "\n",
    "$$C(2,2) = A(2,2) · B(2,2) = \\\\\n",
    "\\begin{bmatrix} a_{1,1} & a_{1,2}\\\\\n",
    "                a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix} ·\n",
    "\\begin{bmatrix} b_{1,1} & b_{1,2}\\\\\n",
    "                b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}= \\\\\n",
    "\\begin{bmatrix} a_{1,1} · b_{1,1}& a_{1,2} · b_{1,2} \\\\\n",
    "                a_{2,1} · b_{2,1}& a_{2,2} · b_{2,2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "On appelle cette manière de définir la multiplication entre matrices **produit de [Hadamard](https://fr.wikipedia.org/wiki/Jacques_Hadamard)**. Cette opération est utilisée dans certains modèles de machine learning, et notamment en deep learning (réseaux de neurones artificiels). Les travaux de Hadamard sont três utilisés dans le domaine du traitement du signal, de la compression de données ou de la cryptographie.\n",
    "\n",
    "Cette définition permet de disposer d’une multiplication qui a de nombreuses propriétés intéressantes, qui sont celles de la multiplication usuelle entre nombres : associativité, distributivité, commutativité… au prix d’une limite : elle n’est possible qu’entre matrices de mêmes dimensions (comme l’addition, vue précédemment, dont le processus est analogue).\n",
    "\n",
    "Néanmoins, on définit le produit matriciel usuel un peu différemment.\n",
    "\n",
    "Revenons à notre exemple. Imaginons que notre commercial, disposant de la matrice des commandes et de la matrices des prix, veuille créer un table qui récapitule ce que chaque client a payé pour l’ensemble de ses commandes. On voudrait obtenir ce résultat en multipliant simplement la matrice des commandes par la matrices des prix, de la même manière qu’on multiplierai le nombre total de commande d’un produit pour connaître le prix du total commandé. Voyons comment il faudrait définir la multiplication pour obtenir ce résultat.\n",
    "\n",
    "En tout premier lieu, on constate que les dimensions de la matrice commandes et de la matrice des prix sont différentes, le produit de Hadamard est d’ores et déjà impossible à exploiter.\n",
    "Pour s’en sortir,on va définir le produit matriciel de manière un peu différente, mais un peu plus complexe.\n",
    "\n",
    "Revenons à nos tables : \n",
    "|Commandes| A | B | C |\n",
    "|---------|---|---|---|\n",
    "|Client 1 | 4 | 10| 2 |\n",
    "|Client 2 | 8 | 1 | 5 |\n",
    "|Client 3 | 3 | 2 | 0 |\n",
    "|Client 4 | 5 | 3 | 1 |\n",
    "\n",
    "|Prix | (€) |\n",
    "|-----|-----|\n",
    "| A   |  5  |\n",
    "| B   |  1.5|\n",
    "| C   | 10  |\n",
    "\n",
    "Ce qu’on voudrait, c’est trouver une définition de la multiplication entre matrices qui permettent d’obtenir la table suivante :\n",
    "\n",
    "|Client   | Prix payé pour A, B et C  |\n",
    "|---------|-------------------------- |\n",
    "|Client 1 | 4 · 5 + 10 · 1.5 + 2 · 10 = 55|\n",
    "|Client 2 | 8 · 5 +  1 · 1.5 + 5 · 10 = 91.5|\n",
    "|Client 3 | 3 · 5 +  2 · 1.5 + 0 · 10 = 18 |\n",
    "|Client 4 | 5 · 5 +  3 · 1.5 + 1 · 10 = 39.5\n",
    "\n",
    "Soit, en généralisant sous forme matricielle :\n",
    "\n",
    "$$M_{facture}(3,3) = M_{commandes}(3,3) × M_{prix}(3,1) = \\\\\n",
    "\\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3}\\\\\n",
    "                a_{2,1} & a_{2,2} & a_{2,3}\\\\\n",
    "                a_{3,1} & a_{3,2} & a_{3,3}\\\\\n",
    "\\end{bmatrix} ×\n",
    "\\begin{bmatrix} b_{1,1}\\\\\n",
    "                b_{2,1}\\\\\n",
    "                b_{3,1}\n",
    "\\end{bmatrix}= \\\\\n",
    "\\begin{bmatrix} a_{1,1} · b_{1,1} + a_{1,2} · b_{2,1} + a_{1,3} · b_{3,1}\\\\\n",
    "                a_{2,1} · b_{1,1} + a_{2,2} · b_{2,1} + a_{2,3} · b_{3,1}\\\\\n",
    "                a_{3,1} · b_{1,1} + a_{3,2} · b_{2,1} + a_{3,3} · b_{3,1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "L’opération décrite ici se fait **ligne à colonne** : si on cherche à calculer $A×B = C$, on parcourt chaque ligne de la matrice $A$ et simultanément chaque colonne de la matrice $B$, et on multiplie chaque élément de chaque ligne de $A$ par chaque élément de chaque colonne de $B$, et ont somme avec les autres produits sur la même ligne. Il vient immédiatement que pour effectuer cette opération, *il faut dans notre exemple que le nombre de colonnes de la matrice $A$ soit égal au nombre de lignes de la matrice $B$*. De plus, la matrice résultante a autant de lignes que la matrice $A$, et autant de colonnes que la matrice $B$.\n",
    "\n",
    "Par ailleurs, on voit aussi rapidement que **cette opération n’est pas commutative** (écrivez explicitement le calcul de $BA$ si cela ne vous saute pas aux yeux). En fait ce n’est même pas que $AB \\neq BA$ mais que bien souvent $BA$ est carrément *impossible* (le nombre de colonnes de $B$ n’est pas égal au nombre de lignes de $A$). \n",
    "\n",
    "Dans notre exemple, la matrice $B$ était un vecteur colonne, mais ce peut très bien être une matrice de dimensions différentes, *la seule chose qui importe est que le nombre de colonnes de la matrice $A$ et de lignes de la matrice $B$ correspondent*.\n",
    "\n",
    "Par exemple si on considère la matrice *ressources*, on pourrait très bien calculer combien de ressources (en matière première, et en temps) chaque commande de chaque client a coûté :\n",
    "\n",
    "|Commandes| A | B | C |\n",
    "|---------|---|---|---|\n",
    "|Client 1 | 4 | 10| 2 |\n",
    "|Client 2 | 8 | 1 | 5 |\n",
    "|Client 3 | 3 | 2 | 0 |\n",
    "|Client 4 | 5 | 3 | 1 |\n",
    "\n",
    "| Ressources| Kilo | Heures|\n",
    "|-----------|------|-------|\n",
    "| A         | 1.5  | 1     |\n",
    "| B         | 2    | 5     |\n",
    "| C         |0.5   | 0.5   |\n",
    "\n",
    "D’où :\n",
    "\n",
    "|Client   | Kilo                       | Heures                   |\n",
    "|---------|----------------------------|--------------------------|\n",
    "|Client 1 | 4 · 1.5 + 10 · 2 + 2 · 0.5 | 4 · 1 + 10 · 5 + 2 · 0.5 |\n",
    "|Client 2 | 8 · 1.5 + 1 · 2 + 5 · 0.5  | 8 · 1 + 1 · 5 + 5 · 0.5  |\n",
    "|Client 3 | 3 · 1.5 + 2 · 2 + 0 · 0.5  |  3 · 1 + 2 · 5 + 0 · 0.5 |\n",
    "|Client 3 | 5 · 1.5 + 3 · 2 + 1 · 0.5  |  5 · 1 + 3 · 5 + 1 · 0.5 |\n",
    "\n",
    "Une manière d’interpréter cette opération ici est qu’elle permet d’exprimer les features de la matrice B en fonction des observations de la matrice A.\n",
    "\n",
    "On peut généraliser cette définition de la multiplication entre matrices avec cette formule :\n",
    "\n",
    "$$\n",
    "\\text{Soit deux matrices } A(m,n)=(a_{i,j}) \\text{ et } B(n,k)= (b_{i,j}) \\text{ alors leur produit } A·B=C(m,p)=(c_{i,j}) \\text{ est défini ainsi :}\\\\\n",
    "\\forall i,j:c_{ij}=\\sum _{k=1}^{n}a_{ik}b_{kj}=a_{i1}b_{1j}+a_{i2}b_{2j}+\\cdots +a_{in}b_{nj}\n",
    "$$\n",
    "\n",
    "Le produit matriciel est une opération tellement importante en calcul numérique que la recherche d’algorithmes efficaces pour optimiser ce calcul est encore à l’ordre du jour. Nous verrons plus bas pourquoi il est particulièrement intéressant de pouvoir enchaîner des multiplications de matrices.\n",
    "\n",
    "Enfin il n’aura pas échappé aux plus observateur-ice-s, que cette définition du produit matriciel ressemble fortement à celle du produit sclaire vu pour les vecteurs de dimension n (somme de produits). Effectivement, on peut considérer le produit matriciel comme une généralisation du produit scalaire, le produit matriciel entre un vecteur colonne et un vecteur ligne donnant une matrice (1, 1) à un seul coefficient, c’est à dire un scalaire :\n",
    "$$\n",
    "\\langle u, v \\rangle = u^{T} · v\n",
    "$$\n",
    "\n",
    "Mais attention, si le produit scalaire jouit de certaines propriétés car il est justement un cas particulier, ces propriétés ne sont pas généralisables au produit matriciel. Voici les différences entre produit scalaire et produit matriciel :\n",
    "\n",
    "* le produit scalaire est réalisé entre deux vecteurs et donne un scalaire, le produit matriciel est réalisé entre deux matrices et donne une matrice\n",
    "* le produit scalaire est commutatif ($ \\langle u, v \\rangle = \\langle v, u \\rangle  $, le produit matriciel ne l’est pas $A·B \\neq B·A$\n",
    "* le produit scalaire exige que les deux vecteurs aient la même dimension, le produit matriciel peut être réalisé entre des matrices de dimensions différentes, à condition que le nombre de colonnes de la première soit égal au nombre de lignes de la seconde\n",
    "\n",
    "Cette distinction se retrouve aussi dans la syntaxe de Python/Numpy.\n",
    "\n",
    "Le produit scalaire est appelé *dot product* en anglais, mais c’est un nom qui est un peu ambigu, car il met l’accent sur l’usage de l’opération $·$ (produit). On va l’utiliser pour toutes les opérations qui réalisent « la somme des produits composante par composante », c’est-à-dire $u · v = u_1·v_1 + u_2 · v_2 + … + u_n·v_n$. Par ailleurs le terme *inner produt* désigne plus généralement ce que nous appelons strictement en français produit scalaire (toute opération entre deux vecteurs dont le résultat est un scalaire − on peut créer d’autres définitions du produit scalaire que celle que nous avons vu), on trouve aussi l’appelation *scalar product.* \n",
    "\n",
    "Ainsi, la méthode `numpy.dot()` permet de faire le produit de deux `arrays`, quelle que soit leur dimension, et elle implémentera des opérations mathématiquement différentes en fonction de la dimension des `arrays` : \n",
    "* s’il s’agit de deux arrays de dimension 0 (des scalaires en fait), ce sera le produit classique\n",
    "* s’il s’agit de deux arrays de dimension 1, ce sera un produit scalaire tel que nous le connaissaons\n",
    "* s’il s’agit d’arrays de dimension 2 (donc des matrices), ce sera un produit matriciel\n",
    "* s’il s’agit de tenseurs (vous verrez cela en deep learning) il réalisera la somme des produits sur les derniers axes de chaque tenseur\n",
    "\n",
    "Le choix de ce nom est historique : d’autre bibliothèque de calcul en algèbre linéaire (comme BLAS) utilisent cette appellation pour la même opération. De plus ce mot décrit bien les opérations effectuées dans plusieurs dimensions, il est concis et n’est pas aussi ambigu que le mot «scalar product» (on multiplie par un scalaire ou est-ce que le résultat de la multiplication est un scalaire ?). Enfin du moment que les arguments sont des arrays, il marche tout le temps, mais le résultat peut-être assez différent, car ce n’est pas la même opération mathématique qui est implémentée à chaque fois.\n",
    "\n",
    "* C’est pour cela **qu’on conseille fortement d’utiliser l’opérateur `@`** lorsque l’on réalise le produit matriciel (ou la méthode `numpy.matmul()`) sur des `2D-arrays` (matrices), par soucis de lisibilité.\n",
    "* Pour les `0D-arrays`, on recommande d’utiliser `numpy.multiply()` ou `*`, pour les mêmes raisons.\n",
    "* **Et attention**, entre deux matrices l’opérateur `*` réalise simplement le produit composant par composant (produit d’Hadamard), c’est une faute d’inattention courante chez les étudiant-e-s en machine learning !!\n",
    "\n",
    "\n",
    "#### Exercice 3 : Produit matriciel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465c2f5-8af6-48fc-8fe0-7770bdc40f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3 : votre code !\n",
    "\n",
    "# Produit matriciel\n",
    "\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "v = np.array([2, 1])\n",
    "\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print(\"\\nB =\")\n",
    "print(B)\n",
    "print(f\"\\nv = {v}\")\n",
    "\n",
    "# Produit AB\n",
    "\n",
    "\n",
    "# Produit BA\n",
    "\n",
    "\n",
    "# Vérifier que AB ≠ BA\n",
    "\n",
    "\n",
    "# Produit matrice-vecteur Av\n",
    "\n",
    "\n",
    "# A²\n",
    "\n",
    "\n",
    "# Vérifier (A²)^T = (A^T)²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a56a47-3658-4c3d-8102-4ab191543fe3",
   "metadata": {},
   "source": [
    "### Matrice inverse\n",
    "\n",
    "#### Position du problème\n",
    "\n",
    "Qui dit multiplication, dit division. Diviser revient à multiplier par l’inverse : on cherche à réaliser une opération « qui va dans l’autre sens » (comme la soustraction avec l’addition). Voyons donc comment on peut définir une telle opération lorsque l’on manipule des matrices.\n",
    "\n",
    "Voilà comment se pose le problème : connaissant les matrices $M_{commandes}$ et $M_{factures}$, saurait-on remonter de la facture aux prix ? \n",
    "$$\n",
    "\\text{Soit } M_{commandes} × M_{prix} = M_{factures}\n",
    "$$ \n",
    "$M_{prix}$ est ici l’inconnue, alors si on note $M^{-1}_{commandes}$ l’inverse de $M_{commandes}$, $M_{prix}$ peut être calculée ainsi :\n",
    "\n",
    "$$\n",
    "M_{prix} = M^{-1}_{commandes} × M_{factures}\n",
    "$$\n",
    "\n",
    "L’idée serait de calculer la matrice inverse d’une matrice $M_{commandes}$ pour retrouver, en la multipliant par la matrice $M_{factures}$, la matrice $M_{prix}$. Mais comment faire ? Est-ce qu’un telle matrice existe ? (la première étape d’un problème en mathématique est souvent déjà de se demander si la solution existe).\n",
    "\n",
    "Revenons à notre équation de départ :\n",
    "\n",
    "$$\n",
    "\\text{Soit } M_{commandes} × M_{prix} = M_{factures}\n",
    "$$ \n",
    "\n",
    "Si $M_{prix}$ est inconnu, écrivons :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 4 & 10 & 2 \\\\\n",
    "                    8 & 1 & 5 \\\\\n",
    "                    3 & 2 & 0 \\\\\n",
    "                    5 & 3 & 1\n",
    "\\end{bmatrix} ×\n",
    "\\begin{bmatrix} p_{1}\\\\\n",
    "                p_{2}\\\\\n",
    "                p_{3}\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix} 55\\\\\n",
    "                91.5\\\\\n",
    "                18\\\\\n",
    "                39.5\n",
    "\\end{bmatrix}\\\\\n",
    "\\text{ce qui correspondont au système linéaire suivant : }\n",
    "\\begin{cases}\n",
    "4p_1 + 10p_2 + 20p_3 = 55 \\\\\n",
    "8p_1 + 1p_2 + 5p_3 = 91.5 \\\\\n",
    "3p_1 + 2p_2 + 0p_3 = 18 \\\\\n",
    "5p_1 + 3p_2 + 1p_3 = 39.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Résoudre ce système, c’est trouver une solution au problème et donc prouver que la matrice $M^{-1}_{commandes}$ existe (mais il nous faudra encore la déterminer). Mais déjà, en posant le problème ainsi, on voit déjà apparaître des difficultés : nous avons 3 variables inconnues, et un système à 4 équations. On dit que le problème est **surdéterminé** : il y a trop de contraintes, ce qui signifie qu’il y a très peu de chances que le système ait une solution (c’est possible si une équation du système est linéairement dépendante d’une autre, et donc expriment la même contrainte). De même, s’il y a moins d’équation que de variables (par exemple si la matrice M_{commandes} n’avait qu’une ou deu lignes, on n’aurait pas assez de contrainte donc les solutions ne seraient pas uniques. \n",
    "\n",
    "C’est pour cela qu’on va considérer qu’une matrice est inversible seulement si on est sûr d’avoir une solution unique, c’est à dire qu’on a une matrice carrée dont les lignes sont linéairement indépendantes (car ainsi on aurait 3 équations et 3 inconnues).\n",
    "\n",
    "#### Rang d’une matrice\n",
    "\n",
    "Le rang indique le nombre de directions (lignes ou colonnes) indépendantes présentes dans la matrice : c‘est le nombre maximal de lignes (ou colonnes) linéairement indépendantes. \n",
    "\n",
    "Le rang demande des calculs un peu rébarbatifs, par exemple mettre la matrice sous forme échelonnée (via [la méthode de Gauss-Jordan](https://fr.wikipedia.org/wiki/%C3%89limination_de_Gauss-Jordan)), le nombre de lignes non nulles dans cette forme échelonnée est alors le rang. On ne détaille pas les calculs mais la forme échelonnée de $M_{commandes}$ est :\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & \\tfrac{5}{2} & 5 \\\\\n",
    "0 & 1 & \\tfrac{35}{19} \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & -\\tfrac{13}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Avec 3 colonnes et 4 lignes (les équations sont donc bien toutes indépendantes), la matrice est de rang 3.\n",
    "\n",
    "Pour qu’une matrice carrée soit inversible, il faut qu’elle soit de **rang maximal** : si une matrice $A$ est de dimension $(n, n)$ alors il faut que $Rang(A) = n$\n",
    "\n",
    "#### Déterminant\n",
    "\n",
    "Pour une matrice carrée de petite dimension, il existe une méthode beaucoup plus simple pour déterminer si une telle matrice est de rang maximal (donc inversible) : calculer son déterminant. Si le déterminant est nul, alors elle n’est pas de rang maximal (et si le déterminant est nul, alors elle l’est).\n",
    "\n",
    "Qu’est-ce que le déterminant d’une matrice, et comment le calculer ?\n",
    "\n",
    "Le déterminant d'une matrice 2×2 est donné par :\n",
    "\n",
    "$$\n",
    "\\det \\begin{bmatrix}⁡a & b\\\\\n",
    "                    c & d\n",
    "\\end{bmatrix}= ad−bc\n",
    "$$\n",
    "Pour une matrice 3×3, on utilise par exemple la règle de [Sarrus](https://fr.wikipedia.org/wiki/Pierre-Fr%C3%A9d%C3%A9ric_Sarrus) (pour un calcul à la main). Il s’agit de :\n",
    "* écrire les trois colonnes de la matrice\n",
    "* rajouter en dessous les deux première lignes de la matrice\n",
    "* faire les produit des coefficients des diagonales et d’en faire la somme pour les diagonales descendantes vers le bas/droite, et la différence pour celles ascendantes vers haut/droite\n",
    "\n",
    "Ce qui nous donne : \n",
    "$$\n",
    "\\det \\begin{bmatrix}⁡a & b & c\\\\\n",
    "                    d & e & f\\\\\n",
    "                    g & h & i\n",
    "\\end{bmatrix}= aei+dhc+gbf−ceg−fha−ibd\n",
    "\\text{ en gardant en tête, pour la méthode de Sarrus : }\n",
    "\\begin{bmatrix}⁡a & b & c\\\\\n",
    "                    d & e & f\\\\\n",
    "                    g & h & i\\\\\n",
    "                    a & b & c\\\\\n",
    "                    d & e & f\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "La formule du déterminant est complexe à écrire (elle fait appel à des permuatations) :\n",
    "$$\n",
    "\\det(A)=\\begin{vmatrix} a_{1;1} & \\cdots & a_{1;n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n;1} & \\cdots & a_{n;n} \\end{vmatrix} = \\sum_{\\sigma \\in \\mathfrak{S}_n}\n",
    "\\varepsilon(\\sigma) \\prod_{i=1}^n a_{ \\sigma(i),i}\n",
    "$$\n",
    "\n",
    "Pour les matrices de dimension supérieure, l’emploi d’algorithmes est plus avantageux.\n",
    "\n",
    "Pourquoi faut-il que le déterminant soit non-nul pour que le rang soit maximal ? Ça va être l’objet de notre exercice !\n",
    "\n",
    "\n",
    "#### Exercice 4 : déterminant et inversion\n",
    "\n",
    "Dans la doc de Numpy, trouvez comment calculer le rang et le déterminant d’une matrice.\n",
    "\n",
    "Puis, considérant cette matrice :\n",
    "$$\n",
    "\\begin{bmatrix}⁡3 & 5 & 7\\\\\n",
    "                    2 & 4 & 3\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Calculer le rang et le déterminant de cette matrice\n",
    "\n",
    "Considérez maintenant la matrice suivante : \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}⁡3 & 5 & 7\\\\\n",
    "                    0 & 2 & 2\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Quelle modification a-t-on apporté à la matrice ?\n",
    "* Calculez le rang et le déterminant de cette matrice. Qu’observez-vous ?\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}⁡3 & 5 & 7\\\\\n",
    "                    0 & 2 & 2\\\\\n",
    "                    2 & 4 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Quelle modification a-t-on apporté à la matrice ?\n",
    "* Calculez le rang et le déterminant de cette matrice. Qu’observez-vous ?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}⁡6 & 10 & 14\\\\\n",
    "                    0 & 2 & 2\\\\\n",
    "                    2 & 4 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Quelle modification a-t-on apporté à la matrice ?\n",
    "* Calculez le rang et le déterminant de cette matrice. Qu’observez-vous ?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}⁡3 & 5 & 7\\\\\n",
    "                    0 & 2 & 2\\\\\n",
    "                    8 & 14 & 17\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Quelle modification a-t-on apporté à la matrice ?\n",
    "* Calculez le rang et le déterminant de cette matrice. Qu’observez-vous ?\n",
    "\n",
    "Cette fois-ci jouons avec les colonnes. Créons une matrice dont la dernière colonne a une relation linéaire avec la première colonne :\n",
    "$$\n",
    "\\begin{bmatrix}⁡3 & 5 & 8\\\\\n",
    "                    2 & 4 & 6\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* quelle est cette relation linéaire ?\n",
    "* calculer rang et déterminant de cette matrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137268e2-01ed-43f1-932c-c0bcd7ddc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCICE SUR LE RANG ET LE DÉTERMINANT DES MATRICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Matrice 1\n",
    "print(\"\\n1. MATRICE INITIALE\")\n",
    "print(\"-\" * 60)\n",
    "A1 = np.array([[3, 5, 7],\n",
    "               [2, 4, 3],\n",
    "               [0, 2, 2]])\n",
    "print(\"Matrice A1 :\")\n",
    "print(A1)\n",
    "print(f\"\\nRang de A1 : \"# VOTRE CODE\n",
    "print(f\"Déterminant de A1 :\" # VOTRE CODE\n",
    "\n",
    "# Matrice identique\n",
    "A_identique = np.array([[3, 5, 7],\n",
    "                        [0, 2, 2],\n",
    "                        [0, 2, 2]])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. LIGNES IDENTIQUES\")\n",
    "print(\"-\" * 60)\n",
    "print(A_identique)\n",
    "\n",
    "print(\"\\nModification apportée : L3 = L2\")\n",
    "print(\"(Les lignes 2 et 3 sont devenues identiques)\")\n",
    "\n",
    "print(f\"\\nRang de A_identique : \" # VOTRE CODE\n",
    "print(f\"Déterminant de A_identique : \" # VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"OBSERVATIONS :\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE / VOS OBSERVATIONS\n",
    "\n",
    "# Matrice 2\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. PERMUTATION DE LIGNES\")\n",
    "print(\"-\" * 60)\n",
    "A2 = np.array([[3, 5, 7],\n",
    "               [0, 2, 2],\n",
    "               [2, 4, 3]])\n",
    "print(\"Matrice A2 :\")\n",
    "print(A2)\n",
    "print(\"\\nModification apportée : Permutation des lignes 2 et 3\")\n",
    "print(\"(L2 et L3 ont été échangées)\")\n",
    "# VOTRE CODE\n",
    "\n",
    "# Matrice 3\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. MULTIPLICATION D'UNE LIGNE PAR UN SCALAIRE\")\n",
    "print(\"-\" * 60)\n",
    "A3 = np.array([[6, 10, 14],\n",
    "               [0, 2, 2],\n",
    "               [2, 4, 3]])\n",
    "print(\"Matrice A3 :\")\n",
    "print(A3)\n",
    "print(\"\\nModification apportée : L1 → 2×L1\")\n",
    "print(\"(La première ligne a été multipliée par 2)\")\n",
    "# VOTRE CODE\n",
    "\n",
    "# Matrice 4\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. COMBINAISON LINÉAIRE DE LIGNES\")\n",
    "print(\"-\" * 60)\n",
    "A4 = np.array([[3, 5, 7],\n",
    "               [0, 2, 2],\n",
    "               [8, 14, 17]])\n",
    "print(\"Matrice A4 :\")\n",
    "print(A4)\n",
    "print(\"\\nModification apportée : L3 → L3 + 2×L1\")\n",
    "print(\"(On a ajouté 2 fois la ligne 1 à la ligne 3)\")\n",
    "print(\"Vérification : [2, 4, 3] + 2×[3, 5, 7] = [2+6, 4+10, 3+14] = [8, 14, 17] ✓\")\n",
    "# VOTRE CODE\n",
    "\n",
    "# Matrice 5\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. DÉPENDANCE LINÉAIRE DES COLONNES\")\n",
    "print(\"-\" * 60)\n",
    "A5 = np.array([[3, 5, 8],\n",
    "               [2, 4, 6],\n",
    "               [0, 2, 2]])\n",
    "print(\"Matrice A5 :\")\n",
    "print(A5)\n",
    "print(\"\\nModification apportée : relation linéaire, C3 = C1 + C2\")\n",
    "\n",
    "# Vérification\n",
    "C1 = A5[:, 0]\n",
    "C2 = A5[:, 1]\n",
    "C3 = A5[:, 2]\n",
    "print(f\"\\nVérification numérique : C1 + C2 = {C1 + C2} = C3 ✓\")\n",
    "\n",
    "# VOTRE CODE \n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RÉCAPITULATIF DES PROPRIÉTÉS DU DÉTERMINANT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Permutation de lignes : change le signe du déterminant\")\n",
    "print(\"2. Multiplication d'une ligne par k : multiplie le déterminant par k\")\n",
    "print(\"3. Addition d'un multiple d'une ligne : conserve le déterminant\")\n",
    "print(\"4. Dépendance linéaire : déterminant = 0, rang non maximal\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac87a3b-302f-442a-a676-4b903aa7494f",
   "metadata": {},
   "source": [
    "#### Pourquoi deux colonnes identiques donnent un déterminant nul ?\n",
    "\n",
    "C'est une conséquence de l'antisymétrie du déterminant :\n",
    "Si on échange deux colonnes, le déterminant change de signe :\n",
    "\n",
    "Soit une matrice $a$ avec deux colonnes $C_i$ et $C_j$. On crée la matrice $A'$ en inversant les colonnes $C_i$ et $C_j$. Nous avons vu que, alors :\n",
    "$\\det(A')$ = $-\\det(A)$\n",
    "\n",
    "Mais si $C_i$ = $C_j$, alors échanger les colonnes ne change pas la matrice !\n",
    "\n",
    "Néanmoins la propriété du déterminant est conservée : $\\det(A)$ = $\\det(A')$ = $-\\det(A)$ \n",
    "Ce qui implique : $2×\\det(A) = 0$\n",
    "Donc : $\\det(A) = 0$\n",
    "\n",
    "Implication pour la combinaison linéaire (ici $C_3 = \\alpha C1 + \\beta C2$ où α = 1 et β = 1). On a vu précédemment que multiplier une colonne par un scalaire était équivalent à multiplier le déterminant par ce scalaire (linéarité du déterminant) :\n",
    "$$\n",
    "\\det \\begin{bmatrix}⁡3 & 5 & 8\\\\\n",
    "                    2 & 4 & 6\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix} =\n",
    "\\alpha \\det ×\\begin{bmatrix}⁡3 & 5 & 3\\\\\n",
    "                    2 & 4 & 2\\\\\n",
    "                    0 & 2 & 0\n",
    "\\end{bmatrix} +\n",
    "\\beta \\det  \\begin{bmatrix}⁡3 & 5 & 5\\\\\n",
    "                    2 & 4 & 4\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix}\\\\\n",
    "\\det \\begin{bmatrix}⁡3 & 5 & 8\\\\\n",
    "                    2 & 4 & 6\\\\\n",
    "                    0 & 2 & 2\n",
    "\\end{bmatrix} = 0 + 0\n",
    "$$\n",
    "\n",
    "Dans tous les cas retenons que :\n",
    "* seules les matrices carrées sont inversibles\n",
    "* il faut par ailleurs que le rang de la matrice soit maximal\n",
    "* c’est à dire que le déterminant de la matrice soit non nul\n",
    "\n",
    "De nombreux algorithmes de machine learning ou d’analyse statistique demandent de calculer des inverses, c’est donc une notion importante à comprendre.\n",
    "\n",
    "#### Calcul d’une matrice inverse\n",
    "\n",
    "Nous avons passé tout ce temps simplement pour simplement savoir si une matrice était inversible.\n",
    "\n",
    "Mais comment calculer cette matrice inverse ? Le problème est loin d’être trivial.\n",
    "\n",
    "Formulons le problème un peu différemment. Si une matrice $A$ est inversible, alors une manière de définir l’inverse est de revenir à la notion d’opération inverse et d’élément neutre pour la multiplication :\n",
    "\n",
    "$$\n",
    "A × A^{-1} = A^{-1} × A = I\n",
    "$$\n",
    "\n",
    "$I$ étant la matrice identité de même dimension que $A$. À partir de là, on peut calculer l’inverse d’une matrice avec différentes méthodes :\n",
    "\n",
    "- le pivot de Gauss : si $A$ est connue, calculer l’inverse revient à résoudre un système. En effectuant un ensemble d’opérations sur $A$ pour obtenir la matrice identité, il suffit de refaire ces opérations sur la matrice identité pour obtenir $A^{-1}$.\n",
    "- calculer la « transposée de la matrice des cofacteurs (ou matrice complémentaire » : c’est un calcul à base de déterminants de sous-matrices, relativement simple pour un calcul à la main lorsque la matrice est de petite dimension\n",
    "\n",
    "Heureusement pour nous `numpy` nous propose une méthode : `np.linalg.inv()`. Nous disposons d’une autre méthode : `np.linalg.solve()`, qui permet de résoudre des systèmes (bien sûr pas très pertinente en dehors de ce cours).\n",
    "\n",
    "#### Exercice 5 : calcul de matrice inverse \n",
    "\n",
    "Caluler avec les deux méthodes (`.inv()` et `.solve()`) les inverses des matrices suivantes :\n",
    "$$\n",
    "A = \\begin{bmatrix}⁡3 & 5 & 7\\\\\n",
    "                  2 & 4 & 3\\\\\n",
    "                  0 & 2 & 2\n",
    "\\end{bmatrix}⁡ \\text{ et } \n",
    "B = \\begin{bmatrix}9 & 10 & 8\\\\\n",
    "              6 & 8 & 6\\\\\n",
    "              0 & 4 & 2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a79868-4d2e-431c-b4d5-ba8985526ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une matrice inversible\n",
    "A = np.array([[3, 5, 7],\n",
    "              [2, 4, 3],\n",
    "              [0, 2, 2]])\n",
    "\n",
    "# Calculer l'inverse avec .inv()\n",
    "A_inv =  # VOTRE CODE\n",
    "\n",
    "# Vérification : A × A⁻¹ = I (matrice identité)\n",
    "produit = # VOTRE CODE\n",
    "print(\"\\nVérification A × A⁻¹ :\")\n",
    "print(produit)\n",
    "print(\"\\nC'est bien la matrice identité (à la précision numérique près) :\")\n",
    "print(np.allclose(produit, np.eye(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c436165-030b-49ac-8b89-ffb4b14072c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode alternative (moins directe, mais illustrative)\n",
    "n = A.shape[0]\n",
    "I = np.eye(n)\n",
    "\n",
    "# Résoudre A × X = I pour trouver X = A⁻¹ avec .solve()\n",
    "A_inv_alt = # VOTRE CODE\n",
    "\n",
    "print(\"Inverse calculée avec solve :\")\n",
    "print(A_inv_alt)\n",
    "print(\"\\nC'est la même que avec inv() :\")\n",
    "print(np.allclose(A_inv, A_inv_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111784c5-6be7-4b44-b7de-adc57fa5bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice avec déterminant nul \n",
    "B = np.array([[9, 10, 8],\n",
    "              [6, 8, 6],\n",
    "              [0, 4, 2]])\n",
    "\n",
    "# déterminant de B \n",
    "# VOTRE CODE\n",
    "\n",
    "# rang de B\n",
    "# VOTRE CODE\n",
    "\n",
    "# inverse de B\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c090f0a-b27d-448c-acaa-3ecbdefadfa0",
   "metadata": {},
   "source": [
    "## Matrices et transformations géométriques\n",
    "\n",
    "Nous venons de voir comment étaient défini certaines opérations pour les matrices, notamment la multiplication.\n",
    "\n",
    "Nous allons dans cette partie voir comment en enchaînant les multiplications de matrices, cela nous permet de réaliser certaines transformations géométriques : rotation, agrandissement/réductions, cisaillement…\n",
    "\n",
    "En bonus on verra quelques problèmes posés et comment ils sont résolus en infographie.\n",
    "\n",
    "Cela nous servira d’introduction pour la partie suivante : les transformations linéaires et les projections.\n",
    "\n",
    "### Rappel des épisodes précédents\n",
    "\n",
    "De tout ce que nous avons vu précédemment, retenons :\n",
    "* nous disposons d’objet de $n$ dimensions appelés vecteurs\n",
    "* nous disons d’objet de $(n, m)$ dimensions appelés matrices\n",
    "* nous avons défini des opérations analogue aux opérations usuelles (+, -, ·, ÷) entre les matrices, et entre les matrices et les vecteurs\n",
    "\n",
    "Penchons-nous sur ces opérations entre matrices et vecteurs. Considérons le produit entre une matrice (2, 2) et un vecteur de dimension 2 :\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}a & b\\\\\n",
    "                    c & d\n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix}x\\\\\n",
    "                    y\n",
    "    \\end{bmatrix}=\n",
    "    \\begin{bmatrix}a·x + b·y\\\\\n",
    "                    c·x + d·y\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "On constate que la multiplication d’un vecteur de dimension 2 par une matrice (2,2) donne un autre vecteur de dimension 2 : géométriquement, on peut dire qu’une matrice carrée de dimension (n,n) transforme un vecteur de dimension n en un autre vecteur de dimension n dans le même espace, et la matrice inverse permet de revenir au vecteur de départ : la matrice établit une correspondance point à point, on dit que la matrice réalise un *isomorphisme*. Voici une illustration dans le plan : \n",
    "\n",
    "Dans cette section nous allons voir des exemples de transformations que l’on peut réaliser dans le plan.\n",
    "\n",
    "### Matrice identité\n",
    "\n",
    "$$ \n",
    "    \\begin{bmatrix} 1 & 0 \\\\\n",
    "                0 & 1 \n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix} x \\\\ \n",
    "                    y\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} x.1 + y.0 \\\\\n",
    "                x.0 + y.1 \n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} x \\\\\n",
    "                    y \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "On vérifie bien que la matrice identité « transforme » bien un vecteur en lui-même (donc pas de transformation). Ce calcul peut sembler inutile, mais en fait il nous permet de saisir la logique derrière l’utilisation du produit matriciel pour transformer des figures géométriques : avec des coefficients bien choisis, une matrice peut avoir un effet (ou pas) sur les coordonnées d’un point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061516d1-7883-4273-a128-480eeab81784",
   "metadata": {},
   "source": [
    "### Changement d’échelle (*scaling*)\n",
    "\n",
    "Si on multiplie la matrice identité par un scalaire, et que nous multiplions le résultat par un vecteur, on modifie l’échelle du vecteur :\n",
    "\n",
    "$$ \n",
    "    10 · \n",
    "    \\begin{bmatrix} 1 & 0 \\\\\n",
    "                0 & 1 \n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix} x \\\\ \n",
    "                    y\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 10 & 0 \\\\\n",
    "                0 & 10 \n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix} x \\\\ \n",
    "                    y\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 10·x + 0·y \\\\\n",
    "                0·x + 10·y \n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 10·x \\\\\n",
    "                    10·y \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hourra on a réaliser un changement d’échelle de notre vecteur. On appelle cette opération une homothétie (d’un facteur 10). Mais dans ce cas on pourrait très bien multiplier directement le vecteur par un scalaire ?\n",
    "\n",
    "En effet mais cette opération met à jour que l’on pourrait très bien choisir des facteurs d’échelle différents pour les x, et pour les y :\n",
    "\n",
    "$$  \n",
    "    \\begin{bmatrix} 10 & 0 \\\\\n",
    "                0 & 5 \n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix} x \\\\ \n",
    "                    y\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 10·x + 0·y \\\\\n",
    "                0·x + 5·y \n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 10·x \\\\\n",
    "                    5·y \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Cette opération ressemble à un étirement de notre figure dans une direction donnée (l’un des axes), car notre figure est déformée selon un axe privilégié.\n",
    "\n",
    "Plus généralement, on peut donc définir une famille de matrice :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} S_x & 0 \\\\\n",
    "                0 & S_y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Avec les facteurs d’échelle $S_x$ et $S_y$ qui vont agir différentiellement sur chacune des deux dimensions. \n",
    "\n",
    "### Exercice 6 : homothétie et étirement\n",
    "\n",
    "Voici, avec matplotlib, la démonstration d’une transformation d’une figure représentant un carré par une matrice réalisant une homothétie, et une autre matrice réalisant un étirement.\n",
    "\n",
    "Mathématiquement, la convention est de présenter les vecteurs comme des vecteurs colonnes (comme nous l’avons fait dans nos calculs). Ce n’est pas la convention la plus pratique lorsque l’on code, car dans `numpy`, de la manière dont sont « imbriquées » les arrays de plusieurs dimensions, l’élément de fondamental est plutôt une ligne :\n",
    "\n",
    "```python\n",
    "u = np.array([[1, 2]]) # vecteur ligne\n",
    "v = np.array([[1],[2]]) # vecteur colonne\n",
    "```\n",
    "Donc selon la manière dont on a créé nos vecteurs, il faut réaliser des opérations de transposition pour transformer nos vecteurs lignes en vecteurs colonne, et vice-versa. C’est une gymnastique dont il faut prendre l’habitude quand on manipule des arrays, ce qui est tout le temps le cas en machine learning (et je ne parle même pas du *deep learning*). C’est ce que nous illustrons dans le code ci-dessous (n’hésitez pas à décomposer les opérations et à afficher les résultats intermédiaires pour bien voir à quoi ressemblent les *arrays* manipulées à chaque étape) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a6926-1f33-4a5b-b5b6-1f79408dcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6 : démo (ce n’est pas à vous de coder)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définition des sommets du carré original (chaque ligne est un point [x, y])\n",
    "carre_original = np.array([[1, 1], \n",
    "                           [1, 3], \n",
    "                           [3, 3], \n",
    "                           [3, 1]])\n",
    "\n",
    "# Matrice de transformation : homothétie de facteur 5\n",
    "matrice_homothetie = np.array([[5, 0], \n",
    "                                [0, 5]])\n",
    "\n",
    "# Matrice d’étirement\n",
    "matrice_etirement = np.array([[3, 0], \n",
    "                                  [0, 4]])\n",
    "\n",
    "# Application de la transformation par homothétie\n",
    "# On transpose pour avoir les points en colonnes, on multiplie, puis on retranpose\n",
    "carre_homothetie = (matrice_homothetie @ carre_original.T).T\n",
    "\n",
    "# Application de la transformation par étirement\n",
    "carre_etirement = (matrice_etirement @ carre_original.T).T\n",
    "\n",
    "# Fermeture des polygones pour le tracé (retour au premier point)\n",
    "carre_original_ferme = np.vstack([carre_original, carre_original[0]])\n",
    "carre_homothetie_ferme = np.vstack([carre_homothetie, carre_homothetie[0]])\n",
    "carre_etirement_ferme = np.vstack([carre_etirement, carre_etirement[0]])\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Tracé du carré original en bleu\n",
    "plt.plot(carre_original_ferme[:, 0], carre_original_ferme[:, 1], \n",
    "         'o-', color='blue', linewidth=2, markersize=8, label='Carré original')\n",
    "\n",
    "# Tracé du carré avec homothétie en rouge\n",
    "plt.plot(carre_homothetie_ferme[:, 0], carre_homothetie_ferme[:, 1], \n",
    "         'o-', color='red', linewidth=2, markersize=8, label='Homothétie [[5, 0], [0, 5]]')\n",
    "\n",
    "# Tracé du carré avec étirement en vert\n",
    "plt.plot(carre_etirement_ferme[:, 0], carre_etirement_ferme[:, 1], \n",
    "         'o-', color='green', linewidth=2, markersize=8, label='Étirement [[3, 0], [0, 4]]')\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Transformations géométriques par produit matriciel\\nHomothétie et étirement', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "\n",
    "# Affichage des axes passant par l'origine\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Ajustement des limites pour bien voir les deux carrés avec aspect ratio égal\n",
    "plt.xlim(-1, 16)\n",
    "plt.ylim(-1, 16)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=== Transformations matricielles ===\\n\")\n",
    "print(\"Carré original :\")\n",
    "print(carre_original)\n",
    "print(\"\\n1. Matrice d'homothétie :\")\n",
    "print(matrice_homothetie)\n",
    "print(\"\\nCarré après homothétie :\")\n",
    "print(carre_homothetie)\n",
    "print(\"\\n2. Matrice d’étirement :\")\n",
    "print(matrice_etirement)\n",
    "print(\"\\nCarré après étirement :\")\n",
    "print(carre_etirement)\n",
    "print(\"\\nInterprétations :\")\n",
    "print(\"- Homothétie [[5,0],[0,5]] : agrandissement uniforme de facteur 5\")\n",
    "print(\"- Étirement [[3,0],[0,4]] : étirement différent selon x (×3) et y (×4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9acc0-2d25-47c0-9192-76cb2663a2a8",
   "metadata": {},
   "source": [
    "### Exercice 6 : votre code !\n",
    "\n",
    "Votre mission est de réécrire me code précédent pour réaliser les mêmes transformations, mais vous dans ce cas sans avoir le droit de modifier vos vecteurs lignes. Cela vous demande de faire le produit matriciel un peu différemment.\n",
    "N’hésitez pas à poser le calcul pour un seul point sur papier pour bien voir quelles opérations faire et dans quel ordre (vous avez par contre le droit de modifier la matrice de transformation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71499002-86be-4092-8385-802ab6ed5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 6 : là, c’est votre code !\n",
    "\n",
    "# Application de la transformation par homothétie\n",
    "carre_homothetie = # VOTRE CODE\n",
    "\n",
    "# Application de la transformation par étirement\n",
    "carre_etirement = # VOTRE CODE\n",
    "\n",
    "# Fermeture des polygones pour le tracé (retour au premier point)\n",
    "carre_original_ferme = np.vstack([carre_original, carre_original[0]])\n",
    "carre_homothetie_ferme = np.vstack([carre_homothetie, carre_homothetie[0]])\n",
    "carre_etirement_ferme = np.vstack([carre_etirement, carre_etirement[0]])\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Tracé du carré original en bleu\n",
    "plt.plot(carre_original_ferme[:, 0], carre_original_ferme[:, 1], \n",
    "         'o-', color='blue', linewidth=2, markersize=8, label='Carré original')\n",
    "\n",
    "# Tracé du carré avec homothétie en rouge\n",
    "plt.plot(carre_homothetie_ferme[:, 0], carre_homothetie_ferme[:, 1], \n",
    "         'o-', color='red', linewidth=2, markersize=8, label='Homothétie [[5, 0], [0, 5]]')\n",
    "\n",
    "# Tracé du carré avec étirement en vert\n",
    "plt.plot(carre_etirement_ferme[:, 0], carre_etirement_ferme[:, 1], \n",
    "         'o-', color='green', linewidth=2, markersize=8, label='Étirement [[3, 0], [0, 4]]')\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Transformations géométriques par produit matriciel\\nHomothétie et étirement', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "\n",
    "# Affichage des axes passant par l'origine\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Ajustement des limites pour bien voir les deux carrés avec aspect ratio égal\n",
    "plt.xlim(-1, 16)\n",
    "plt.ylim(-1, 16)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=== Transformations matricielles ===\\n\")\n",
    "print(\"Carré original :\")\n",
    "print(carre_original)\n",
    "print(\"\\n1. Matrice d'homothétie :\")\n",
    "print(matrice_homothetie)\n",
    "print(\"\\nCarré après homothétie :\")\n",
    "print(carre_homothetie)\n",
    "print(\"\\n2. Matrice d’étirement :\")\n",
    "print(matrice_etirement)\n",
    "print(\"\\nCarré après étirement :\")\n",
    "print(carre_etirement)\n",
    "print(\"\\nInterprétations :\")\n",
    "print(\"- Homothétie [[5,0],[0,5]] : agrandissement uniforme de facteur 5\")\n",
    "print(\"- Étirement [[3,0],[0,4]] : étirement différent selon x (×3) et y (×4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33149e33-5248-4c1f-9914-7a35685395a1",
   "metadata": {},
   "source": [
    "### Cisaillement (*shearing*)\n",
    "\n",
    "On peut très bien se demander ce qu’il peut arriver si on place un facteur d’échelle non pas sur la diagonale de la matrice identité, mais à l’extérieur :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & S_x \\\\\n",
    "                0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dans ce cas le calcul est le suivant :\n",
    "\n",
    "$$  \n",
    "    \\begin{bmatrix} 1 & S_x \\\\\n",
    "                0 & 1 \n",
    "    \\end{bmatrix} · \n",
    "    \\begin{bmatrix} x \\\\ \n",
    "                    y\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} 1·x + S_x·y \\\\\n",
    "                0·x + 1·y \n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix} x + S_x·y \\\\\n",
    "                     y \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Exercice 7 : Quel est le résultat ?\n",
    "\n",
    "Affichez le résultat de cette transformation avec $S_x = 5$ et testez un facteur $S_y = 5$ également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa948866-9b23-48b9-bdf6-6b4c9eac8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7 : votre code !\n",
    "\n",
    "# Définition des sommets du carré original (chaque ligne est un point [x, y])\n",
    "carre_original = np.array([[1, 1], \n",
    "                           [1, 3], \n",
    "                           [3, 3], \n",
    "                           [3, 1]])\n",
    "\n",
    "# Matrice de transformation : cisaillement\n",
    "matrice_cisaillementX = # VOTRE CODE\n",
    "\n",
    "# Matrice d’étirement\n",
    "matrice_cisaillementY = # VOTRE CODE\n",
    "\n",
    "# Application de la transformation par homothétie\n",
    "carre_cisaillementX= # VOTRE CODE\n",
    "\n",
    "# Application de la transformation par étirement\n",
    "carre_cisaillementY = # VOTRE CODE\n",
    "\n",
    "# Fermeture des polygones pour le tracé (retour au premier point)\n",
    "carre_original_ferme = np.vstack([carre_original, carre_original[0]])\n",
    "carre_cisaillementX_ferme = np.vstack([carre_cisaillementX, carre_cisaillementX[0]])\n",
    "carre_cisaillementY_ferme = np.vstack([carre_cisaillementY, carre_cisaillementY[0]])\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Tracé du carré original en bleu\n",
    "plt.plot(carre_original_ferme[:, 0], carre_original_ferme[:, 1], \n",
    "         'o-', color='blue', linewidth=2, markersize=8, label='Carré original')\n",
    "\n",
    "# Tracé du carré avec homothétie en rouge\n",
    "plt.plot(carre_cisaillementX_ferme[:, 0], carre_cisaillementX_ferme[:, 1], \n",
    "         'o-', color='red', linewidth=2, markersize=8, label='Cisaillement x [[1, 5], [0, 1]]')\n",
    "\n",
    "# Tracé du carré avec étirement en vert\n",
    "plt.plot(carre_cisaillementY_ferme[:, 0], carre_cisaillementY_ferme[:, 1], \n",
    "         'o-', color='green', linewidth=2, markersize=8, label='Cisaillement y[[1, 0], [5, 1]]')\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Transformations géométriques par produit matriciel\\nCisaillement axes x et y', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "\n",
    "# Affichage des axes passant par l'origine\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Ajustement des limites pour bien voir les deux carrés avec aspect ratio égal\n",
    "plt.xlim(-1, 20)\n",
    "plt.ylim(-1, 20)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=== Transformations matricielles ===\\n\")\n",
    "print(\"Carré original :\")\n",
    "print(carre_original)\n",
    "print(\"\\n1. Matrice de cisaillement selon x :\")\n",
    "print(matrice_cisaillementX)\n",
    "print(\"\\nCarré après cisaillement selon x :\")\n",
    "print(carre_cisaillementX)\n",
    "print(\"\\n2. Matrice de cisaillement selon y :\")\n",
    "print(matrice_cisaillementY)\n",
    "print(\"\\nCarré après cisaillement selon y :\")\n",
    "print(carre_cisaillementY)\n",
    "print(\"\\nInterprétations :\")\n",
    "print(\"- Cisaillement selon x [[1,5],[0,1]] : agrandissement uniforme de facteur 5\")\n",
    "print(\"- cisaillement selon y [[1,0],[5,1]] : étirement différent selon x (×3) et y (×4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ffe07-62ad-4591-94d1-a947332fccd5",
   "metadata": {},
   "source": [
    "On appele ces transformation « cisaillement » (*shearing*) pour une raison évidente : la figure semble étirée selon un axe mais avec une déformation caractéristique.\n",
    "\n",
    "Pour le cisaillement selon x, on constate que plus y devient grand, plus x est affecté par la transformation (et vice-versa), ce qui crée la déformation. Il en est de même pour le cisaillement selon y. La déformation est proportionnelle selon la valeur de l’autre axe.\n",
    "\n",
    "### Renversement selon les axes (miroir)\n",
    "\n",
    "Revenons à la matrice identité et remplaçons l’un des « 1 » par un « -1 » :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -1 & 0 \\\\\n",
    "                0 & 1 \n",
    "\\end{bmatrix}. \n",
    "\\begin{bmatrix} x \\\\ \n",
    "                y\n",
    "\\end{bmatrix} =\n",
    "    \\begin{bmatrix} -1·x + 0·y \\\\\n",
    "                    x.0 + y.1 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} -x \\\\ \n",
    "                y \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Le résultat est un renversement horizontal (symétrie par rapport à l’axe y).\n",
    "\n",
    "La matrice pour un renversement vertical :\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 0 \\\\\\ \n",
    "                0 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Vous pouvez, si vous le désirez, utiliser `matplotlib` pour réaliser ces transformations, mais leur simplicité les rend faciles à visualiser. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c6f97-712c-4cf8-8ca2-5f862ac8c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus : codez une illustration des renversements si vous le souhaitez\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386c873-2913-4e11-ad68-35d9b690268d",
   "metadata": {},
   "source": [
    "### Rotation\n",
    "\n",
    "On peut également réaliser des transformations qui correspondent à des rotations avec des matrices. La démonstration est plus longue, et ce cours n’est pas un cours d’infographie, je vais donc me contenter de vous donner la formule pour une matrice de rotation dans le sens horaire. Mais si vous voulez approfondir le sujet, voici un dépôt qui contient la démonstration https://github.com/Jehadel/2D_transformations_demo\n",
    "\n",
    "La matrice de transformation dans le sens horaire est celle-ci : \n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\ \n",
    "    - \\sin \\theta & \\cos \\theta \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Exercice 8 : rotation de 30° dans le sens horaire\n",
    "\n",
    "Attention, les calculs se font en radian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259c525-4a06-43ae-9e9d-3130b88cd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 8 : votre code \n",
    "\n",
    "\n",
    "# Définition des sommets du carré original (chaque ligne est un point [x, y])\n",
    "carre_original = np.array([[1, 1], \n",
    "                           [1, 3], \n",
    "                           [3, 3], \n",
    "                           [3, 1]])\n",
    "\n",
    "# Angle de rotation en degrés et conversion en radians\n",
    "theta_deg = # VOTRE CODE\n",
    "theta_rad = # VOTRE CODE\n",
    "\n",
    "# Matrice de rotation de 30° dans le sens horaire\n",
    "matrice_rotation = # VOTRE CODE\n",
    "\n",
    "# Application de la transformation par rotation\n",
    "carre_rotation = # VOTRE CODE\n",
    "\n",
    "# Fermeture des polygones pour le tracé (retour au premier point)\n",
    "carre_original_ferme = np.vstack([carre_original, carre_original[0]])\n",
    "carre_rotation_ferme = np.vstack([carre_rotation, carre_rotation[0]])\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Tracé du carré original en bleu\n",
    "plt.plot(carre_original_ferme[:, 0], carre_original_ferme[:, 1], \n",
    "         'o-', color='blue', linewidth=2, markersize=8, label='Carré original')\n",
    "\n",
    "# Tracé du carré après rotation en rouge\n",
    "plt.plot(carre_rotation_ferme[:, 0], carre_rotation_ferme[:, 1], \n",
    "         'o-', color='red', linewidth=2, markersize=8, label=f'Rotation de {theta_deg}°')\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title(f'Transformation géométrique : Rotation de {theta_deg}°', \n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "\n",
    "# Affichage des axes passant par l'origine\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Ajustement des limites pour bien voir les deux carrés avec aspect ratio égal\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=== Transformation matricielle : Rotation ===\\n\")\n",
    "print(\"Carré original :\")\n",
    "print(carre_original)\n",
    "print(f\"\\nAngle de rotation : {theta_deg}° (sens trigonométrique)\")\n",
    "print(f\"Angle en radians : {theta_rad:.4f} rad\")\n",
    "print(\"\\nMatrice de rotation :\")\n",
    "print(matrice_rotation)\n",
    "print(\"\\nCarré après rotation :\")\n",
    "print(carre_rotation)\n",
    "print(\"\\nInterprétation : Cette matrice effectue une rotation de 30° dans le sens\")\n",
    "print(\"anti-horaire (trigonométrique) autour de l'origine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c06103-bac2-4999-98fd-f7584012791e",
   "metadata": {},
   "source": [
    "### Composition\n",
    "\n",
    "Pourquoi est-il particulièrement intéressant de réaliser les transformations dans le plan en multipliant par une matrice ?\n",
    "\n",
    "Tout simplement car on va pouvoir enchaîner les transformations (les composer) en enchaînant la même  opération : multiplier par une matrice carrée. Par exemple multiplier par une matrice d’homothétie, puis une matrice de rotation, puis une matrice de renversement correspondra à la réalisation de ces trois transformation. Mais, encore mieux si on multiplie ces trois matrices entre elles pour obtenir une seule matrice à la fin, alors cette matrice réalisera les trois opérations en multipliant par une seule matrice.\n",
    "\n",
    "### Exercice 9 : composition\n",
    "\n",
    "Écrivez un bout de code qui réalise les transformations qui permettent d’obtenir la figure suivante (vous pouvez déduire la valeur du cisaillement d’après la figure):\n",
    "\n",
    "![Exercice de composition de transformation](./images/CompositionTransformations.png)\n",
    "\n",
    "Réalisez cette transformation en utilisant dans un premier temps 3 matrices, puis répétez la transformation en utilisant une seule matrice. Comparez.\n",
    "\n",
    "Attention, l’ordre des opérations est très important !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f0ebb-2257-4ae7-86f2-a11596b564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerice 9 : Votre code !\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définition des sommets du carré original\n",
    "carre_original = np.array([[1, 1], \n",
    "                           [1, 3], \n",
    "                           [3, 3], \n",
    "                           [3, 1]])\n",
    "\n",
    "# Définition des matrices de transformation\n",
    "# 1. Shearing (cisaillement)\n",
    "matrice_shearing = # VOTRE CODE\n",
    "\n",
    "# 2. Flip vertical (renversement vertical)\n",
    "matrice_flip = # VOTRE CODE\n",
    "\n",
    "# 3. Rotation de 45° dans le sens horaire\n",
    "theta_deg = # VOTRE CODE\n",
    "theta_rad = # VOTRE CODE\n",
    "matrice_rotation = # VOTRE CODE\n",
    "\n",
    "# Application successive des transformations\n",
    "# Étape 1 : Shearing\n",
    "carre_shearing = # VOTRE CODE\n",
    "\n",
    "# Étape 2 : Shearing puis Flip\n",
    "carre_shearing_flip = # VOTRE CODE\n",
    "\n",
    "# Étape 3 : Shearing puis Flip puis Rotation (composition complète)\n",
    "carre_final = # VOTRE CODE\n",
    "\n",
    "# Calcul de la matrice de composition (produit des trois matrices)\n",
    "matrice_composee = # VOTRE CODE\n",
    "\n",
    "# Vérification : application directe de la matrice composée\n",
    "carre_final_direct = # VOTRE CODE\n",
    "\n",
    "# Fermeture des polygones pour le tracé\n",
    "carre_original_ferme = np.vstack([carre_original, carre_original[0]])\n",
    "carre_shearing_ferme = np.vstack([carre_shearing, carre_shearing[0]])\n",
    "carre_shearing_flip_ferme = np.vstack([carre_shearing_flip, carre_shearing_flip[0]])\n",
    "carre_final_ferme = np.vstack([carre_final, carre_final[0]])\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Tracé des différentes étapes\n",
    "plt.plot(carre_original_ferme[:, 0], carre_original_ferme[:, 1], \n",
    "         'o-', color='blue', linewidth=2, markersize=8, label='Original', alpha=0.7)\n",
    "\n",
    "plt.plot(carre_shearing_ferme[:, 0], carre_shearing_ferme[:, 1], \n",
    "         'o-', color='green', linewidth=2, markersize=8, label='Après shearing', alpha=0.7)\n",
    "\n",
    "plt.plot(carre_shearing_flip_ferme[:, 0], carre_shearing_flip_ferme[:, 1], \n",
    "         'o-', color='orange', linewidth=2, markersize=8, label='Après flip vertical', alpha=0.7)\n",
    "\n",
    "plt.plot(carre_final_ferme[:, 0], carre_final_ferme[:, 1], \n",
    "         'o-', color='red', linewidth=2.5, markersize=10, label='Après rotation (final)', alpha=0.9)\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Composition de transformations matricielles\\nShearing → Flip vertical → Rotation 45°', \n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='upper left')\n",
    "\n",
    "# Affichage des axes passant par l'origine\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Ajustement des limites\n",
    "plt.xlim(-4, 10)\n",
    "plt.ylim(-10, 6)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPOSITION DE TRANSFORMATIONS MATRICIELLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nCarré original :\")\n",
    "print(carre_original)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ÉTAPE 1 : SHEARING (cisaillement)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Matrice :\")\n",
    "print(matrice_shearing)\n",
    "print(\"\\nCarré après shearing :\")\n",
    "print(carre_shearing)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ÉTAPE 2 : FLIP VERTICAL (renversement)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Matrice :\")\n",
    "print(matrice_flip)\n",
    "print(\"\\nCarré après shearing + flip :\")\n",
    "print(carre_shearing_flip)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ÉTAPE 3 : ROTATION 45° (sens horaire)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Matrice :\")\n",
    "print(matrice_rotation)\n",
    "print(\"\\nCarré final :\")\n",
    "print(carre_final)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPOSITION : Matrice unique équivalente\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Matrice composée = Rotation × Flip × Shearing\")\n",
    "print(matrice_composee)\n",
    "\n",
    "print(\"\\nVérification : application directe de la matrice composée\")\n",
    "print(\"(doit donner le même résultat que les étapes successives)\")\n",
    "print(carre_final_direct)\n",
    "\n",
    "print(\"\\nDifférence (doit être proche de zéro) :\")\n",
    "print(np.max(np.abs(carre_final - carre_final_direct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4b7e0-363e-4f9d-8e63-99d6712e20fd",
   "metadata": {},
   "source": [
    "### Pour aller plus loin\n",
    "\n",
    "Les transformations telles qu’on les a présentées ici ont quelques limites pour être utilisées pour les transformations géométriques appliquées, comme la manipulation d’image (infographie). En effet on constate :\n",
    "* pour la mise à l’échelle/homothétie : la figure augmente/diminue de taille, mais cela affecte également sa position, on aimerait pour voir modifier sa taille en la maintenant à la même position\n",
    "* pour le cisaillement, le problème est le même, la figure est étirée mais cela provoque aussi un déplacement de cette figure\n",
    "* les renversements se font toujours par rapport aux axes (et non par rapport à un axe de référence sur la figure)\n",
    "* les rotations  se font toujours par rapport à l’origine des axes (et non par rapport à un point de référence sur la figure)\n",
    "* et la difficulté principale : on n’a pas trouvé de matrice de transformation qui réalisait une translation (pour cela on fait plutôt des additions)\n",
    "\n",
    "Il y a un moyen de résoudre ces problèmes, par une petite astuce qui consiste à utiliser des vecteurs à 3 dimensions et des matrices (3, 3) même si on en 2D : cette dimension supplémentaire nous permet d’introduire des termes constants qui vont permettre de réaliser des calculs plus libre du point de vue de la translation et des points de référence. Le principe sera d’ailleurs le même en 3D, où va utiliser des matrices (4, 4) et des vecteurs de dimension 4 : ce sont les fameux quaternions que nous avons déjà évoqués et étudiés par Hamilton au XIXe siècle.\n",
    "\n",
    "Mais il ne s’agit pas d’un cours d’infographie, vous pourrez étancher votre curiosité sur ce dépôt si le sujet vous intéresse : https://github.com/Jehadel/3x3-2D-transformations_demo\n",
    "\n",
    "En ce qui nous concerne, poursuivons notre programme d’algèbre linéaire avec les transformations linéaires et les projections, concepts qui seront plus faciles à appréhender après tout ce temps passé à réaliser des transformations géométriques avec des matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d506db-86ca-4be2-8a75-a68700b80ade",
   "metadata": {},
   "source": [
    "## Transformations linéaires, projections et bases\n",
    "\n",
    "### Transformation linéaire\n",
    "\n",
    "Après toutes les manipulations géométriques que nous venons de réaliser, la notion de transformation linéaire devrait vous être connu intuitivement.\n",
    "\n",
    "#### Définition\n",
    "\n",
    "En réalité, une **transformation linéaire** est une fonction $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ qui peut s'écrire comme un produit matrice-vecteur :\n",
    "\n",
    "$$y = Ax$$\n",
    "\n",
    "où $A$ est une matrice $m \\times n$.\n",
    "\n",
    "On appelle parfois les transformations linéaires des transformation matricielle.\n",
    "\n",
    "C’est ce que nous venons de faire dans le plan avec des matrices carrées, qui transformaient un vecteur du plan en un autre vecteur du plan. Une matrice de dimension (m, n) va transformer un vecteur de dimension n en vecteur de dimension m. Nous avons simplement généralisé la notion. On parle de transformation linéaire car elle possède cetaines propriétés.\n",
    "\n",
    "### Propriétés de linéarité\n",
    "\n",
    "Une transformation $T$ est linéaire si elle satisfait :\n",
    "1. $T(u + v) = T(u) + T(v)$ (additivité)\n",
    "2. $T(\\alpha v) = \\alpha T(v)$ (homogénéité)\n",
    "\n",
    "Ce sont des propriétés que vérifie le produit matrice-vecteur.\n",
    "\n",
    "Ci-dessous des bouts de code qui produisent d’autres illustrations de ce principe par des transformations dans le plan qu’on a vu plus dans la section précédente (rotation, homothétie, cisaillement) mais qui met en avant le fait que ces transformations s’appliquent à tous les vecteurs du plan :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ba48d-501f-410e-a34e-db594682f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de transformations linéaires en 2D\n",
    "\n",
    "def plot_transformation(A, title):\n",
    "    \"\"\"Visualiser l'effet d'une transformation linéaire\"\"\"\n",
    "    # Créer une grille de points\n",
    "    x = np.linspace(-2, 2, 20)\n",
    "    y = np.linspace(-2, 2, 20)\n",
    "    X_grid, Y_grid = np.meshgrid(x, y)\n",
    "    \n",
    "    # Points originaux\n",
    "    points = np.vstack([X_grid.ravel(), Y_grid.ravel()])\n",
    "    \n",
    "    # Appliquer la transformation\n",
    "    transformed = A @ points\n",
    "    \n",
    "    # Créer la figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Avant transformation\n",
    "    ax1.scatter(points[0], points[1], alpha=0.5, s=20)\n",
    "    ax1.set_xlim(-5, 5)\n",
    "    ax1.set_ylim(-5, 5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_title('Avant transformation')\n",
    "    ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Après transformation\n",
    "    ax2.scatter(transformed[0], transformed[1], alpha=0.5, s=20, color='red')\n",
    "    ax2.set_xlim(-5, 5)\n",
    "    ax2.set_ylim(-5, 5)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_title(f'Après transformation : {title}')\n",
    "    ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1. Rotation de 45°\n",
    "theta = np.pi/4\n",
    "R = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta),  np.cos(theta)]\n",
    "])\n",
    "print(\"Matrice de rotation (45°) :\")\n",
    "print(R)\n",
    "plot_transformation(R, \"Rotation 45°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16d02b-60be-4a86-962b-4ca20d01b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Homothétie (scaling)\n",
    "S = np.array([\n",
    "    [2, 0],\n",
    "    [0, 0.5]\n",
    "])\n",
    "print(\"Matrice d'homothétie :\")\n",
    "print(S)\n",
    "plot_transformation(S, \"Étirement (×2 en x, ×0.5 en y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea1b7f-206c-4f56-8277-52dcfb58064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Cisaillement (shear)\n",
    "Shear = np.array([\n",
    "    [1, 0.5],\n",
    "    [0, 1]\n",
    "])\n",
    "print(\"Matrice de cisaillement :\")\n",
    "print(Shear)\n",
    "plot_transformation(Shear, \"Cisaillement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e1ea0-fc11-4f08-ac1f-998f1f7c2a33",
   "metadata": {},
   "source": [
    "Mais si ces illustrations, parce que dans le plan, sont faciles à produire et assez parlantes, il ne faut pas qu’elle nourrissent la croyance qu’une transformation linéaire transforme seulement un vecteur en un autre vecteur du même espace. Par exemple la matrice :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}1 & 0 \\\\\n",
    "               0 & 1 \\\\\n",
    "               1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "de dimension (3, 2) transforme un vecteur de dimension 2, soit une  matrice (2, 1), en un vecteur de dimension 3, soit une matrice (3,1) :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 0 \\\\\n",
    "               0 & 1 \\\\\n",
    "               1 & 0\n",
    "\\end{bmatrix} ·\n",
    "\\begin{bmatrix}x \\\\\n",
    "               y \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} 1·x + 0·y \\\\\n",
    "               0·x + 1·y \\\\\n",
    "               1·x + 0.y\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} x \\\\\n",
    "               y \\\\\n",
    "               x\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "C’est aussi une transformation linéaire.\n",
    "\n",
    "Ensuite, il existe des transformation qui ne sont pas linéaires. Supposons une transformation qui transforme un vecteur à 2 dimensions en un autre vecteur de dimension 2 de la manière suivante :\n",
    "$$\n",
    "\\begin{bmatrix}x \\\\\n",
    "               y \n",
    "\\end{bmatrix} \\longmapsto\n",
    "\\begin{bmatrix}x^3 \\\\\n",
    "               e^y \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Ce ne sera pas une transformation linéaire, et aucune matrice ne permettra de transformer $x$ en $x^3$ et $y$ en $e^y$.\n",
    "\n",
    "En fait faire un produit matriciel c’est exprimer les composantes d’un vecteur comme une somme de produit par des coefficients, c’est créer un vecteur dont les composantes sont une combinaison linéaire (ce qui n’est pas le cas dans le contre-exemple précédent). Alors, comment savoir si une transformation donnée est linéaire  (donc réalisable par une matrice) ? Il suffit de vérifier les propriété d’additivité et d’homégénéité vues précédemment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff4248-8f68-487a-bde8-da443b7121da",
   "metadata": {},
   "source": [
    "### Bases et projections\n",
    "\n",
    "La notion de projection va être extrêmement importante pour la suite de ce cours où notre objectif est de coder un algorithme de réduction de dimensions (analyse en composantes principales) mais c’est aussi une notion importante pour le machine learning en général. \n",
    "\n",
    "Pour le dire en langage ordinaire, la **projection** consiste à \"rabattre\" un point sur un sous-espace (une droite, un plan, etc.).\n",
    "\n",
    "Dans notre propos, la notion de projection sera fortement liée à celle de base.\n",
    "\n",
    "La base que nous connaissons intuitivement ou parce que nous l’avons souvent manipulée au lycée (et normalement au collège aussi) est la base orthonormée du plan, avec les fameux axes x et y\n",
    "\n",
    "![Base orthonormée et projection de vecteurs](./images/BaseOrthonormee.png)\n",
    "\n",
    "Comme bien des fois nous allons voir que l’algèbre linéaire pose un cadre qui permet de généraliser des notions intuitives ou courantes qui ne se révèlent souvent être que des cas particuliers.\n",
    "\n",
    "Nous avons l’habitude d’utiliser les coordonnées cartésiennes, et nous l’avons fait jusqu’ici : nous avons représenté les vecteurs par un couple de valeur x et y, coordonnées dans un repère orthonormé du plan.\n",
    "Mais un vecteur peut être caractérisé d’une autre manière : sa norme (longueur) et l’angle θ qu‘il fait avec l’un des axes. En écrivant les choses de cette manière, on se rend compte la valeur de x et y correspond à en fait à la projection du vecteur respectivement sur les axes x et y du plan, que l’on peut exprimer en fonction de la norme du vecteur et de l’angle θ.\n",
    "\n",
    "Nous avons vu au chapitre précédent que l’on peut calculer une projection à l’aide du produit scalaire entre deux vecteurs : \n",
    "* le produit scalaire retournt un scalaire qui indique la longueur du vecteur obtenu après projection\n",
    "* la direction du vecteur obtenu après projection est celle du vecteur sur lequel il est projeté : il suffit donc de multiplier ce dernier vecteur par le résultat du produit scalaire pour obtenir le vecteur après projection \n",
    "\n",
    "Ici, pour calculer les projections sur les axes x et y, nous allons – comme l’illustre le schéma ci-dessus – considérer des vecteurs unitaires (dont la norme est 1) et dont les directions sont celles des axes x et y, respectivement. Le fait que la norme de ces vecteurs vaut 1 fait qu’elle va être « transparente » dans le calcul des produits scalaires (ils n’affecteront pas la norme du nouveau vecteur), qui permettront de calculer les projections qui correspondront aux coordonnées du vecteur projeté sur les axes x et y que nous auront choisi.\n",
    "\n",
    "Soit $u$ un **vecteur unitaire** ($\\|u\\| = 1$).\n",
    "\n",
    "La **projection** d'un vecteur $v$ sur $u$ est :\n",
    "\n",
    "$$\\text{proj}_u(v) = \\langle v, u \\rangle u = (v^T u) u $$\n",
    "\n",
    "- **Composante scalaire** : $v^T u$ (longueur de la projection)\n",
    "- **Vecteur projeté** : $(v^T u) u$\n",
    "\n",
    "Voici un petit exercice pour illustrer cela, avec tout d’abord, une fonction qui calcule la projection d’un  vecteur quelconque sur un vecteur unitaire donné :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c35cd-1fdc-44ce-99b9-e72a8dea5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection d’un  vecteur v sur un vecteur unitaire u\n",
    "\n",
    "def project_on_vector(v, u):\n",
    "    \"\"\"\n",
    "    Projeter v sur le vecteur unitaire u\n",
    "    \"\"\"\n",
    "    # Normaliser u (au cas où)\n",
    "    u = u / np.linalg.norm(u)\n",
    "    \n",
    "    # Composante scalaire (longueur de la projection)\n",
    "    scalar_proj = np.dot(v, u)\n",
    "    \n",
    "    # Vecteur projeté\n",
    "    vector_proj = scalar_proj * u\n",
    "    \n",
    "    return scalar_proj, vector_proj\n",
    "\n",
    "# Exemple\n",
    "v = np.array([3, 4])\n",
    "u = np.array([1, 0])  # projeter sur l'axe x\n",
    "\n",
    "scalar, proj = project_on_vector(v, u)\n",
    "print(f\"Vecteur v : {v}\")\n",
    "print(f\"Direction u : {u}\")\n",
    "print(f\"\\nComposante scalaire : {scalar}\")\n",
    "print(f\"Vecteur projeté : {proj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df67dca-4d84-4b88-b959-4d3c8d6c39b4",
   "metadata": {},
   "source": [
    "### Exercice 10 : visualiser les projections\n",
    "À l’aide de la fonction `project_on_vector()` précédente, réalisez une fonction pour visualiser les projections de vecteur (v et u, u est par défaut un vecteur unité de l’axe x)\n",
    "* Cette fonction prend en arguments deux vecteurs v et et u (dimension 2, par défaut u sera le vecteur [1, 0]))\n",
    "* Elle calcule la projection de v sur u\n",
    "* Elle affiche sur une figure matplotlib :\n",
    "    * Le vecteur v (en bleu)\n",
    "    * Le vecteur u (en vert)\n",
    "    * Le vecteur résultant de la projection (en rouge)\n",
    "    * Une « ligne de projection » en pointillée (méthode `plt.plot`avec l’argument `linestyle = '--'`), pour aider à visualiser la projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8065a-60bf-449d-9ac6-7e74346f80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerice : Visualisation de la projection\n",
    "\n",
    "def visualize_projection(#VOTRE CODE)):\n",
    "    \"\"\"Visualiser la projection de x sur u\"\"\"\n",
    "    u = u / np.linalg.norm(u) \n",
    "    scalar, proj = # VOTRE CODE\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Vecteur original v (avec quiver)\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Projection\n",
    "   # VOTRE CODE\n",
    "\n",
    "        # Direction de projection, soit le vecteur u (on l’affiche en dernier pour mieux la voir)\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Ligne de projection (perpendiculaire en pointillée)\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    plt.xlim(-1, 5)\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.title('Projection', fontsize=14)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# Exemple 1 : projection sur l'axe x\n",
    "x = np.array([3, 4])\n",
    "u = np.array([1, 0])\n",
    "visualize_projection(x, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962c52d-944d-482c-9708-c5b7d382e9a6",
   "metadata": {},
   "source": [
    "### 💡 Exercice 11 : Projections\n",
    "\n",
    "Soit $v = [2, 3]$ et deux directions :\n",
    "- $u_1 = [1, 0]$ (axe x)\n",
    "- $u_2 = [0, 1]$ (axe y)\n",
    "\n",
    "- a) Calculez la projection de $v$ sur $u_1$\n",
    "- b) Calculez la projection de $v$ sur $u_2$\n",
    "- c) Vérifiez que $\\text{proj}_{u_1}(v) + \\text{proj}_{u_2}(v) = v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aed822-1d37-4ad1-8724-25aa08d3e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 11 : Votre code ! \n",
    "\n",
    "v = np.array([2, 3])\n",
    "u1 = np.array([1, 0])\n",
    "u2 = np.array([0, 1])\n",
    "\n",
    "print(f\"v = {v}\")\n",
    "\n",
    "# a) Projection sur u1\n",
    "\n",
    "\n",
    "# b) Projection sur u2\n",
    "\n",
    "\n",
    "# c) Vérification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80969069-3379-49bc-9bd5-4883f1e6b3bd",
   "metadata": {},
   "source": [
    "### Projection matricielle\n",
    "\n",
    "La projection sur un vecteur unitaire $u$ peut aussi s'écrire avec une matrice :\n",
    "\n",
    "$$\n",
    "P = u·u^T \\\\\n",
    "\\text{Alors : }P·v = u·u^T·v = (u^T·v)·u = Proj_u(v)\n",
    "$$\n",
    "\n",
    "**Propriété importante** : $P^2 = P$ (idempotence) : une fois qu’un vecteur a été projeté, la même projection a pour résultat le vecteur déjà projeté (vu que les aures composantes ont disparu dans l’opération).\n",
    "\n",
    "Une matrice de projection, appliquée deux fois, donne le même résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc23af5-0f08-4130-b87a-204fab0d7a7d",
   "metadata": {},
   "source": [
    "Tout ce que nous venons de voir avec les axes du repère cartésien classique devrait nous interroger : si les coordonnées d’un vecteur sont en fait les projections sur d’autres vecteurs (unitaires) qui caractérisent un repère, en fait on pourrait très bien calculer les coordonnées d’un même vecteur dans différents repères, chacun  caractérisé par une paire (en 2D) de vecteurs unitaires associés. Et en fait rien n’oblige à ce que ces vecteurs soit orientés de telle ou telle manière, leur orientation peut être quelconque. Un vecteur unitaire orienté à 45° dans un repère donné peut très bien définir l’axe d’un autre repère sur lequel on peut parfaitement réaliser des projections et donc calculer de nouvelles coordonnées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503058f-28b8-4979-a849-05ebb8178f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection sur une direction quelconque\n",
    "v = np.array([3, 4])\n",
    "u = np.array([1, 1]) / np.sqrt(2)  # direction à 45°\n",
    "visualize_projection(v, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2759f-d8e7-43d9-95cc-6e615fe9b2d7",
   "metadata": {},
   "source": [
    "Voilà qui nous amène à la notion de base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2b395-fdac-4499-8e88-051ab6b4e6b6",
   "metadata": {},
   "source": [
    "### Base orthonormée ou canonique\n",
    "\n",
    "Si nous poussons notre réflexion précédente un peu plus loin, on peut très bien imaginer que l’on puisse exprimer les coordonnées des vecteurs dans des repères basés sur une paire de vecteurs quelconques, qui pointent dans des directions différentes. Par exemple : \n",
    "\n",
    "![Base quelconque](./images/BaseQuelconque.png)\n",
    "\n",
    "Si ces vecteurs qui forment la base ne sont pas orthogonaux, alors la projection de l’un sur l’autre est différente du vecteur nul. Cela signifique que la projection sur l’un de ces vecteurs aurant une composante projetable sur l’autre vecteur, et donc que les composantes des vecgteurs ne peuvent pas être linéairement indépendantes (une variation de l’une, modifiera immanquablement les autres).\n",
    "\n",
    "Par contre, si ces vecteurs qui forment la base sont orthogonaux, alors la projection sur l’un des vecteur sera totalement indépendante de celle sur l’autre vecteur (vu que le produit scalaire de deux vecteurs orthogonaux est nul), la somme des projections sera égale au vecteur projeté (ce que nous avons vu dans l’exercice 11). On parlera de base orthonormée (ou canonique) définie sur $\\mathbb{R}^n$ comme suit :\n",
    "\n",
    "$$\n",
    "v =\n",
    "\\begin{bmatrix} v_1 \\\\\n",
    "                v_2 \\\\\n",
    "                v_3 \\\\\n",
    "                \\vdots \\\\\n",
    "                v_n\n",
    "\\end{bmatrix} = v_1·e_1 + v_2·e_2 + v_3·e_3 + … + v_n·e_n \\\\\n",
    "\\text{avec }\n",
    "e_1 = \\begin{bmatrix} 1 \\\\\n",
    "                0 \\\\\n",
    "                0 \\\\\n",
    "                \\vdots \\\\\n",
    "                0\n",
    "\\end{bmatrix},\n",
    "e_2 = \\begin{bmatrix} 0 \\\\\n",
    "                1 \\\\\n",
    "                0 \\\\\n",
    "                \\vdots \\\\\n",
    "                0\n",
    "\\end{bmatrix},\n",
    "e_3 = \\begin{bmatrix} 0 \\\\\n",
    "                0 \\\\\n",
    "                1 \\\\\n",
    "                \\vdots \\\\\n",
    "                0\n",
    "\\end{bmatrix}, \n",
    "…, \n",
    "e_n = \\begin{bmatrix} 0 \\\\\n",
    "                0 \\\\\n",
    "                0 \\\\\n",
    "                \\vdots \\\\\n",
    "                1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Une écriture plus rigoureuse pour définir la base serait celle-ci :\n",
    "\n",
    "une **base orthonormée** est un ensemble de vecteurs $u_1, u_2, \\cdots, u_p$ qui sont :\n",
    "- **Orthogonaux** : $\\langle u_i, u_j \\rangle = 0$ si $i \\neq j$\n",
    "- **Normés** : $\\|u_i\\| = 1$\n",
    "\n",
    "On peut résumer ces conditions avec le **delta de Kronecker** (qui est une fonction particulière qui prend pour valeur 1 si certaines conditions sont réunies, 0 sinon) :\n",
    "\n",
    "$$\\langle u_i, u_j \\rangle = \\delta_{ij} = \\begin{cases} 1 & \\text{si } i = j \\\\ 0 & \\text{si } i \\neq j \\end{cases}$$\n",
    "\n",
    "Ce sont ces bases qui nous intéresseront dans la suite du cours, du fait de leurs propriétés au regard des transformations linéaires.\n",
    "\n",
    "Une propriété intéressante de l’orthogonalité va concerner les matrices :\n",
    "\n",
    "$A$ de dimension (n, n) est une matrice orthogonale si elle vérifie l’égalité : $A^T·A = I_n$\n",
    "\n",
    "En d’autres termes, l’inverse de $A$ n’est rien d’autres que sa transposée : $A^{-1} = A^T$\n",
    "\n",
    "### Changement de base\n",
    "\n",
    "#### Linéarité\n",
    "\n",
    "Nous venons de voir que les bases orthonormés permettaient une composition indépendamment linéaire des vecteurs.  \n",
    "\n",
    "Par ailleurs, rappelons qu’est linéaire toute transformations qui satisfait :\n",
    "* $T(u + v) = T(u) + T(v)$\n",
    "* $T(\\alpha v) = \\alpha T(v)$\n",
    "\n",
    "Rappelons aussi que les transformations matricielles vérifient ces conditions.\n",
    "\n",
    "De cette définition, nous pouvons déduire la propriété suivante :\n",
    "$$\n",
    "\\forall u, v \\in \\mathbb{R}^n, \\forall \\alpha, \\beta \\in \\mathbb{R} \\text{ :} \\qquad T(\\alpha·u + \\beta·v) = \\alpha·T(u) + \\beta·T(v)\n",
    "$$\n",
    "\n",
    "Il vient immédiatemment de ce que nous venons de voir plus haut : \n",
    "\n",
    "$$\n",
    "T(v) = T(v_1·e_1 + v_2·e_2 + v_3·e_3 + … + v_n·e_n) = v_1·T(e_1) + v_2·T(e_2) + v_3·T(e_3) + \\cdots + v_n·T(e_n)\n",
    "$$\n",
    "\n",
    "On constate donc que transformer un vecteur revient en fait à transformer sa base : on peut donc transformer facilement n’importe quel vecteur d’une base donnée.\n",
    "\n",
    "#### Matrices et changement de base\n",
    "\n",
    "Imaginons une matrice $A$, et concentrons nous sur ses $n$ colonnes $C_i$ :  \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "C_1 & C_2 & \\cdots & C_n \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Si on considère un vecteur $e_i$ d’une base orthonormée, on constate rapidement par le calcul du produit matriciel que :\n",
    "$$\n",
    "A·e_i = \\begin{bmatrix}\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "C_1 & C_2 & \\cdots & C_n \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "\\end{bmatrix} · \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1\\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix} = C_i\n",
    "$$\n",
    "\n",
    "En fait cela revient à sélectionner la i<sup>ème</sup> colonne de la matrice : l’action de la matrice (transformation linéaire) sur le i<sup>ème</sup> vecteur de la base est donné par la i<sup>ème</sup> colonne.\n",
    "\n",
    "Dans le même ordre d’idée, si on crée une matrice dont les colonnes sont constituées par les transformations linéaires d’une base canonique :\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "T(e_1) & T(e_2) & \\cdots & T(e_n) \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Alors tout simplement :\n",
    "\n",
    "$$\n",
    "T(v) = A·v\n",
    "$$\n",
    "\n",
    "Nous allons voir dans la prochaine section pourquoi les changements de base sont si important lorsque l’on va vouloir réaliser des réductions de dimension (si vous avez été assez attentif-ve, vous devez bien vous douter que réduire les dimensions = projeter vers un espace « réduit » en dimension). Ce que nous venons de voir – et qui peut vous sembler un peu abstrait – c’est que tout le but du jeux va être de trouver la « bonne » matrice pour passer d’une base à l’autre.\n",
    "\n",
    "Voyons cela de manière un peu opérationnelle avec un exercice en deux étapes (1) réaliser un changement de base à l’aide d’une opération matricielle, (2) visualiser ce changement.\n",
    "\n",
    "#### Exercice 12a : changement de base\n",
    "\n",
    "* soit la base canonique $e_1 = [1, 0]^T$ et $e_2 = [0, 1]^T$ de $\\mathbb{R}^2$\n",
    "* soit une nouvelle base $u_1 = [\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]^T$ et $u_2 = [\\frac{-1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]^T$ qui est simplement issue d’une rotation à 45° de la base (pour comprendre d’ou vient $\\sqrt{2}$ dessiner la diagonale d’un carré de côté 1 et appliquer pythagore pour connaître sa longueur)\n",
    "* soit un vecteur $v$ de coordonnée $[3, 4]^T$ dans la base canonique\n",
    "\n",
    "Réalisez les étapes suivantes :\n",
    "* vérifiez que $(u_1, u_2)$ est orthonormée\n",
    "* créez une matrice $U$ de changemnet de base à partir des vecteurs $u_1$ et $u_2$\n",
    "* calculez les coordonnées de $v$ dans la nouvelle base\n",
    "* faites l’opération inverse : à partir de ces nouvelles coordonnées, calculez les coordonnées de v dans la base canonique (c’est juste pour montrer qu’on peut faire l’opération dans les deux sens)\n",
    "* testez que l’on retombe bien sur le vecteur d’origine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830978e-5f02-43e0-8012-d1a676de5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changement de base - Exemple\n",
    "\n",
    "# Base canonique de ℝ²\n",
    "e1 = np.array([1, 0])\n",
    "e2 = np.array([0, 1])\n",
    "\n",
    "# Nouvelle base (rotation de 45°)\n",
    "u1 = np.array([1/np.sqrt(2), 1/np.sqrt(2)])\n",
    "u2 = np.array([-1/np.sqrt(2), 1/np.sqrt(2)])\n",
    "\n",
    "# Vecteur v dans la base canonique\n",
    "v = np.array([3, 4])\n",
    "print(f\"\\nv dans base canonique : {v}\")\n",
    "\n",
    "# Vérifier que (u1, u2) est une base orthonormée\n",
    "print(\"Vérification base orthonormée :\")\n",
    "print(f\"⟨u1, u1⟩ = {np.dot(u1, u1):.4f} (doit être 1)\")\n",
    "print(f\"⟨u2, u2⟩ = {np.dot(u2, u2):.4f} (doit être 1)\")\n",
    "print(f\"⟨u1, u2⟩ = {np.dot(u1, u2):.4f} (doit être 0)\")\n",
    "\n",
    "# Matrice de changement de base\n",
    "U = np.column_stack([u1, u2])\n",
    "print(\"\\nMatrice U (colonnes = vecteurs de la base) :\")\n",
    "print(U)\n",
    "\n",
    "\n",
    "# Coordonnées dans la nouvelle base\n",
    "new_coord = U.T @ v\n",
    "print(f\"Coordonnées dans nouvelle base : U^T·v = {new_coord}\")\n",
    "\n",
    "# Reconstruction\n",
    "v_reconstructed = U @ new_coord\n",
    "print(f\"Reconstruction : v = U·new_coord = {v_reconstructed}\")\n",
    "\n",
    "#test\n",
    "print(f\"Égal à l'original ? {np.allclose(v, v_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca83b11-020a-430d-85c7-fdf34d954490",
   "metadata": {},
   "source": [
    "#### Exercice 12b : visualisation\n",
    "\n",
    "Complètez le code ci-dessous pour visualiser le changement de base :\n",
    "\n",
    "* dans la première figure, complèter les méthodes `ax1.quiver()` pour afficher la base canonique en vert et le vecteur $v$ en bleu\n",
    "* dans la seconde figure, complèter les méthodes `ax2.quiver()` pour afficher la base après rotation de 45° en rouge et le vecteur transformé $v$ en bleu (à partir des coordonnées `new_coord` bien sûr)\n",
    "* dans la seconde figure, affichez également par des lignes en pointillé de couleur noire ou grise la projection du vecteur $v$ sur les deux axes définis par les vecteurs $u_1$ et $u_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670755dc-7d8d-44f6-8a98-eaf0cdc8ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du changement de base\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Base canonique\n",
    "ax1.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='green', width=0.008, label='e₁')\n",
    "ax1.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='green', width=0.008, label='e₂')\n",
    "ax1.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='blue', width=0.01, label=f'v = {v}')\n",
    "\n",
    "ax1.set_xlim(-2.5, 5)\n",
    "ax1.set_ylim(-2.5, 5)\n",
    "ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Base canonique', fontsize=13)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Nouvelle base\n",
    "ax2.quiver(0, 0, u1[0], u1[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='red', width=0.008, label='u₁')\n",
    "ax2.quiver(0, 0, u2[0], u2[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='red', width=0.008, label='u₂')\n",
    "ax2.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "           color='blue', width=0.01, label=f'v = {new_coord.round(decimals=2)}')\n",
    "\n",
    "# Projections sur nouvelle base\n",
    "ax2.plot([0, new_coord[0]*u1[0]], [0, new_coord[0]*u1[1]], 'k--', alpha=0.8)\n",
    "ax2.plot([0, new_coord[1]*u2[0]], [0, new_coord[1]*u2[1]], 'k--', alpha=0.8)\n",
    "\n",
    "ax2.set_xlim(-2.5, 5)\n",
    "ax2.set_ylim(-2.5, 5)\n",
    "ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_title(f'Nouvelle base', fontsize=13)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dans la base canonique : v = {v}\")\n",
    "print(f\"Dans la nouvelle base : new_coord = {new_coord}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da993ffa-78d0-4e7d-a36e-0a3a2e0cc283",
   "metadata": {},
   "source": [
    "Bravo ! Nous disposons maintenant de (presque) toutes les bases mathématiques pour réaliser une ACP. Nous avons encore une ou deux notions importantes à découvrir, mais tout d’abord voyons la logique derrière cette réduction de dimension pour bien comprendre comment nous allons résoudre le problème. Pour cela, il va falloir parler à nouveau de la variance.\n",
    "\n",
    "## Variance, covariance, et changement de base\n",
    "\n",
    "### Rappels sur la variance et la covariance\n",
    "\n",
    "#### Variance univariée\n",
    "\n",
    "La **variance** mesure la dispersion d'une variable autour de sa moyenne :\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n",
    "\n",
    "Plus la variance est élevée, plus les données sont dispersées.\n",
    "\n",
    "S’il n‘y avait pas de variance, cela voudrait dire que toutes les observations auraient la même valeur. D’une certaine manière, la variance nous donne une information : nous observons des variations qui :\n",
    "\n",
    "* sont causées par les variations d’une ou plusieurs autres variables (ce sont les modèles que nous essayons d’établir pour *expliquer* ces variations)\n",
    "* provoque les variations d’une ou plusieurs autres variables\n",
    "\n",
    "Cela nous amène à la notion de covariance.\n",
    "\n",
    "#### Covariance entre deux variables\n",
    "\n",
    "La **covariance** mesure comment deux variables varient ensemble :\n",
    "\n",
    "$$\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "**Interprétation** :\n",
    "- $\\text{Cov}(X, Y) > 0$ : les variables varient dans le même sens (corrélation positive)\n",
    "- $\\text{Cov}(X, Y) < 0$ : les variables varient en sens inverse (corrélation négative)\n",
    "- $\\text{Cov}(X, Y) \\approx 0$ : pas de relation linéaire\n",
    "\n",
    "\n",
    "#### Exercice 13 : rappel\n",
    "\n",
    "Soit les données suivantes :\n",
    "\n",
    "* x = [1, 2, 3, 4, 5]\n",
    "* y = [2, 4, 5, 4, 5]\n",
    "\n",
    "* Calculez la variance de x\n",
    "* Calculez la covariance entre x et y\n",
    "* Concluez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93222437-4360-4d14-8735-bb090b66c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 13 : votre code !\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Calcul manuel variance de x\n",
    "\n",
    "\n",
    "# Avec NumPy\n",
    "\n",
    "\n",
    "print(f\"Variable x : {x}\")\n",
    "print(f\"Moyenne : {x_mean}\")\n",
    "print(f\"\\nVariance (calcul manuel) : {var_manual}\")\n",
    "print(f\"Variance (NumPy) : {var_numpy}\")\n",
    "\n",
    "\n",
    "# Calcul manuel covariance x, y\n",
    "\n",
    "\n",
    "# Avec NumPy (matrice 2×2). Attention, lisez bien la doc de Numpy, il faudra mettre le paramètre bias=True\n",
    "\n",
    "\n",
    "print(f\"x : {x}\")\n",
    "print(f\"y : {y}\")\n",
    "print(f\"\\nCovariance (calcul manuel) : {cov_manual:.3f}\")\n",
    "print(f\"Covariance (NumPy) : {cov_xy:.3f}\")\n",
    "print(\"\") # VOS CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdca33-bb3d-4009-bd96-dade102d39c5",
   "metadata": {},
   "source": [
    "### Matrice de covariance\n",
    "\n",
    "Après tout un cours sur les matrices, vous vous doutez bien qu’on allait y revenir.\n",
    "\n",
    "Pour des données **multivariées** (plusieurs variables), on calcule la **matrice de covariance**.\n",
    "\n",
    "Soit $X$ une matrice de données de dimension $n \\times p$ :\n",
    "- $n$ = nombre d'échantillons\n",
    "- $p$ = nombre de variables\n",
    "\n",
    "#### Définition\n",
    "\n",
    "1. **Centrer** les données : $X_c = X - \\bar{X}$\n",
    "2. **Matrice de covariance** : \n",
    "\n",
    "$$\\Sigma = \\frac{1}{n} X_c^T X_c$$\n",
    "\n",
    "La matrice des covariance est carrée, de dimension : $p \\times p$\n",
    "De plus c’est une matrice symétrique (car le produit entre une matrice et sa transposée est toujours symétrique)\n",
    "\n",
    "#### Structure de la matrice\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix} \n",
    "\\sigma_1^2 & \\text{Cov}(1,2) & \\cdots & \\text{Cov}(1,p) \\\\\n",
    "\\text{Cov}(2,1) & \\sigma_2^2 & \\cdots & \\text{Cov}(2,p) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(p,1) & \\text{Cov}(p,2) & \\cdots & \\sigma_p^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- **Diagonale** : variances de chaque variable ($\\sigma_i^2$)\n",
    "- **Hors-diagonale** : covariances entre paires de variables\n",
    "\n",
    "#### Propriétés importantes\n",
    "\n",
    "- Comme nous l’avons déjà souligné, la matrice des covariances est **symétrique** : en effet $\\text{Cov}(i,j) = \\text{Cov}(j,i)$. Nous allons voirque cette symétrie a une importance capitale dans la suite.\n",
    "- **Semi-définie positive** : $v^T \\Sigma v \\geq 0$ pour tout vecteur $v$. « Semi-défini » signifie qu’il y a des vecteurs v pour lesquels cette opération peut s’annuler. Dans ce cas, cela indique qu’il existe des dépendances linéaires entre les colonnes de la matrice, et c’est précisément ce qui nous intéresse ici, exploiter ces dépendances pour réduire les dimensions. Pour s’assurer qu’une matrice est semi-définie positive, vérifiez que son déterminant n’est pas négatif. Dans le cas d’une matrice 2×2 par exemple, il faut vérifier que $\\sigma_1 × \\sigma_2 > covariance^2$\n",
    "\n",
    "*Note* : Une matrice définie positive (entièrement) voudrait dire que le résultat est strictement supérieur à zéro.\n",
    "\n",
    "### Réduction de dimension : pourquoi parler de la variance ?\n",
    "\n",
    "#### Exemple : éliminer les dimensions avec le moins de variance\n",
    "\n",
    "Dans le bout de code ci-dessous je vous présente une situation assez simple : \n",
    "\n",
    "* simulation de données (500 observations) avec des caractéristiques particulières :\n",
    "    * 3 dimensions\n",
    "    * des variances différentes sur chaque dimension : forte sur la dimension x, moyenne sur la dimension y et faible sur la dimension z\n",
    "* visualisation des données en conservant les 3 dimensions\n",
    "* projection dans des espaces à deux dimensions en éliminant à chaque fois une dimension : dans un cas la dimension avec une faible variance, et dans un autre cas la dimension avec une variance moyenne\n",
    "* on va se demander si ces projections altèrent la structure des données :\n",
    "    * si non, ça veut dire qu’on peut très bien ignorer une dimension qui en fait n’apporte pas beaucoup d’information)\n",
    "    * si oui, ca veut dire que la dimension éliminée nous permet de bien comprendre la structure de ces données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef7081-e2e2-447e-97dc-97915b8c1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Paramètres\n",
    "n_points = 500\n",
    "np.random.seed(42)  # Pour reproductibilité\n",
    "\n",
    "# Génération des points avec variances différentes selon les axes\n",
    "# Variance forte sur X, moyenne sur Y, faible sur Z\n",
    "var_x = 10   # Variance forte\n",
    "var_y = 4    # Variance moyenne\n",
    "var_z = 0.3  # Variance faible (peu d'information)\n",
    "\n",
    "x = np.random.randn(n_points) * np.sqrt(var_x)\n",
    "y = np.random.randn(n_points) * np.sqrt(var_y)\n",
    "z = np.random.randn(n_points) * np.sqrt(var_z)\n",
    "\n",
    "# Couleur basée sur la position en X pour visualiser la structure\n",
    "colors = x\n",
    "\n",
    "# Création de la figure avec deux sous-graphiques\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Sous-graphique 1 : Points en 3D\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "# Tracé des points avec plus de transparence\n",
    "scatter1 = ax1.scatter(x, y, z, c=colors, cmap='plasma', s=10, alpha=0.2, depthshade=False)\n",
    "\n",
    "# Vecteurs de variance en 3D avec plot3D (plus visible que quiver)\n",
    "# Vecteur variance selon X (rouge)\n",
    "ax1.quiver(0, 0, 0,      # Point de départ (x, y, z)\n",
    "          var_x, 0, 0,   # Direction du vecteur (dx, dy, dz)\n",
    "          color='red', \n",
    "          arrow_length_ratio=0.2,  # Taille de la pointe de flèche\n",
    "          linewidth=2,\n",
    "          label=f'Var X = {var_x}')\n",
    "\n",
    "# Vecteur variance selon Y (vert)\n",
    "ax1.quiver(0, 0, 0, \n",
    "          0, var_y, 0, \n",
    "          color='green', \n",
    "          arrow_length_ratio=0.2, \n",
    "          linewidth=2,\n",
    "          label=f'Var Y = {var_y}')\n",
    "\n",
    "\n",
    "# Vecteur variance selon Z (bleu) - même petit, on le montre\n",
    "ax1.quiver(0, 0, 0, \n",
    "          0, 0, var_z, \n",
    "          color='blue', \n",
    "          arrow_length_ratio=0.2, \n",
    "          linewidth=2,\n",
    "          label=f'Var Z = {var_z}')\n",
    "\n",
    "ax1.set_xlabel('X (var=10)', fontsize=10)\n",
    "ax1.set_ylabel('Y (var=4)', fontsize=10)\n",
    "ax1.set_zlabel('Z (var=0.3)', fontsize=10)\n",
    "ax1.set_title('Données en 3D\\nVariances différentes', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(-11, 13)\n",
    "ax1.set_ylim(-11, 13)\n",
    "ax1.set_zlim(-11, 13)\n",
    "ax1.view_init(elev=35, azim=-60) # positionnement de la caméra (pour mieux voir les vecteurs variances)\n",
    "ax1.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# Sous-graphique 2 : Projection sur le plan XY\n",
    "ax2 = fig.add_subplot(132)\n",
    "scatter2 = ax2.scatter(x, y, c=colors, cmap='plasma', s=15, alpha=0.7)\n",
    "\n",
    "# Vecteurs de variance en 2D (XY)\n",
    "ax2.quiver(0, 0, var_x, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02, label=f'Var X = {var_x}')\n",
    "ax2.quiver(0, 0, 0, var_y, angles='xy', scale_units='xy', scale=1, color='green', width=0.02, label=f'Var Y = {var_y}')\n",
    "\n",
    "ax2.set_xlabel('X (var=10)', fontsize=10)\n",
    "ax2.set_ylabel('Y (var=4)', fontsize=10)\n",
    "ax2.set_title('Projection sur XY\\n(Z ignoré → peu de perte)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(-11, 13)\n",
    "ax2.set_ylim(-11, 13)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# Sous-graphique 3 : Projection sur le plan XZ (mauvaise projection)\n",
    "ax3 = fig.add_subplot(133)\n",
    "scatter3 = ax3.scatter(x, z, c=colors, cmap='plasma', s=15, alpha=0.7)\n",
    "\n",
    "# Vecteurs de variance en 2D (XZ)\n",
    "ax3.quiver(0, 0, var_x, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02, label=f'Var X = {var_x}')\n",
    "ax3.quiver(0, 0, 0, var_z, angles='xy', scale_units='xy', scale=1, color='blue', width=0.02, label=f'Var Z = {var_z}')\n",
    "\n",
    "ax3.set_xlabel('X (var=10)', fontsize=10)\n",
    "ax3.set_ylabel('Z (var=0.3)', fontsize=10)\n",
    "ax3.set_title('Projection sur XZ\\n(Y ignoré → perte importante)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlim(-11, 13)\n",
    "ax3.set_ylim(-11, 13)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# Barre de couleur positionnée à droite\n",
    "plt.subplots_adjust(left=0.03, right=0.88, wspace=0.4)\n",
    "cbar = fig.colorbar(scatter2, ax=ax3, shrink=0.8, pad=0.15)\n",
    "cbar.set_label('Valeur de X', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=\" * 60)\n",
    "print(\"VARIANCE ET RÉDUCTION DE DIMENSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VARIANCES DES DONNÉES\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Variance sur X : {var_x:5.1f}  (FORTE)   → beaucoup d'information\")\n",
    "print(f\"Variance sur Y : {var_y:5.1f}  (MOYENNE) → information modérée\")\n",
    "print(f\"Variance sur Z : {var_z:5.1f}  (FAIBLE)  → peu d'information\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VARIANCES CALCULÉES SUR LES DONNÉES\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Variance réelle X : {np.var(x):.2f}\")\n",
    "print(f\"Variance réelle Y : {np.var(y):.2f}\")\n",
    "print(f\"Variance réelle Z : {np.var(z):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ANALYSE DES PROJECTIONS\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\")\n",
    "print(\"PROJECTION XY (ignorer Z) :\")\n",
    "print(\"  • Z a une variance faible (0.3)\")\n",
    "print(\"  • Ignorer Z = perdre peu d'information\")\n",
    "print(\"  • La structure des données est PRÉSERVÉE\")\n",
    "print(\"  • C'est ce que ferait PCA : garder X et Y\")\n",
    "print(\"\")\n",
    "print(\"PROJECTION XZ (ignorer Y) :\")\n",
    "print(\"  • Y a une variance moyenne (4)\")\n",
    "print(\"  • Ignorer Y = perdre de l'information\")\n",
    "print(\"  • Les points s'écrasent sur une bande étroite en Z\")\n",
    "print(\"  • La structure est MOINS BIEN préservée\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VARIANCE EXPLIQUÉE\")\n",
    "print(\"-\" * 60)\n",
    "total_var = var_x + var_y + var_z\n",
    "print(f\"Variance totale : {total_var:.1f}\")\n",
    "print(f\"\")\n",
    "print(f\"Si on garde X et Y :\")\n",
    "print(f\"  Variance conservée : {var_x + var_y:.1f} / {total_var:.1f} = {100*(var_x + var_y)/total_var:.1f}%\")\n",
    "print(f\"\")\n",
    "print(f\"Si on garde X et Z :\")\n",
    "print(f\"  Variance conservée : {var_x + var_z:.1f} / {total_var:.1f} = {100*(var_x + var_z)/total_var:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f525e-64b1-4dd4-a2ca-4b1489072a3a",
   "metadata": {},
   "source": [
    "Si on veut voir plus précisément à quoi ressemble le nuage de point en 3D, voici le moyen de l’explorer de manière interactive en 3D (vous pouvez zoomer et faire tourner la figure en 3D) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de76a5-d9c4-46d6-9f86-5f68abd95f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "# peut nécessiter d’installer l’extension plotly pour jupyter lab : \n",
    "#!jupyter labextension install jupyterlab-plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd18dd4-c480-4ba2-9f60-a3bd102d38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter_3d(x = x,\n",
    "              y = y,\n",
    "            z = z,\n",
    "              range_x = [-11, 13],\n",
    "              range_y = [-11, 13],\n",
    "              range_z = [-11, 13],\n",
    "              opacity=0.7, width=800, height=500\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874bee7-c617-449f-8d66-efed4d0e94d2",
   "metadata": {},
   "source": [
    "L’exemple que nous venons de voir est très théorique : les données sont dispersées selon un alignement avec les axes (x, y et z). Dans la réalité, les données peuvent avoir une structure où la variance change effectivement selon certaines directions – c’est d’ailleurs ce qui nous intéresse – mais ces directions ne sont pas triviales. \n",
    "\n",
    "#### Exercice 14 : projection, transformation et changement de base\n",
    "\n",
    "Nous allons donc faire un exercice un peu plus aligné avec ce qu’il peut se passer dans la réalité, où les données qui ne sont pas orientées dans la direction des axes (x, y, z), et où l’on verra donc l’intérêt de changer de base. Sans être à strictement parler une analyse en composantes principales, cet exercice nous permettra d’en comprendre la logique. Le but est de générer les figures suivantes :\n",
    "\n",
    "![Nuages de point dans des directions quelconques](./images/NuageDirectionQuelconque.png)\n",
    "\n",
    "Nous allons procéder en plusieurs étapes. Pour générer un nuage de points orienté dans des directions diverses, nous allons comme dans l’exercice précédent générer un nuage avec des variances dans les différents axes, puis nous allons appliquer des transformations (rotations) qui vont modifier les orientations.\n",
    "\n",
    "* Étape 1 : générer un nuage de point avec différentes variances selon les 3 dimensions, comme dans l’exemple précédent\n",
    "* Étape 2 : créer une matrice de rotation 3D. Vous savez faire des rotations en 2D. Pour créer cette matrice nous allons dans un premier temps réaliser des rotations autour de chacun des axes. Nous allons donc créer une matrice $Rx$ pour une rotation autour de l’axe $x$ d’un angle γ, puis $Ry$ avec un angle β et $Rz$ avec un angle α. Nous allons ensuite composer ces rotations pour créer une matrice de rotation $R$. Créez donc une fonction `matrice_rotation_3d(alpha, beta, gamma)` qui retournera cette matrice $R$.\n",
    "  Pour vous aider, voici la matrice de rotation autour de l’axe $x$ :\n",
    "  $$\n",
    "  \\begin{bmatrix}  1 & 0 & 0 \\\\\n",
    "                  0 & \\cos \\gamma & -\\sin \\gamma \\\\\n",
    "                  0 & \\sin \\gamma & \\cos \\gamma\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  À vous de trouver les matrices pour les autres axes.\n",
    "  *(Vous devriez au moins comprendre intuitivement qu’il y a un axe qu’on laisse inchangé, et qu’on fait la rotation dans le plan des deux autres axes)\n",
    "* Étape 3 : appliquer la rotation au nuage de point, et extraire les coordonnées des points selon les 3 axes. La figure ci-dessus a été obtenue avec des rotations de 40° autour de $x$, 25° autour de $y$ et 35° autour de $z$, mais vous pouvez (et je vous encourage à la fin de l’exercice de tester plusieurs valeurs pour voir les effets). Nous avons notre nouveau nuage de point, qui a ses propres directions (alignées sur la base qui correspond à la rotation que nous venons d’effectuer). Imaginons que ce sont les poins que nous observons, **tels qu’ils apparaissent dans le repère (x, y, z)**.\n",
    "* Étape 4 : nous pouvons aussi, pour ce nuage de points dans le repère (x, y, z), nous pouvons créer les vecteurs correspondant au variances dans les différentes directions. Si vous avez bien suivi, chaque colonne de notre matrice de rotation correspond à l’une de ces directions. Pour chacune de ces direction, le vecteur variance coorespondant a juste pour longueur la variance correspondante.\n",
    "* Étape 5 : figure 1, visualisation 3D : affichez le nuage de points dans (x, y z) et les vecteurs variances dans chacune des directions (que vous venez de calculer)\n",
    "* Étape 6 : figure 2, projection 2D sans changement de base : affichez la projection du nuage sur le plan (x, y) et les vecteurs variances en éliminant la variance z, tout simplement (comme précédemment). Qu’observez-vous pour les variances ?\n",
    "* Étape 7 : figure 3, projection 2D avec changement de base : on va afficher le nuage de points mais cette fois dans la base transformée. On ne va plus appeler les axes $x$ et $y$, mais « composante principale 1 » et « composante principale 2 », qui forment le repère qu’on appelera « CP », car elles correspondront aux composantes liées aux variances qui caractérisent notre nuage de point. Nous retenons seulement les deux directions avec les deux plus fortes variances. Calculez donc les coordonnées des points dans le repère CP (en faisant donc la rotation inverse : rappelez-vous une propriété intéressante des matrices orthogonales), et afficher les vecteurs variances correspondants.\n",
    "* Quelles sont vos observations ? Comprenez vous l’intérêt de prendre en compte le changement de base ?\n",
    "* Pour guider votre réflexion, votre code devra afficher :\n",
    "    * La matrice de rotation $R$\n",
    "    * Les variances  dans (x, y, z) et les variances dans CP (pour les trois directions)\n",
    "    * Vérifier que le calcul des coordonnées des points dans CP (par rotation inverse) correspondent aux vecteurss (p1, p2, p3) qu’on a utilisé au départ pour créer notre nuage de point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c58d1-0beb-4908-ba8f-99145f80fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 14 : Votre code !\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Paramètres\n",
    "n_points = 500\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : On commence par générer un nuage de points aligné sur les axes (comme dans l’exemple)\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# VOTRE CODE\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : Créer une matrice de rotation pour orienter le nuage\n",
    "# -----------------------------------------------------------------------------\n",
    "# Pour créer une rotation 3D quelconque on va composer 3 rotations autour de chacun des axes\n",
    "\n",
    "def matrice_rotation_3d(alpha, beta, gamma):\n",
    "    \"\"\"\n",
    "    Crée une matrice de rotation 3D à partir des angles d'Euler.\n",
    "    alpha : rotation autour de l'axe Z\n",
    "    beta  : rotation autour de l'axe Y\n",
    "    gamma : rotation autour de l'axe X\n",
    "    \"\"\"\n",
    "    # Rotation autour de Z\n",
    "    # VOTRE CODE\n",
    "\n",
    "    \n",
    "    # Rotation autour de Y\n",
    "    # VOTRE CODE\n",
    "\n",
    "    \n",
    "    # Rotation autour de X\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Composition des rotations\n",
    "    return # VOTRE CODE\n",
    "\n",
    "# Angles de rotation (en radians)\n",
    "alpha = np.radians(35)   # Rotation autour de Z\n",
    "beta = np.radians(25)    # Rotation autour de Y\n",
    "gamma = np.radians(40)   # Rotation autour de X\n",
    "\n",
    "# Matrice de rotation\n",
    "R = matrice_rotation_3d(alpha, beta, gamma)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : Appliquer la rotation au nuage de points\n",
    "# -----------------------------------------------------------------------------\n",
    "# Chaque point est transformé par la matrice de rotation\n",
    "points_tournes = # VOTRE CODE\n",
    "\n",
    "# Extraire les coordonnées\n",
    "x = points_tournes[:, 0]\n",
    "y = points_tournes[:, 1]\n",
    "z = points_tournes[:, 2]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : Calculer les directions principales (vecteurs propres)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Les directions principales sont les colonnes de R (car on a appliqué R aux données)\n",
    "# En pratique, on les retrouverait avec la PCA\n",
    "\n",
    "# Direction de variance maximale (var_1 = 10)\n",
    "dir_1 = R[:, 0] * var_1\n",
    "\n",
    "# Direction de variance moyenne (var_2 = 4)\n",
    "dir_2 = R[:, 1] * var_2\n",
    "\n",
    "# Direction de variance minimale (var_3 = 0.3)\n",
    "dir_3 = R[:, 2] * var_3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VISUALISATION\n",
    "# -----------------------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Figure 1 : Nuage de points tourné en 3D\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "# Points avec transparence\n",
    "colors = p1  # Couleur basée sur la position dans la direction principale\n",
    "scatter1 = ax1.scatter(x, y, z, c=colors, cmap='plasma', s=10, alpha=0.2, depthshade=False)\n",
    "\n",
    "# Vecteurs des directions principales (longueur = variance)\n",
    "# VOTRE CODE\n",
    "\n",
    "ax1.set_xlabel('X', fontsize=10)\n",
    "ax1.set_ylabel('Y', fontsize=10)\n",
    "ax1.set_zlabel('Z', fontsize=10)\n",
    "ax1.set_title('Nuage tourné en 3D\\nDirections principales ≠ axes', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(-11, 13)\n",
    "ax1.set_ylim(-11, 13)\n",
    "ax1.set_zlim(-11, 13)\n",
    "ax1.view_init(elev=20, azim=45)\n",
    "ax1.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Figure 2 : Projection sur XY (mauvaise sans changement de base)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "scatter2 = ax2.scatter(x, y, c=colors, cmap='plasma', s=15, alpha=0.7)\n",
    "\n",
    "# Projection des vecteurs sur XY\n",
    "# VOTRE CODE\n",
    "\n",
    "ax2.set_xlabel('X', fontsize=10)\n",
    "ax2.set_ylabel('Y', fontsize=10)\n",
    "ax2.set_title('Projection sur XY\\n(mélange des composantes)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(-11, 13)\n",
    "ax2.set_ylim(-11, 13)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Figure 3 : Vue dans le repère des composantes principales\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "# CHANGEMENT DE BASE : projection dans le repère des CP\n",
    "# On applique la transformation inverse : R^T @ points_tournes^T\n",
    "# (R est orthogonale donc R^(-1) = R^T)\n",
    "points_dans_repere_CP = # VOTRE CODE\n",
    "\n",
    "# Extraction des coordonnées dans le repère des CP\n",
    "cp1 = points_dans_repere_CP[:, 0]\n",
    "cp2 = points_dans_repere_CP[:, 1]\n",
    "cp3 = points_dans_repere_CP[:, 2]\n",
    "\n",
    "# Affichage des deux premières composantes principales\n",
    "scatter3 = ax3.scatter(cp1, cp2, c=colors, cmap='plasma', s=15, alpha=0.7)\n",
    "\n",
    "# Vecteurs alignés sur les axes\n",
    "# VOTRE CODE\n",
    "\n",
    "ax3.set_xlabel('Composante Principale 1', fontsize=10)\n",
    "ax3.set_ylabel('Composante Principale 2', fontsize=10)\n",
    "ax3.set_title('Après changement de base\\n(repère des CP)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlim(-11, 13)\n",
    "ax3.set_ylim(-11, 13)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# Barre de couleur\n",
    "plt.subplots_adjust(left=0.03, right=0.88, wspace=0.4)\n",
    "cbar = fig.colorbar(scatter3, ax=ax3, shrink=0.8, pad=0.15)\n",
    "cbar.set_label('Position sur PC1', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# AFFICHAGE DES INFORMATIONS\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"EXERCICE : NUAGE DE POINTS AVEC DIRECTIONS QUELCONQUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"PARAMÈTRES DE GÉNÉRATION\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Nombre de points : {n_points}\")\n",
    "print(f\"\\nVariances dans le repère des composantes principales :\")\n",
    "print(f\"  • Variance CP1 : {var_1:5.1f}  (direction principale)\")\n",
    "print(f\"  • Variance CP2 : {var_2:5.1f}  (direction secondaire)\")\n",
    "print(f\"  • Variance CP3 : {var_3:5.1f}  (direction de moindre variance)\")\n",
    "\n",
    "print(f\"\\nAngles de rotation appliqués :\")\n",
    "print(f\"  • Alpha (autour de Z) : {np.degrees(alpha):.0f}°\")\n",
    "print(f\"  • Beta  (autour de Y) : {np.degrees(beta):.0f}°\")\n",
    "print(f\"  • Gamma (autour de X) : {np.degrees(gamma):.0f}°\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"MATRICE DE ROTATION\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Les colonnes de cette matrice sont les directions des composantes principales :\")\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"POURQUOI LE CHANGEMENT DE BASE EST NÉCESSAIRE\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "# VOTRE CODE\n",
    "\"\"\".format(var_1 + var_2))\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"VARIANCE DANS LE REPÈRE ORIGINAL vs REPÈRE DES CP\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nVariance dans le repère (x, y, z) :\")\n",
    "# VOTRE CODE\n",
    "\n",
    "print(f\"\\nVariance dans le repère des CP (après changement de base) :\")\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"VÉRIFICATION : TRANSFORMATION INVERSE\")\n",
    "print(\"-\" * 70)\n",
    "print(\"On a calculé les coordonnées dans le repère des CP par :\")\n",
    "print(\"  points_dans_repere_CP = R^T @ points_tournes\")\n",
    "print(\"\")\n",
    "print(\"Vérification que cp1, cp2, cp3 correspondent bien à p1, p2, p3 (différence nulle):\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Les différences sont quasi-nulles : le changement de base\")\n",
    "print(\"a bien retrouvé les coordonnées initiales p1, p2, p3 !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6b50d-18bc-487c-803e-6986ac3eccb6",
   "metadata": {},
   "source": [
    "Bien sûr cet exercice est toujours un peu artificiel (on crée un nuage de point, on le transforme de manière connue, et on fait la transformée inverse pour retomber sur le nuage dont on est parti) mais il montre bien étape par étape la logique qu’on va mettre en œuvre dans l’analyse en composante principale.\n",
    "\n",
    "Dans l’exercice, nous savions exactement quelle transformation nous avions appliquée au nuage de points pour lui donner les directions que nous recherchions, donc nous savions exactement quelle matrice de transformation nous permettait d’effectuer le changement de base. Avec des données réelles, il n‘y a personne pour nous indiquer quelles sont les directions qui maximisent les variances, il nous faut donc une méthode pour trouver ces directions, et donc la matrice de transformation qui convient.\n",
    "\n",
    "C’est l’objet de la prochaine section, celle où l’on va évoquer la dernière notion d’algèbre linéaire que nous allons découvrir : les valeurs propres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b211ef7-50d8-4bca-9c32-f6b38d8a466f",
   "metadata": {},
   "source": [
    "## ACP : Valeurs propres et diagonalisation\n",
    "\n",
    "### Matrice symétrique et ellipsoïde d’inertie\n",
    "\n",
    "Rappel de notre problème :\n",
    "\n",
    "* nous avons des donnnées dans plusieurs dimensions, et nous souhaitons conserver seulement les dimensions qui apportent de l’information (avec le maximum de variance) et éliminer les dimensions qui n’apportent que peu d’information (avec le moins de variance)\n",
    "* ces données, qu’on peut se représenter comme un nuage de points, sont organisées selon des directions privilégiées, les directions où la variance est maximale\n",
    "* il nous faut donc trouver un repère orienté sur ces directions où la variance est maximale, donc trouver la matrice de transformation qui permet de calculer les coordonnées dans cet espace de variance maximale\n",
    "* une matrice que nous avons sous la main est la matrice de variance covariance, qui, outre le fait d’être une matrice carrée, est symétrique.\n",
    "\n",
    "Penchons donc nous plus en détail sur les matrices symétriques.\n",
    "\n",
    "Nous avons vu que tout matrice est en fait une transformation linéaire. Une matrice carré transforme un vecteur de dimension $n$ en un autre vecteur de dimension $n$, et comme une matrice carré est inversible, son inverse permet la transformation inverse. C’est comme ça (ce que nous venons de voir ci-dessus) qu’on peut utiliser une matrice pour calculer les coordonnées d’un vecteur dans différents espaces.\n",
    "\n",
    "Nous avons vu plus haut différents types de transformations à l’aide d’une matrice carrée (homothétie, cisaillement, rotation…). Voyons donc quel type de transformation une matrice symétrique permet de réaliser.\n",
    "\n",
    "#### Exercice 15 : transformation par une matrice symétrique\n",
    "\n",
    "* Nous allons expérimenter avec une matrice symétrique simple, dimension (2,2), par exemple :\n",
    "$$\n",
    "A = \\begin{bmatrix}5 & 0,5\\\\\n",
    "            0,5 & 3\\end{bmatrix}\n",
    "$$\n",
    "* Étape 1 : Pour voir la déformation, il faut partir d’une figure dont nous pourrons apprécier la transformation. Pour cette exercice, le mieux est de partir d’un cercle (vous comprendrez vite pourquoi). Créez donc 10 vecteurs dont les extrémités sont situés sur un cercle centré sur l’origine et de rayon 1 (le cercle trigonométrique). Indice : pour un angle donnée, la composante x du vecteur est le cosinus de l’angle, et la composante y le sinus.\n",
    "*  Étape 2 : pour tout ces vecteurs, calculez la transformation par la matrice $A$\n",
    "*  Étape 3 : pour vous, nous générons ce que nous appelons « valeurs propres » et « vecteurs propres ». Votre mission dans cet exercice sera de comprendre à quoi cela correspond\n",
    "*  Étape 4 : pour faciliter la visualisation, générer un cercle de rayon 1 et centré sur 0, et la transformation de ce cercle par la matrice A\n",
    "*  Étape 5 : visualisation de tout ces objets :\n",
    "    *  tracez le cercle en pointillés bleus,\n",
    "    *  le cercle transformé en pointillés rouges,\n",
    "    *  des flèches bleues pour les vecteurs sur le cercle,\n",
    "    *  des flèches rouges pour les vecteurs sur le cercle tranformé\n",
    "    *  pour faciliter la lecture de la figure et suivre les transformation, vous pouvez aussi tracer des lignes pointillées grises entre chaque vecteur sur le cercle d’origine, et le vecteur transformé correspondant\n",
    "* Pour vous, nous traçons également en vert les vecteurs propres « scalés » par les valeurs propres que nous avons calculé précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3afbc-8f5a-41bc-a8d5-e8aa01d32f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 15 : votre code !\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matrice symétrique\n",
    "A = np.array([[5, 0.5],\n",
    "              [0.5, 3]])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : Générer les vecteurs sur le cercle unité\n",
    "# -----------------------------------------------------------------------------\n",
    "n_vecteurs = 10\n",
    "angles = np.linspace(0, 2 * np.pi, n_vecteurs, endpoint=False)\n",
    "\n",
    "# Vecteurs unitaires (extrémités sur le cercle de rayon 1)\n",
    "vecteurs_cercle = np.array([[ # VOTRE CODE] for theta in angles])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : Appliquer la transformation matricielle\n",
    "# -----------------------------------------------------------------------------\n",
    "# Chaque vecteur v devient A @ v\n",
    "vecteurs_ellipse = # VOTRE CODE\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : Calculer les valeurs et vecteurs propres\n",
    "# -----------------------------------------------------------------------------\n",
    "valeurs_propres, vecteurs_propres = np.linalg.eig(A)\n",
    "\n",
    "# Trier par valeur propre décroissante\n",
    "idx = np.argsort(valeurs_propres)[::-1]\n",
    "valeurs_propres = valeurs_propres[idx]\n",
    "vecteurs_propres = vecteurs_propres[:, idx]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : Générer le cercle et l'ellipse pour le tracé\n",
    "# -----------------------------------------------------------------------------\n",
    "theta_contour = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Cercle unité\n",
    "cercle_x = # VOTRE CODE\n",
    "cercle_y =# VOTRE CODE\n",
    "\n",
    "# Ellipse = transformation du cercle par A\n",
    "ellipse_points = np.array([# VOTRE CODE @ np.array(# VOTRE CODE) for t in theta_contour])\n",
    "ellipse_x = ellipse_points[:, 0]\n",
    "ellipse_y = ellipse_points[:, 1]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VISUALISATION\n",
    "# -----------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Cercle unité en pointillés bleus\n",
    "ax.plot(cercle_x, cercle_y, 'b--', linewidth=2, label='Cercle unité')\n",
    "\n",
    "# Ellipse en pointillés rouges\n",
    "ax.plot(ellipse_x, ellipse_y, 'r--', linewidth=2, label='Ellipse (image par A)')\n",
    "\n",
    "# Vecteurs originaux (sur le cercle)\n",
    "for i, v in enumerate(vecteurs_cercle):\n",
    "   # VOTRE CODE\n",
    "\n",
    "# Vecteurs transformés (sur l'ellipse)\n",
    "for i, v in enumerate(vecteurs_ellipse):\n",
    "   # VOTRE CODE\n",
    "\n",
    "# Lignes de correspondance entre cercle et ellipse\n",
    "for i in range(len(vecteurs_cercle)):\n",
    "    v_cercle = vecteurs_cercle[i]\n",
    "    v_ellipse = vecteurs_ellipse[i]\n",
    "    # VOTRE CODE\n",
    "\n",
    "# Vecteurs propres (axes de l'ellipse)\n",
    "for i in range(2):\n",
    "    vp = vecteurs_propres[:, i]\n",
    "    val = valeurs_propres[i]\n",
    "    # Vecteur propre mis à l'échelle par la valeur propre\n",
    "    ax.quiver(0, 0, vp[0] * val, vp[1] * val, angles='xy', scale_units='xy', scale=1,\n",
    "              color='green', width=0.008, linewidth=2,\n",
    "              label=f'VP{i+1}: λ={val:.1f}' if i == 0 else f'VP{i+1}: λ={val:.1f}')\n",
    "\n",
    "# Configuration\n",
    "ax.set_xlim(-7, 7)\n",
    "ax.set_ylim(-7, 7)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Ellipsoïde d\\'inertie : transformation du cercle en ellipse\\n' +\n",
    "             'Matrice A = [[5, 0.5], [0.5, 3]]', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b7a70-ee9a-4ecc-9315-41b816e9a4da",
   "metadata": {},
   "source": [
    "Que constatez vous ?\n",
    "* À quoi correspond la transformation par une matrice symétrique ?\n",
    "* À quoi, selon vous, correspondent les vecteurs propres et les valeurs propres ?\n",
    "* Pourquoi parle-t-on d’éllipsoïde d’inertie ?\n",
    "\n",
    "Essayez de répondre par vous-même à ces questions avant de lire la suite.\n",
    "\n",
    "Revenons à notre transformation : nous constatons que nous transformons un cercle en ellipse. Cela signifie que nous appliquons deux « dilatations » différentes selon deux directions orthogonales.\n",
    "\n",
    "Si nous reprenons notre matrice symétrique de départ $A$ la transformation d’un vecteur $v = [v_x, v_y]^T$ en un vecteur $u =[u_x, u_v]^T$ s’écrit :\n",
    "$$\n",
    "u_x = 5·v_x + 0,5·v_y\\\\\n",
    "u_y = 0,5·v_x + 3·v_y\n",
    "$$\n",
    "qui, si elle transforme en ellipse, devrait être équivalente à :\n",
    "$$\n",
    "u_x = \\lambda_1·x + 0·y\n",
    "u_y = 0·x + \\lambda_2·y\n",
    "$$\n",
    "à condition de faire correspondre les axes x et y aux axes de l’ellipse.\n",
    "\n",
    "Dans ce cas, la matrice de transformation en ellipse est : \n",
    "$$\n",
    "E = \\begin{bmatrix}\\lambda_1 & 0 \\\\\n",
    "                0 & \\lambda_2\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Qui est une matrice diagonale : comme $A$ et $E$ réalisent une transformation équivalente (mais dans une base différente), on dit que $E$ est la diagonalisation de la matrice $A$. $\\lambda_1$ et $\\lambda_2$ sont les **valeurs propres** de la matrice. Ici la matrice diagonalisée est :\n",
    "$$\n",
    "\\begin{bmatrix}\\lambda_1 & 0 \\\\\n",
    "                0 & \\lambda_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}5,11803399 & 0 \\\\\n",
    "                0 & 2,88196601\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Propriété : \n",
    "* les valeurs propres ne peuvent pas être négatives (cela vient de ce que la matrice des covariances est semi-définie positive)\n",
    "* les deux matrices (variance/covariance et diagonalisée) ont la même trace !\n",
    "$$ \n",
    "\\text{Trace }A = \\text{Trace } \\begin{bmatrix}5 & 0,5 \\\\\n",
    "                0,5 & 3\n",
    "\\end{bmatrix} = 5 + 3 = 8\n",
    " \\\\\n",
    "\\text{Trace }E = \\text{Trace } \\begin{bmatrix}5,11803399 & 0 \\\\\n",
    "                0 & 2,88196601\n",
    "\\end{bmatrix} = 5,11803399 + 2,88196601 = 8\n",
    "$$\n",
    "\n",
    "La matrice diagonalisée conserve la variance, mais annule les termes de covariance.\n",
    "\n",
    "Si vous avez été attentif-ve-s, vous avez bien noté que tout ceci nécessite de se placer dans la bonne base orthonormée, qui correspond aux axes de l’ellipse. Cette fois, ce sont les **vecteurs propres** qui vont constituer cette base. Techniquement ils forment les petits et grands axes de l’ellipse, on obtient des vecteurs unités en les divisant par les valeurs propres (normalisation). \n",
    "Les vecteurs propres normalisés permettent de créer la matrice qui permet de changer de base : chaque colonne de la matrice sera un vecteur propre normalisé. Ici si $vp_1 = [0.9732, 0.2298]^T$ et $vp_2 = [-0.2298, 0.9732]$ alors la matrice de changement de base est :\n",
    "$$\n",
    "P = \\begin{bmatrix}0.9732 & -0.2298 \\\\\n",
    "                    0.2298 & 0.9732   \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Et vous aurez reconnu la forme d’une matrice de rotation. \n",
    "**Note importante** : cette rotation se fait autour de l’origine, il est donc très important que les données soient centrées ! \n",
    "\n",
    "#### Exercice 16 : récapitulation\n",
    "\n",
    "Pour récapituler, faite un petit script :\n",
    "\n",
    "* Affichez la matrice A, quelle sont ses spécificités ?\n",
    "* Affichez les valeurs propres. À quoi correspondent-elles ?\n",
    "* Affichez les vecteurs propres. À quoi correspondent-ils ?\n",
    "* Affichez la matrice de changement de base P\n",
    "* Affichez la matrice diagonalisée D\n",
    "* Retrouver A à partir de P et de D. Indice : il faut exprimer D dans le repère P, puis passer la matrice obtenue dans le repère d’origine avec la transformation inverse (attention à l’ordre des opérations, et n’oubliez pas que nous pouvons exploiter une relation particulière entre inverse et transposée dans notre cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c28801-67b9-4751-a8a4-f4c39f31be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 16 : votre code\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGONALISATION ET ELLIPSOÏDE D'INERTIE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MATRICE A\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VALEURS PROPRES\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VECTEURS PROPRES (normalisés)\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MATRICE DE PASSAGE P (colonnes = vecteurs propres)\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MATRICE DIAGONALE D\")\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VÉRIFICATION : A = P @ D @ P^T\")\n",
    "print(\"-\" * 60)\n",
    "# VOTRE CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4e42d-b64b-4221-975e-eac841520805",
   "metadata": {},
   "source": [
    "**Bien sûr tout ce que nous venons de voir pour 3 dimensions est parfaitement généralisable à $n$ dimensions (c’est tout l’intérêt).**\n",
    "\n",
    "Bravo ! vous avez acquis toutes les bases mathématiques qui vous permettent d’implémenter votre propre algorthme d’analyse en composante principale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0adb4-0217-4063-8d8d-966340d4c0ef",
   "metadata": {},
   "source": [
    "## L’analyse en composante principale, ou ACP (ou PCA en anglais)\n",
    "\n",
    "### Pourquoi réduire les dimensions ?\n",
    "\n",
    "#### Beaucoup de dimension = problèmes\n",
    "\n",
    "En data science, on travaille souvent avec des données de **haute dimension**. Par exemple :\n",
    "- Images : millions de pixels\n",
    "- Textes : milliers de mots\n",
    "- Données biologiques : milliers de gènes\n",
    "\n",
    "**Cela pose de nombreux problèmes** :\n",
    "* difficile à visualiser (au-delà de 3D)\n",
    "* coûteux en calcul. À noter réduire les dimensions d’un problème ne va généralement pas *faciliter* l’apprentissage par un modèle de machine learning, mais va plutôt *l’accélérer* (moins de calculs = temps gagné = moins cher, mais pas forcément de meilleures performances)\n",
    "* \"fléau (*curse*) de la dimensionalité\" : plus il y a de dimensions, et plus chaque observation va devenir très spécifique (combinaison de valeurs singulières de chaque dimension), les données seront éparses, difficile de trouver des traits communs, donc de généraliser. Pour cela, il faut arriver à « gommer » des petites différences (introduire un biais), « lisser » les données en perdant de l’information (mais pas trop) donc en supprimant les dimensions qui apportent plus du bruit que de l’information utile.\n",
    "* difficile d’interpréter des données quand il y a plus de dimensions que ce que l’on peut se représenter de manière intelligible (cf. point 1)\n",
    "\n",
    "#### Objectif de l'ACP\n",
    "\n",
    "* **Réduire la dimensionnalité** $n \\rightarrow k$ (avec $k < n$) tout en **conservant le maximum d'information** (variance).\n",
    "\n",
    "* Principe : Trouver les directions où les données **varient le plus**.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "* facilite la **visualisation** de données à hautes dimensions\n",
    "* **compression** de données\n",
    "* **débruitage** (on ne garde que l’info pertinente)\n",
    "* **preprocessing** pour le Machine Learning pour accélérer l’apprentissage ou le rendre moins coûteux en ressources (ça fait partie de l’arsenal d’exploration des données et de prétraitement, comme K-means)\n",
    "* analyse d'images (on va farie un TP sur la reconnaissance de visages)\n",
    "* analyse de données génomiques (isoler les séquences significatives)\n",
    "\n",
    "### Algorithme de l’ACP\n",
    "\n",
    "#### Rappel des fondements mathématiques (au cas où ce ne serait pas bien rentré)\n",
    "\n",
    "Problème : trouver une direction $u_1$ (vecteur unitaire) telle que la **variance** des données projetées sur $u_1$ soit **maximale** :\n",
    "\n",
    "$$\\max_{u_1} \\text{Var}(\\text{projection sur } u_1) = \\max_{u_1} u_1^T \\Sigma u_1$$\n",
    "\n",
    "sous la contrainte : $\\|u_1\\| = 1$\n",
    "\n",
    "Solution : $u_1$ est le **vecteur propre** associé à la **plus grande valeur propre** de $\\Sigma$.\n",
    "\n",
    "La variance capturée est $\\lambda_1$ (la plus grande valeur propre), ce sera la première **composante principale**.\n",
    "\n",
    "Ensuite :\n",
    "\n",
    "* **2ème composante principale** $u_2$ : direction orthogonale à $u_1$ de variance maximale\n",
    "* $u_2$ = vecteur propre de la 2ème plus grande valeur propre $\\lambda_2$\n",
    "* et ainsi de suite...\n",
    "\n",
    "Pour réduire les dimensions, on va juste sélectionner les $k$ premières composantes principales. Exemple : pour réduire un espace à 3 dimensions, on va retenir seulement les $k=2$ premières composantes principales.\n",
    "\n",
    "Les $k$ premières composantes principales :\n",
    "* correspondent aux $k$ vecteurs propres des $k$ plus grandes valeurs propres\n",
    "* forment une **base orthonormée optimale** pour représenter les données\n",
    "\n",
    "#### Exercice 17 : Implémentation\n",
    "\n",
    "Maintenant que nous connaissons les principes mathématiques en jeu, l’implémentation de l’algorithme est pratiquement évidente. Même principe que pour le k-means : je vous propose du code à trou, à vous de le compléter !\n",
    "\n",
    "Vous allez créer une classe `PCA()` qui prend en argument le nombre `k`de composantes principales que vous voulez conserver, avec :\n",
    "* une méthode `.fit()` qui :\n",
    "    1. centre le jeu de données\n",
    "    2. calcule la matrice de covariance\n",
    "    3. calcule les valeurs propres et les vecteurs propres\n",
    "    4. ordonne les valeurs propres par ordre décroissant (attention, les vecteurs aussi)\n",
    "    5. sélectionner les `k` premières composantes\n",
    "* une méthodee `.transform()` qui réalise la projection dans l’espace des vecteurs propres et renvoie les données projetées\n",
    "* pour le reste sont déjà écrites :\n",
    "    * l’initialisation de la classe (qui déclare les variables qui seront utilisées)\n",
    "    * une méthode `.fit_transform()` (standard dans beaucoup de classes de machine learning) qui enchaîne les deux méthodes précédentes\n",
    "    * une méthode `.inverse_transform()` qui recrée les données originales à partir des données projetées dans l’espace des vecteurs propres\n",
    "    * des méthodes `.explained_variance()` et `.explained_variance_ratio()` qui renvoie la part de variance expliquée par chaque composante\n",
    "\n",
    "À vous de jouer pour compléter ce code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2864f2f-1f74-4d67-bea2-81c2676aff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 16 : compléter le code \n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, k_components):\n",
    "        \"\"\"\n",
    "        Analyse en Composantes Principales\n",
    "        \n",
    "        Paramètres :\n",
    "        -----------\n",
    "        k_components : int\n",
    "            Nombre de composantes principales à garder\n",
    "        \"\"\"\n",
    "        self.k_components = k_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.eigenvalues = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Apprendre les composantes principales\n",
    "        \n",
    "        Paramètres :\n",
    "        -----------\n",
    "        X : array de shape (n_samples, n_features)\n",
    "            Données d'entraînement\n",
    "        \"\"\"\n",
    "        n, p = X.shape\n",
    "        \n",
    "        # ÉTAPE 1 : Centrer les données\n",
    "        # VOTRE CODE\n",
    "        \n",
    "        # ÉTAPE 2 : Calculer la matrice de covariance\n",
    "        # VOTRE CODE\n",
    "        \n",
    "        # ÉTAPE 3 : Calculer valeurs et vecteurs propres\n",
    "        # VOTRE CODE\n",
    "        \n",
    "        # ÉTAPE 4 : Trier par valeurs propres décroissantes\n",
    "        # VOTRE CODE\n",
    "        \n",
    "        # ÉTAPE 5 : Sélectionner les k premières composantes\n",
    "        # VOTRE CODE\n",
    "        \n",
    "        print(f\"ACP entraînée avec {self.k_components} composantes\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Projeter les données dans le nouvel espace\n",
    "        \n",
    "        Paramètres :\n",
    "        -----------\n",
    "        X : array de shape (n_samples, n_features)\n",
    "        \n",
    "        Retourne :\n",
    "        ---------\n",
    "        Z : array de shape (n_samples, n_components)\n",
    "            Données projetées\n",
    "        \"\"\"\n",
    "        X_centered = X - self.mean\n",
    "        # Projection : Z = X_centered @ U\n",
    "        Z = # VOTRE CODE\n",
    "        return Z\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Apprendre et transformer en une seule étape\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, Z):\n",
    "        \"\"\"\n",
    "        Reconstruire les données originales (approximation)\n",
    "        \n",
    "        Paramètres :\n",
    "        -----------\n",
    "        Z : array de shape (n_samples, k_components)\n",
    "        \n",
    "        Retourne :\n",
    "        ---------\n",
    "        X_reconstructed : array de shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        return Z @ self.components.T + self.mean\n",
    "    \n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"Proportion de variance expliquée par chaque composante\"\"\"\n",
    "        total_variance = np.sum(self.eigenvalues)\n",
    "        return self.eigenvalues[:self.k_components] / total_variance\n",
    "    \n",
    "    def explained_variance(self):\n",
    "        \"\"\"Variance expliquée par chaque composante\"\"\"\n",
    "        return self.eigenvalues[:self.k_components]\n",
    "\n",
    "print(\"Classe PCA définie avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a7367-1ebb-431d-82f1-b3315100de6c",
   "metadata": {},
   "source": [
    "#### Test du code \n",
    "\n",
    "##### Génération de données\n",
    "\n",
    "Simulons quelques données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653fed1-346f-47b0-ba36-5d192b18f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Générer des données corrélées en 2D\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2.5],\n",
    "       [2.5, 2]]\n",
    "# ATTENTION : quand vous inventez une matrice de variance/covariance, vérifiez toujours qu’elle est semi-définie positive !\n",
    "X_2d = np.random.multivariate_normal(mean, cov, 200)\n",
    "\n",
    "print(f\"Données : {X_2d.shape[0]} points dans ℝ²\")\n",
    "print(f\"\\nPremiers points :\")\n",
    "print(X_2d[:5])\n",
    "\n",
    "# Visualiser les données originales\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Données originales en 2D', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b7c6-94d8-4394-aa24-919c439059b1",
   "metadata": {},
   "source": [
    "##### Application de notre algorithme ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d00919-0684-4905-965e-57f052e9587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(k_components=2)\n",
    "Z_2d = pca_2d.fit_transform(X_2d)\n",
    "\n",
    "print(\"\\nVariance expliquée par chaque composante :\")\n",
    "for i, var in enumerate(pca_2d.explained_variance()):\n",
    "    ratio = pca_2d.explained_variance_ratio()[i]\n",
    "    print(f\"  PC{i+1} : {var:.3f} ({ratio*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVariance totale expliquée : {np.sum(pca_2d.explained_variance_ratio())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af02b2b-a8d4-4437-88d3-8d0be91d12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation avec les composantes principales\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Gauche : données originales avec axes principaux\n",
    "ax1.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50)\n",
    "\n",
    "# Tracer les composantes principales\n",
    "colors = ['red', 'orange']\n",
    "for i in range(2):\n",
    "    v = pca_2d.components[:, i] * np.sqrt(pca_2d.eigenvalues[i]) * 3\n",
    "    ax1.arrow(0, 0, v[0], v[1], \n",
    "              head_width=0.3, head_length=0.4, \n",
    "              fc=colors[i], ec=colors[i], linewidth=3,\n",
    "              label=f'PC{i+1} (λ={pca_2d.eigenvalues[i]:.2f})')\n",
    "\n",
    "ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_xlabel('Feature 1', fontsize=12)\n",
    "ax1.set_ylabel('Feature 2', fontsize=12)\n",
    "ax1.set_title('Espace original avec axes principaux', fontsize=13, fontweight='bold')\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Droite : données dans le nouvel espace\n",
    "ax2.scatter(Z_2d[:, 0], Z_2d[:, 1], alpha=0.6, s=50, color='green')\n",
    "ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio()[0]*100:.1f}%)', fontsize=12)\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio()[1]*100:.1f}%)', fontsize=12)\n",
    "ax2.set_title('Nouvel espace (composantes principales)', fontsize=13, fontweight='bold')\n",
    "ax2.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Observation : Les axes sont maintenant alignés avec les directions de variance maximale !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb49407-f705-4b44-87fc-2a1948986159",
   "metadata": {},
   "source": [
    "##### On a quand même fait tout ça pour faire de la réduction de dimension. On y va ?\n",
    "\n",
    "Maintenant, utilisons l'ACP sur des données de dimensions (vraiment) plus élevées pour réduire la dimensionnalité de manière *vraiment* significative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea2849-ebea-4330-8613-2b5919201b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données en haute dimension\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "n_features = 50\n",
    "\n",
    "# Créer des données avec structure (certaines features corrélées)\n",
    "X_high = np.random.randn(n_samples, n_features)\n",
    "# Ajouter des corrélations\n",
    "for i in range(10):\n",
    "    X_high[:, i*5:(i+1)*5] = X_high[:, i*5:(i+1)*5] @ np.random.randn(5, 5)\n",
    "\n",
    "print(f\"Données : {X_high.shape[0]} échantillons × {X_high.shape[1]} features\")\n",
    "print(f\"Dimension originale : ℝ^{n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ffdc2-ce37-4458-a2a6-df10cb2f8682",
   "metadata": {},
   "source": [
    "n = 50 : là on commence à discuter !\n",
    "\n",
    "Jetons un œil à l’évolution de la variance expliquée en fonction du nombre de composantes considérées. À votre avis, on pourrait se satisfaire d’un modèle avec combien de composantes pour expliquer la variance de nos données ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557facc-0d78-46ac-b984-0f13737a7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'ACP avec différents nombres de composantes\n",
    "pca_full = PCA(k_components=n_features)\n",
    "pca_full.fit(X_high)\n",
    "\n",
    "# Variance expliquée\n",
    "var_ratio = pca_full.explained_variance_ratio()\n",
    "cumsum_var = np.cumsum(var_ratio)\n",
    "\n",
    "# Scree plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Variance par composante\n",
    "ax1.bar(range(1, 21), var_ratio[:20])\n",
    "ax1.set_xlabel('Composante', fontsize=12)\n",
    "ax1.set_ylabel('Proportion de variance expliquée', fontsize=12)\n",
    "ax1.set_title('Variance expliquée par composante', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Variance cumulée\n",
    "ax2.plot(range(1, n_features+1), cumsum_var, 'o-', linewidth=2, markersize=4)\n",
    "ax2.axhline(y=0.9, color='r', linestyle='--', linewidth=2, label='90% variance')\n",
    "ax2.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, label='95% variance')\n",
    "ax2.set_xlabel('Nombre de composantes', fontsize=12)\n",
    "ax2.set_ylabel('Variance cumulée', fontsize=12)\n",
    "ax2.set_title('Variance cumulée', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trouver le nombre de composantes pour 90% et 95%\n",
    "n_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "n_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nPour capturer 90% de la variance : {n_90} composantes\")\n",
    "print(f\"Pour capturer 95% de la variance : {n_95} composantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27a030-b04e-410a-9b3c-eca66ab483d9",
   "metadata": {},
   "source": [
    "#### Exercice 17 : Choix du nombre de composantes\n",
    "\n",
    "Utilisez l'ACP sur ces données haute dimension et :\n",
    "\n",
    "* Réduisez à 10 composantes et calculez la variance expliquée\n",
    "* Réduisez à 30 composantes et calculez la variance expliquée\n",
    "* Quel nombre de composantes recommanderiez-vous (la bonne réponse peut être une autre valeur que 10 ou 30, on a vue précédemment !) ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2a6f8-d7f2-4007-b488-446bdc974421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 17 : Votre code !\n",
    "\n",
    "# a) 10 composantes\n",
    "# VOTRE CODE\n",
    "\n",
    "print(f\"a) Avec 10 composantes :\")\n",
    "print(f\"   Dimension réduite : {Z_10.shape}\")\n",
    "print(f\"   Variance expliquée : {var_10*100:.2f}%\")\n",
    "\n",
    "# b) 30 composantes\n",
    "\n",
    "\n",
    "print(f\"\\nb) Avec 30 composantes :\")\n",
    "print(f\"   Dimension réduite : {Z_30.shape}\")\n",
    "print(f\"   Variance expliquée : {var_30*100:.2f}%\")\n",
    "\n",
    "# c) Recommandation\n",
    "print(f\"\\nc) Recommandation :\")\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb399f-f597-4bba-8731-16ad23fb2d4e",
   "metadata": {},
   "source": [
    "##### Qualité de la « reconstruction »\n",
    "\n",
    "Si on diminue le nombre de composantes, c’est pour disposer d’un modèle plus simple, mais qui reste proche de la réalité, complexe, à haute dimension. On peut se demander comment le fait de diminuer le nombre de composante impacte la capacité à « reconstruire » (faire la transformation inverse) les données originale à partir du modèle en dimension réduite. Le code suivant teste cela en essayant différentes reconstructions avec différentes valeurs de $k$. Pour mesurer l’erreur, on va utiliser l’indice MSE : Mean Square Error. Si vous n’êtes pas familier avec cette notion, on y reviendra plus tard dans le cours, lorsque l’on abordera le machine learning et la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb591f1-cffe-436a-bc87-d296263c6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction avec différents k\n",
    "\n",
    "def reconstruction_error(X, k_components):\n",
    "    \"\"\"Calculer l'erreur de reconstruction\"\"\"\n",
    "    pca = PCA(k_components=k_components)\n",
    "    Z = pca.fit_transform(X)\n",
    "    X_reconstructed = pca.inverse_transform(Z)\n",
    "    \n",
    "    # Erreur MSE\n",
    "    mse = np.mean((X - X_reconstructed)**2)\n",
    "    \n",
    "    return mse, pca.explained_variance_ratio().sum()\n",
    "\n",
    "# Tester différents nombres de composantes\n",
    "k_values = [5, 10, 20, 30, 40, 50]\n",
    "errors = []\n",
    "variances = []\n",
    "\n",
    "for k in k_values:\n",
    "    if k <= X_high.shape[1]:\n",
    "        mse, var = reconstruction_error(X_high, k)\n",
    "        errors.append(mse)\n",
    "        variances.append(var * 100)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Erreur de reconstruction\n",
    "ax1.plot(k_values, errors, 'o-', linewidth=2, markersize=8, color='red')\n",
    "ax1.set_xlabel('Nombre de composantes k', fontsize=12)\n",
    "ax1.set_ylabel('Erreur MSE', fontsize=12)\n",
    "ax1.set_title('Erreur de reconstruction', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Variance expliquée\n",
    "ax2.plot(k_values, variances, 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax2.axhline(y=90, color='r', linestyle='--', alpha=0.5, label='90%')\n",
    "ax2.axhline(y=95, color='orange', linestyle='--', alpha=0.5, label='95%')\n",
    "ax2.set_xlabel('Nombre de composantes k', fontsize=12)\n",
    "ax2.set_ylabel('Variance expliquée (%)', fontsize=12)\n",
    "ax2.set_title('Variance capturée', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Plus k est élevé, plus l'erreur diminue et plus de variance est capturée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec75a60-8df4-467d-b54f-0d5ffefec86f",
   "metadata": {},
   "source": [
    "### Jouons avec des données réélles : iris\n",
    "\n",
    "Utilisons l'ACP sur un dataset réel pour visualiser des données multidimensionnelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb17a0-e5a3-4161-b8f2-d18674bafff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset Iris\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Dataset Iris :\")\n",
    "print(f\"  {X_iris.shape[0]} échantillons\")\n",
    "print(f\"  {X_iris.shape[1]} features : {feature_names}\")\n",
    "print(f\"  {len(target_names)} classes : {target_names}\")\n",
    "print(f\"\\nPremières lignes :\")\n",
    "print(X_iris[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04765bb-4fef-4f16-ab4a-ec5fd5aaafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'ACP pour réduire à 2D\n",
    "pca_iris = PCA(k_components=2)\n",
    "X_iris_2d = pca_iris.fit_transform(X_iris)\n",
    "\n",
    "print(\"\\nVariance expliquée :\")\n",
    "for i, (var, ratio) in enumerate(zip(pca_iris.explained_variance(), \n",
    "                                     pca_iris.explained_variance_ratio())):\n",
    "    print(f\"  PC{i+1} : {var:.3f} ({ratio*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal : {pca_iris.explained_variance_ratio().sum()*100:.1f}% de la variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a22ab-4432-41bd-9fe9-6e346eebdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation en 2D avec les classes\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (color, marker, name) in enumerate(zip(colors, markers, target_names)):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_iris_2d[mask, 0], X_iris_2d[mask, 1],\n",
    "               c=color, marker=marker, s=100, alpha=0.7,\n",
    "               edgecolors='black', linewidth=0.5,\n",
    "               label=name)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_iris.explained_variance_ratio()[0]*100:.1f}%)', fontsize=13)\n",
    "plt.ylabel(f'PC2 ({pca_iris.explained_variance_ratio()[1]*100:.1f}%)', fontsize=13)\n",
    "plt.title('Dataset Iris projeté sur les 2 premières composantes principales', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nL'ACP permet de visualiser les 4 dimensions originales en 2D !\")\n",
    "print(\"   Les classes sont bien séparées dans cet espace réduit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf4101-7faf-4e66-8859-4c8c5f6968ab",
   "metadata": {},
   "source": [
    "## Connexions avec K-means (Chapitre 1)\n",
    "\n",
    "L'ACP et K-means sont **complémentaires** :\n",
    "\n",
    "1. **ACP avant K-means** : Réduire la dimension pour accélérer K-means\n",
    "2. **Visualisation** : Projeter les clusters en 2D/3D\n",
    "3. **Preprocessing** : Éliminer le bruit et les redondances\n",
    "\n",
    "**Pipeline typique** :\n",
    "```\n",
    "Données haute dimension\n",
    "    ↓\n",
    "ACP (réduction de dimension)\n",
    "    ↓\n",
    "K-means (clustering)\n",
    "    ↓\n",
    "Visualisation des clusters en 2D\n",
    "```\n",
    "\n",
    "Exercice final :\n",
    "\n",
    "1. Chargez Iris ou un dataset de votre choix\n",
    "2. Appliquez l'ACP pour réduire à 2-3 dimensions\n",
    "3. Appliquez K-means sur les données réduites\n",
    "4. Visualisez les clusters\n",
    "5. Comparez avec K-means sur les données originales\n",
    "\n",
    "**Questions** :\n",
    "- L'ACP améliore-t-elle les résultats de K-means ?\n",
    "- Combien de variance faut-il garder ?\n",
    "- Les clusters sont-ils plus nets en dimension réduite ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac5d60-717f-4aa3-a694-d27461284194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice final : votre code !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedda1c-995c-4661-b85f-0d593b122751",
   "metadata": {},
   "source": [
    "## Limites de la PCA\n",
    "\n",
    "La PCA ne fonctionne qu’avec des données où une projection linéaire peut résoudre le problème. Un exemple classique qui ne fonctionne pas est le fameux « swiss roll » :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344a99b-3701-4472-a47b-1ba339fb8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Paramètres du Swiss Roll\n",
    "n_points = 1500\n",
    "noise = 0.1\n",
    "\n",
    "# Génération du Swiss Roll\n",
    "# t : paramètre qui contrôle la position sur le rouleau (angle)\n",
    "t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_points))\n",
    "\n",
    "# Coordonnées 3D du Swiss Roll\n",
    "x = t * np.cos(t)\n",
    "y = 20 * np.random.rand(n_points)  # hauteur aléatoire\n",
    "z = t * np.sin(t)\n",
    "\n",
    "# Ajout de bruit\n",
    "x += noise * np.random.randn(n_points)\n",
    "z += noise * np.random.randn(n_points)\n",
    "\n",
    "# Création de la figure 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Tracé des points avec dégradé de couleur basé sur t (position sur le rouleau)\n",
    "scatter = ax.scatter(x, y, z, c=t, cmap='viridis', s=10, alpha=0.8)\n",
    "\n",
    "# Ajout de la barre de couleur\n",
    "cbar = fig.colorbar(scatter, ax=ax, shrink=0.6, pad=0.1)\n",
    "cbar.set_label('Position sur le rouleau (t)', fontsize=11)\n",
    "\n",
    "# Configuration des axes\n",
    "ax.set_xlabel('X', fontsize=11)\n",
    "ax.set_ylabel('Y', fontsize=11)\n",
    "ax.set_zlabel('Z', fontsize=11)\n",
    "ax.set_title('Swiss Roll - Exemple pour la réduction de dimension\\n' +\n",
    "             f'{n_points} points', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Ajustement de l'angle de vue\n",
    "ax.view_init(elev=15, azim=70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des informations dans la console\n",
    "print(\"=\" * 60)\n",
    "print(\"SWISS ROLL - GÉNÉRATION ET VISUALISATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nNombre de points : {n_points}\")\n",
    "print(f\"Bruit ajouté : {noise}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"FORMULES DE GÉNÉRATION\")\n",
    "print(\"-\" * 60)\n",
    "print(\"t ∈ [1.5π, 4.5π]  (paramètre angulaire)\")\n",
    "print(\"x = t × cos(t)\")\n",
    "print(\"y = aléatoire ∈ [0, 20]  (hauteur)\")\n",
    "print(\"z = t × sin(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b65bc7-50b2-40f7-89e7-24bc7e5d5c44",
   "metadata": {},
   "source": [
    "Le Swiss Roll est un exemple classique de **variété** non-linéaire :\n",
    "* en 3D, on distingue bien les données organisées sour la forme d’un rouleau\n",
    "* mais on perçoit bien que la structure intrinsèque est 2D (t, y) qui est « enroulée » dans l’espace\n",
    "* cet enroulement est mis en évidence par le dégradé de couleur qui montre la vraie « distance » entre les points sur le rouleau\n",
    "\n",
    "Pour résoudre ce type de problème, on va soit utiliser des techniques qui font des hyptothèse a priori sur la variété non-linéaire, soit être capable de gérer directement la non-linéarité, car la PCA sera en échec. On le voit facilement en réalisant une projection naïve des points sur (x, y), où la relation de « distance » entre les points est totalement ignorée par le modèle. Des points de même couleurs – proches sur le rouleau – se retrouvent éloignés sur la projection sur le plan, et des points éloignés sur le rouleau se retrouvent rapprochés sur la projection, qui a « superposé » les différents « plis » du rouleau (qui aurait plutôt dû être « déroulé ») :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e865749-033a-44de-868e-c076a5c6b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Paramètres du Swiss Roll\n",
    "n_points = 1500\n",
    "noise = 0.1\n",
    "\n",
    "# Génération du Swiss Roll\n",
    "t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_points))\n",
    "\n",
    "# Coordonnées 3D du Swiss Roll\n",
    "x = t * np.cos(t)\n",
    "y = 20 * np.random.rand(n_points)\n",
    "z = t * np.sin(t)\n",
    "\n",
    "# Ajout de bruit\n",
    "x += noise * np.random.randn(n_points)\n",
    "z += noise * np.random.randn(n_points)\n",
    "\n",
    "# Création de la figure avec deux sous-graphiques\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Sous-graphique 1 : Swiss Roll en 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "scatter1 = ax1.scatter(x, y, z, c=t, cmap='viridis', s=10, alpha=0.8)\n",
    "ax1.set_xlabel('X', fontsize=11)\n",
    "ax1.set_ylabel('Y', fontsize=11)\n",
    "ax1.set_zlabel('Z', fontsize=11)\n",
    "ax1.set_title('Swiss Roll en 3D', fontsize=12, fontweight='bold')\n",
    "ax1.view_init(elev=15, azim=70)\n",
    "\n",
    "# Sous-graphique 2 : Projection sur le plan XY\n",
    "ax2 = fig.add_subplot(122)\n",
    "scatter2 = ax2.scatter(x, y, c=t, cmap='viridis', s=10, alpha=0.8)\n",
    "ax2.set_xlabel('X', fontsize=11)\n",
    "ax2.set_ylabel('Y', fontsize=11)\n",
    "ax2.set_title('Projection sur le plan XY\\n(Réduction de dimension naïve)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Barre de couleur commune\n",
    "cbar = fig.colorbar(scatter2, ax=ax2, shrink=0.8)\n",
    "cbar.set_label('Position sur le rouleau (t)', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be5d46-a2e7-4818-b613-fa74fe45c075",
   "metadata": {},
   "source": [
    "Le problème ne vient pas de ce que la PCA pourrait géré ce qu’une projection naïve ne sait pas faire (elle sont de même nature, projection linéaire, qui n’arrive pas à capturer la structure non-linéaire des données). Essayez donc par vous-même une PCA sur ces données.\n",
    "\n",
    "Pour résoudre ce problème vous pouvez faire des recherches sur :\n",
    "* ACP à noyau (kernel-PCA)\n",
    "* t-SNE\n",
    "* Isomap (qui gère des distances géodésiques)\n",
    "* UMAP\n",
    "\n",
    "Pour aller plus loin, comparez votre implémentation de la PCA avec celle de scikit-learn (indice : `from sklearn.decomposition import PCA`)\n",
    "\n",
    "Pour les problèmes non-linéaires, recherchez la dans la documentation de scikit-learn le mot clé « manifold » (repli). Autres indices pour vous aider dans vos explorations :\n",
    "* `from sklearn.datasets import make_swiss_roll`\n",
    "* `from sklearn.manifold import Isomap, TSNE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dab3c-f273-46ff-9acb-33e80764b7f5",
   "metadata": {},
   "source": [
    "**En tout cas, bravo, vous êtes venu à bout d’un gros morceau, tant en matière de mathématiques que de modèle en data science !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b83376-e1e8-47ea-8e8f-7d23c27eb682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
