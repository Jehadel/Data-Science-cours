{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Système de recommandation musicale avec K-means et PCA\n",
    "\n",
    "Dans ce TP, vous allez découvrir comment combiner deux techniques fondamentales du machine learning :\n",
    "- **K-means** : algorithme de clustering pour regrouper des morceaux similaires\n",
    "- **PCA (Analyse en Composantes Principales)** : technique de réduction de dimensionnalité pour visualiser et améliorer le clustering\n",
    "\n",
    "Vous allez construire un système de recommandation musicale en analysant des caractéristiques audio de morceaux Spotify.\n",
    "\n",
    "## Plan du TP\n",
    "\n",
    "1. Exploration des données\n",
    "2. Première visualisation 3D de features sélectionnées\n",
    "3. Premier clustering K-means sur données brutes\n",
    "4. Amélioration avec mise à l'échelle des données\n",
    "5. Application de la PCA pour réduction de dimensionnalité\n",
    "6. Clustering optimisé avec K-means sur les composantes principales\n",
    "7. Détermination du nombre optimal de clusters\n",
    "8. Création de playlists personnalisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn==0.13.2\n",
    "#!pip install plotly==6.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configuration pour de meilleurs graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et exploration des données\n",
    "\n",
    "### 1.1 Chargement du dataset\n",
    "\n",
    "[Lien pour télécharger le dataset sur Kaggle](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "df = pd.read_csv(os.path.join('data','SpotifyTracksDataset','dataset.csv'))\n",
    "\n",
    "print(f\"Dimensions du dataset : {df.shape}\")\n",
    "print(f\"Nombre de morceaux : {df.shape[0]}\")\n",
    "print(f\"Nombre de colonnes : {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Aperçu des premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a une colonne `unnamed` qui correspond à l’index visiblement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Unnamed: 0',drop=True, inplace=True)\n",
    "df.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Informations sur le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Vérification des valeurs manquantes et doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Valeurs manquantes par colonne :\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans le dataset\")\n",
    "\n",
    "# Vérification des doublons\n",
    "n_duplicates = df.duplicated().sum()\n",
    "if n_duplicates > 0:\n",
    "    print(f\"\\n{n_duplicates} ligne(s) en doublon détectée(s)\")\n",
    "    print(f\"Pourcentage de doublons : {n_duplicates/len(df)*100:.2f}%\")\n",
    "    \n",
    "    # Vérifier les doublons sur track_id uniquement\n",
    "    n_duplicates_track = df.duplicated(subset=['track_id']).sum()\n",
    "    if n_duplicates_track > 0:\n",
    "        print(f\"\\nEt {n_duplicates_track} morceau(x) avec le même track_id\")\n",
    "        print(\"Supprimer les doublons avec df.drop_duplicates()\")\n",
    "else:\n",
    "    print(\"\\nAucun doublon détecté dans le dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Création d'un DataFrame avec uniquement les données numériques\n",
    "\n",
    "Pour notre analyse, nous allons nous concentrer sur les features audio quantitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes numériques pertinentes pour l'analyse audio\n",
    "#numeric_features = ['popularity', 'duration_ms', 'danceability', 'energy', 'key', \n",
    "#                   'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "#                   'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
    "#\n",
    "#data_num = df[numeric_features].copy()\n",
    "# ou\n",
    "data_num = df_cleaned.select_dtypes(exclude = ['object'])\n",
    "numeric_features = data_num.columns # on en aura besoin à la fin\n",
    "\n",
    "print(f\"Dimensions des données numériques : {data_num.shape}\")\n",
    "print(f\"\\nFeatures disponibles :\")\n",
    "for i, col in enumerate(data_num.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Matrice de corrélation\n",
    "\n",
    "Analysons les corrélations entre les différentes features audio pour comprendre leurs relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de corrélation\n",
    "correlation_matrix = data_num.corr()\n",
    "\n",
    "# Visualisation avec une heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matrice de corrélation des features audio Spotify', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations à noter :\")\n",
    "print(\"- Quelles variables sont fortement corrélées ?\")\n",
    "print(\"- Y a-t-il des corrélations négatives intéressantes ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lisez bien la description de chaque feature sur la page Kaggle pour d’une part comprendre ces corrélations, et d’autre part sélectionnez trois features (pas forcément corrélées) qu’il vous semble intéressant d’analyser de plus près afin de voir si elles permettraient de regrouper des morceaux ressemblant. Faisons-en une visualisation 3D pour voir comment ces features sont partagées.\n",
    "\n",
    "Par exemple danceability, energy, valence ou acousticness, instrumentalness, speechiness ou tout autre combinaison qui vous inspire/interpelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Première visualisation 3D avec trois features\n",
    "\n",
    "Commençons par visualiser les données en sélectionnant trois features intéressantes. Nous choisissons :\n",
    "- **danceability** : mesure de la facilité à danser sur un morceau (0.0 à 1.0)\n",
    "- **energy** : mesure de l'intensité et de l'activité (0.0 à 1.0)\n",
    "- **valence** : positivité musicale transmise, du triste (0.0) au joyeux (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D des données brutes\n",
    "fig = px.scatter_3d(data_num, \n",
    "                    x='danceability', \n",
    "                    y='energy', \n",
    "                    z='valence',\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title='Visualisation 3D : Danceability, Energy et Valence (données brutes)')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3, color='steelblue'))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : Les points semblent distribués de manière relativement uniforme.\")\n",
    "print(\"Aucune structure claire n'émerge visuellement de ces trois dimensions.\")\n",
    "print(\"En clair : on a affaire à un gros pâté.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Premier clustering K-means sur données brutes\n",
    "\n",
    "### 3.1 Choix du nombre de clusters\n",
    "\n",
    "Utilisons une règle empirique simple : pour N observations, on peut estimer le nombre de clusters optimal à environ √(N/2). Mais on va éviter d’avoir à considérer plus de 10 clusters (on se pose une limite dans notre exercice, pour se simplifier la vie, on pourrait aussi limiter le nombre d’observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de clusters selon la règle empirique\n",
    "n_samples = len(data_num)\n",
    "k_empirical = int(np.sqrt(n_samples / 2))\n",
    "k_empirical = max(5, min(k_empirical, 10))  # Contraindre entre 5 et 10\n",
    "\n",
    "print(f\"Nombre d'échantillons : {n_samples}\")\n",
    "print(f\"Nombre de clusters suggéré (règle empirique) : {k_empirical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Application de K-means sur les données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données brutes\n",
    "kmeans_raw = KMeans(n_clusters=k_empirical, random_state=42, n_init=10)\n",
    "labels_raw = kmeans_raw.fit_predict(data_num)\n",
    "\n",
    "print(f\"Clustering effectué avec {k_empirical} clusters\")\n",
    "print(f\"Inertie (somme des distances au carré aux centroïdes) : {kmeans_raw.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_raw, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_raw)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualisation du clustering sur données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame temporaire pour la visualisation\n",
    "df_viz_raw = data_num[['danceability', 'energy', 'valence']].copy()\n",
    "df_viz_raw['cluster'] = labels_raw.astype(str)\n",
    "\n",
    "# Visualisation 3D avec les clusters\n",
    "fig = px.scatter_3d(df_viz_raw,\n",
    "                    x='danceability',\n",
    "                    y='energy',\n",
    "                    z='valence',\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title=f'K-means sur données brutes ({k_empirical} clusters)')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nConstat : Les clusters se chevauchent significativement.\")\n",
    "print(\"La séparation n'est pas claire sur ces trois dimensions.\")\n",
    "print(\"Solution : normaliser les données pour que toutes les features contribuent équitablement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Amélioration avec mise à l'échelle des données\n",
    "\n",
    "### 4.1 Pourquoi scaler les données ?\n",
    "\n",
    "Les features ont des échelles différentes (ex: duration_ms en millisecondes vs danceability entre 0 et 1). \n",
    "Le K-means utilise la distance euclidienne, donc les features avec de grandes valeurs dominent le calcul.\n",
    "\n",
    "Nous utilisons **RobustScaler** qui est résistant aux outliers (utilise la médiane et l'IQR plutôt que la moyenne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à l'échelle avec RobustScaler\n",
    "scaler = RobustScaler()\n",
    "data_scaled = scaler.fit_transform(data_num)\n",
    "\n",
    "# Création d'un DataFrame pour faciliter l'analyse\n",
    "data_scaled_df = pd.DataFrame(data_scaled, columns=data_num.columns)\n",
    "\n",
    "print(\"Données mises à l'échelle avec RobustScaler\")\n",
    "print(f\"\\nAperçu des données scalées :\")\n",
    "print(data_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 K-means sur les données scalées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données scalées\n",
    "kmeans_scaled = KMeans(n_clusters=k_empirical, random_state=42, n_init=10)\n",
    "labels_scaled = kmeans_scaled.fit_predict(data_scaled)\n",
    "\n",
    "print(f\"Clustering effectué avec {k_empirical} clusters sur données scalées\")\n",
    "print(f\"Inertie : {kmeans_scaled.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_scaled, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_scaled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un gros déséquilibre dans la taille (nombre de morceaux) des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualisation avec données scalées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D avec les données scalées\n",
    "df_viz_scaled = data_scaled_df[['danceability', 'energy', 'valence']].copy()\n",
    "df_viz_scaled['cluster'] = labels_scaled.astype(str)\n",
    "\n",
    "fig = px.scatter_3d(df_viz_scaled,\n",
    "                    x='danceability',\n",
    "                    y='energy',\n",
    "                    z='valence',\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title=f'K-means sur données scalées ({k_empirical} clusters) - 3 features')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : L'amélioration est légère, mais visible.\")\n",
    "print(\"\\nElle tient surtout à ce qu’un gros cluster constitue >40% des données.\")\n",
    "print(\"Le problème principal : nous n'observons que 3 des 14 dimensions disponibles !\")\n",
    "print(\"\\nSolution : utiliser la PCA pour réduire toutes les dimensions en composantes\")\n",
    "print(\"principales qui capturent la variance maximale, tout en restant visualisables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse en Composantes Principales (PCA)\n",
    "\n",
    "### 5.1 PCA complète pour analyser la variance expliquée\n",
    "\n",
    "Commençons par faire une PCA sur toutes les composantes possibles pour voir combien de variance est capturée par chacune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA complète\n",
    "pca_full = PCA()\n",
    "pca_full.fit(data_scaled)\n",
    "\n",
    "# Variance expliquée par chaque composante\n",
    "variance_explained = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "print(\"Variance expliquée par les premières composantes :\")\n",
    "for i in range(min(10, len(variance_explained))):\n",
    "    print(f\"  PC{i+1} : {variance_explained[i]*100:.2f}% (cumulé : {cumulative_variance[i]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Graphique de la variance cumulée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la variance expliquée\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Variance par composante\n",
    "ax1.bar(range(1, len(variance_explained)+1), variance_explained, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Composante Principale', fontsize=12)\n",
    "ax1.set_ylabel('Variance expliquée', fontsize=12)\n",
    "ax1.set_title('Variance expliquée par composante', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Variance cumulée\n",
    "ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'o-', color='steelblue', linewidth=2)\n",
    "ax2.axhline(y=0.8, color='red', linestyle='--', label='80% de variance')\n",
    "ax2.axhline(y=0.9, color='orange', linestyle='--', label='90% de variance')\n",
    "ax2.set_xlabel('Nombre de composantes', fontsize=12)\n",
    "ax2.set_ylabel('Variance cumulée', fontsize=12)\n",
    "ax2.set_title('Variance cumulée expliquée', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trouver le nombre de composantes pour 80% et 90% de variance\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.8) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "\n",
    "print(f\"\\nRésultats :\")\n",
    "print(f\"  • Les 3 premières composantes expliquent {cumulative_variance[2]*100:.2f}% de la variance\")\n",
    "print(f\"  • Il faut {n_components_80} composantes pour expliquer >80% de la variance\")\n",
    "print(f\"  • Il faut {n_components_90} composantes pour expliquer >90% de la variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 PCA à 3 composantes pour visualisation\n",
    "\n",
    "Nous allons projeter nos données sur 3 composantes principales pour pouvoir les visualiser en 3D, de plus elles expliquent quasiment 90% de la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA à 3 composantes\n",
    "pca_3d = PCA(n_components=3)\n",
    "data_proj = pca_3d.fit_transform(data_scaled)\n",
    "\n",
    "print(f\"PCA effectuée : {data_scaled.shape[1]} dimensions → 3 composantes principales\")\n",
    "print(f\"\\nVariance expliquée par les 3 composantes :\")\n",
    "for i, var in enumerate(pca_3d.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i} : {var*100:.2f}%\")\n",
    "print(f\"\\nVariance totale expliquée : {pca_3d.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Création d'un DataFrame pour faciliter l'analyse\n",
    "df_pca = pd.DataFrame(data_proj, columns=['PC1', 'PC2', 'PC3'])\n",
    "print(f\"\\nDimensions des données projetées : {df_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualisation des données dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D dans l'espace PCA\n",
    "fig = px.scatter_3d(df_pca,\n",
    "                    x='PC1',\n",
    "                    y='PC2',\n",
    "                    z='PC3',\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title='Données projetées dans l\\'espace PCA (3 composantes)',\n",
    "                    labels={'PC1': f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "                           'PC2': f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "                           'PC3': f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)'})\n",
    "\n",
    "fig.update_traces(marker=dict(size=3, color='steelblue'))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : La projection PCA révèle une structure plus étalée des données.\")\n",
    "print(\"Les composantes principales capturent les directions de variance maximale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Analyse des composantes principales\n",
    "\n",
    "Voyons quelles features originales contribuent le plus à chaque composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame des composantes\n",
    "components_df = pd.DataFrame(\n",
    "    pca_3d.components_.T,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=data_num.columns\n",
    ")\n",
    "\n",
    "# Visualisation des contributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, pc in enumerate(['PC1', 'PC2', 'PC3']):\n",
    "    # Trier par valeur absolue pour voir les contributions importantes\n",
    "    sorted_features = components_df[pc].abs().sort_values(ascending=True)\n",
    "    colors = ['red' if x < 0 else 'steelblue' for x in components_df.loc[sorted_features.index, pc]]\n",
    "    \n",
    "    axes[i].barh(range(len(sorted_features)), \n",
    "                 components_df.loc[sorted_features.index, pc],\n",
    "                 color=colors, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(sorted_features)))\n",
    "    axes[i].set_yticklabels(sorted_features.index)\n",
    "    axes[i].set_xlabel('Contribution', fontsize=11)\n",
    "    axes[i].set_title(f'{pc}\\n({pca_3d.explained_variance_ratio_[i]*100:.1f}% variance)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[i].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterprétation des composantes principales :\")\n",
    "print(\"\\nPC1 - Principales contributions :\")\n",
    "top_pc1 = components_df['PC1'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc1.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC1']:.3f}\")\n",
    "\n",
    "print(\"\\nPC2 - Principales contributions :\")\n",
    "top_pc2 = components_df['PC2'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc2.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC2']:.3f}\")\n",
    "\n",
    "print(\"\\nPC3 - Principales contributions :\")\n",
    "top_pc3 = components_df['PC3'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc3.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC3']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-means optimisé sur les composantes principales\n",
    "\n",
    "### 6.1 Clustering dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données projetées PCA\n",
    "kmeans_pca = KMeans(n_clusters=k_empirical, random_state=42, n_init=10)\n",
    "labels_pca = kmeans_pca.fit_predict(data_proj)\n",
    "\n",
    "print(f\"K-means effectué sur l'espace PCA avec {k_empirical} clusters\")\n",
    "print(f\"Inertie : {kmeans_pca.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_pca, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_pca)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualisation du clustering dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation avec les clusters dans l'espace PCA\n",
    "df_pca_clustered = df_pca.copy()\n",
    "df_pca_clustered['cluster'] = labels_pca.astype(str)\n",
    "\n",
    "fig = px.scatter_3d(df_pca_clustered,\n",
    "                    x='PC1',\n",
    "                    y='PC2',\n",
    "                    z='PC3',\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=900,\n",
    "                    height=700,\n",
    "                    title=f'K-means dans l\\'espace PCA ({k_empirical} clusters)',\n",
    "                    labels={'PC1': f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "                           'PC2': f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "                           'PC3': f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)'})\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : Les clusters sont bien plus distincts dans l'espace PCA !\")\n",
    "print(\"La PCA a permis de révéler la structure sous-jacente des données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Retour aux features originales\n",
    "\n",
    "Visualisons maintenant ces clusters dans l'espace des features originales (danceability, energy, valence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des clusters PCA dans l'espace original\n",
    "df_original_clustered = data_num[['danceability', 'energy', 'valence']].copy()\n",
    "df_original_clustered['cluster'] = labels_pca.astype(str)\n",
    "\n",
    "fig = px.scatter_3d(df_original_clustered,\n",
    "                    x='danceability',\n",
    "                    y='energy',\n",
    "                    z='valence',\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=900,\n",
    "                    height=700,\n",
    "                    title=f'Clusters PCA visualisés dans l\\'espace original (danceability, energy, valence)')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nInsight important :\")\n",
    "print(\"Même dans l'espace original 3D, on voit maintenant une structure émergente (si on dézoome un peu).\")\n",
    "print(\"Les clusters basés sur TOUTES les features (via PCA) révèlent des patterns\")\n",
    "print(\"qui n'étaient pas visibles en ne considérant que 3 dimensions arbitraires.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Détermination du nombre optimal de clusters\n",
    "\n",
    "### 7.1 Méthode du coude (Elbow method)\n",
    "\n",
    "Testons différents nombres de clusters et analysons l'inertie (somme des distances au carré aux centroïdes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de différents nombres de clusters\n",
    "K_range = range(1, 21)\n",
    "inertias = []\n",
    "\n",
    "print(\"Calcul de l'inertie pour différents nombres de clusters...\")\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(data_proj)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    if k % 5 == 0:\n",
    "        print(f\"  k={k}: inertie={kmeans_temp.inertia_:.2f}\")\n",
    "\n",
    "print(\"\\nCalculs terminés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visualisation de la courbe du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'inertie\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(K_range, inertias, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xlabel('Nombre de clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertie (somme des distances²)', fontsize=12)\n",
    "plt.title('Méthode du coude pour déterminer le nombre optimal de clusters', fontsize=14, pad=20)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(K_range)\n",
    "\n",
    "# Marquer la position que nous avions choisie arbitrairement\n",
    "plt.axvline(x=k_empirical, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Suggestion arbitraire (k={k_empirical})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse de la courbe du coude :\")\n",
    "print(\"Le 'coude' représente le point où ajouter des clusters supplémentaires\")\n",
    "print(\"n'apporte plus d'amélioration significative.\")\n",
    "print(\"\\nQuestion : Quel nombre de clusters vous semble optimal ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Calcul du taux de décroissance de l'inertie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la dérivée seconde pour identifier le coude automatiquement\n",
    "# Plus le taux de décroissance change, plus c'est un bon candidat\n",
    "differences = np.diff(inertias)\n",
    "differences_2nd = np.diff(differences)\n",
    "\n",
    "# Le point optimal est là où la seconde dérivée est maximale (coude le plus prononcé)\n",
    "optimal_k = np.argmax(differences_2nd) + 2  # +2 car on a fait 2 diff\n",
    "\n",
    "print(f\"\\nAnalyse automatique :\")\n",
    "print(f\"Nombre optimal de clusters suggéré : {optimal_k}\")\n",
    "print(f\"\\nDécroissance de l'inertie entre clusters :\")\n",
    "for i in range(min(10, len(differences))):\n",
    "    k = i + 2\n",
    "    pct_decrease = -differences[i] / inertias[i] * 100\n",
    "    print(f\"  k={k-1}→{k} : {-differences[i]:.2f} (-{pct_decrease:.1f}%)\")\n",
    "\n",
    "print(\"\\nOn se rend compte que la méthode automatique n’est pas très satisfaisante,\")\n",
    "print(\"c’est à partir de 7 clusters qu’à vu de nez l’inertie ne décroît plus vraiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Clustering final avec le nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir le nombre final de clusters (vous pouvez modifier cette valeur)\n",
    "k_final = 7\n",
    "\n",
    "print(f\"Nombre final de clusters choisi : {k_final}\")\n",
    "print(\"\\nEntraînement du modèle K-means final...\")\n",
    "\n",
    "# K-means final\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "labels_final = kmeans_final.fit_predict(data_proj)\n",
    "\n",
    "print(f\"\\nClustering final effectué avec {k_final} clusters\")\n",
    "print(f\"Inertie finale : {kmeans_final.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition finale des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_final, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_final)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Visualisation finale du clustering optimisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation dans l'espace PCA avec le clustering final\n",
    "df_pca_final = df_pca.copy()\n",
    "df_pca_final['cluster'] = labels_final.astype(str)\n",
    "\n",
    "fig = px.scatter_3d(df_pca_final,\n",
    "                    x='PC1',\n",
    "                    y='PC2',\n",
    "                    z='PC3',\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=900,\n",
    "                    height=700,\n",
    "                    title=f'Clustering final optimisé ({k_final} clusters) - Espace PCA',\n",
    "                    labels={'PC1': f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "                           'PC2': f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "                           'PC3': f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)'})\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Création de playlists personnalisées\n",
    "\n",
    "### 8.1 Analyse des caractéristiques de chaque cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les labels de cluster au DataFrame original\n",
    "df_with_clusters = df_cleaned.copy()\n",
    "df_with_clusters['cluster'] = labels_final\n",
    "\n",
    "# Calculer les moyennes des features pour chaque cluster\n",
    "cluster_profiles = df_with_clusters.groupby('cluster')[numeric_features].mean()\n",
    "\n",
    "print(\"Profil moyen de chaque cluster :\")\n",
    "print(\"=\"*80)\n",
    "print(cluster_profiles.round(3))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualisation des profils de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de features clés pour la visualisation\n",
    "key_features = ['danceability', 'energy', 'valence', 'acousticness', 'instrumentalness', 'tempo']\n",
    "\n",
    "# Normalisation pour la visualisation en radar\n",
    "cluster_profiles_norm = cluster_profiles[key_features].copy()\n",
    "for col in key_features:\n",
    "    if col == 'tempo':\n",
    "        # Normaliser tempo sur [0, 1]\n",
    "        cluster_profiles_norm[col] = (cluster_profiles_norm[col] - cluster_profiles_norm[col].min()) / \\\n",
    "                                      (cluster_profiles_norm[col].max() - cluster_profiles_norm[col].min())\n",
    "\n",
    "# Heatmap des profils de clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_profiles_norm.T, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Valeur normalisée'}, linewidths=0.5)\n",
    "plt.title(f'Profils des {k_final} clusters musicaux', fontsize=14, pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Features audio', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterprétation : Chaque cluster a un profil musical distinct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Caractérisation automatique des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour caractériser un cluster\n",
    "def characterize_cluster(cluster_id, profile):\n",
    "    \"\"\"Génère une description textuelle d'un cluster basée sur ses caractéristiques.\"\"\"\n",
    "    characteristics = []\n",
    "    \n",
    "    # Danceability\n",
    "    if profile['danceability'] > 0.7:\n",
    "        characteristics.append(\"très dansant\")\n",
    "    elif profile['danceability'] > 0.5:\n",
    "        characteristics.append(\"dansant\")\n",
    "    else:\n",
    "        characteristics.append(\"peu dansant\")\n",
    "    \n",
    "    # Energy\n",
    "    if profile['energy'] > 0.7:\n",
    "        characteristics.append(\"énergique\")\n",
    "    elif profile['energy'] > 0.5:\n",
    "        characteristics.append(\"modérément énergique\")\n",
    "    else:\n",
    "        characteristics.append(\"calme\")\n",
    "    \n",
    "    # Valence\n",
    "    if profile['valence'] > 0.6:\n",
    "        characteristics.append(\"joyeux\")\n",
    "    elif profile['valence'] > 0.4:\n",
    "        characteristics.append(\"neutre\")\n",
    "    else:\n",
    "        characteristics.append(\"mélancolique\")\n",
    "    \n",
    "    # Acousticness\n",
    "    if profile['acousticness'] > 0.6:\n",
    "        characteristics.append(\"acoustique\")\n",
    "    elif profile['acousticness'] < 0.3:\n",
    "        characteristics.append(\"électronique\")\n",
    "    \n",
    "    # Instrumentalness\n",
    "    if profile['instrumentalness'] > 0.5:\n",
    "        characteristics.append(\"instrumental\")\n",
    "    \n",
    "    # Tempo\n",
    "    if profile['tempo'] > 140:\n",
    "        characteristics.append(\"rapide\")\n",
    "    elif profile['tempo'] < 90:\n",
    "        characteristics.append(\"lent\")\n",
    "    \n",
    "    return \", \".join(characteristics)\n",
    "\n",
    "# Générer les descriptions pour chaque cluster\n",
    "print(\"\\nCaractérisation des clusters musicaux :\\n\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in range(k_final):\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    description = characterize_cluster(cluster_id, profile)\n",
    "    n_songs = (labels_final == cluster_id).sum()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({n_songs} morceaux)\")\n",
    "    print(f\"   Style : {description.capitalize()}\")\n",
    "    print(f\"   Caractéristiques :\")\n",
    "    print(f\"     • Danceability : {profile['danceability']:.2f}\")\n",
    "    print(f\"     • Energy       : {profile['energy']:.2f}\")\n",
    "    print(f\"     • Valence      : {profile['valence']:.2f}\")\n",
    "    print(f\"     • Tempo        : {profile['tempo']:.0f} BPM\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Génération de playlists thématiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer une playlist à partir d'un cluster\n",
    "def create_playlist(cluster_id, n_songs=10, seed=None):\n",
    "    \"\"\"Crée une playlist de n_songs morceaux du cluster spécifié.\"\"\"\n",
    "    cluster_songs = df_with_clusters[df_with_clusters['cluster'] == cluster_id]\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_songs = min(n_songs, len(cluster_songs))\n",
    "    playlist = cluster_songs.sample(n=n_songs)\n",
    "    \n",
    "    return playlist[['track_name', 'artists', 'danceability', 'energy', 'valence', 'tempo']]\n",
    "\n",
    "# Créer des playlists pour quelques clusters\n",
    "print(\"\\nExemples de playlists générées :\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in range(min(3, k_final)):  # Afficher les 3 premiers clusters\n",
    "    print(f\"\\nPlaylist du Cluster {cluster_id}\")\n",
    "    playlist = create_playlist(cluster_id, n_songs=5, seed=42)\n",
    "    print(playlist.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Système de recommandation basé sur un morceau\n",
    "\n",
    "Créons une fonction qui recommande des morceaux similaires à un morceau donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_songs(track_name, n_recommendations=10):\n",
    "    \"\"\"Recommande des morceaux similaires basés sur le même cluster.\"\"\"\n",
    "    # Trouver le morceau\n",
    "    track_matches = df_with_clusters[df_with_clusters['track_name'].str.contains(track_name, case=False, na=False)]\n",
    "    \n",
    "    if len(track_matches) == 0:\n",
    "        print(f\"Aucun morceau trouvé contenant '{track_name}'\")\n",
    "        return None\n",
    "    \n",
    "    # Prendre le premier match\n",
    "    track = track_matches.iloc[0]\n",
    "    cluster_id = track['cluster']\n",
    "    \n",
    "    print(f\"\\nMorceau de référence : {track['track_name']} - {track['artists']}\")\n",
    "    print(f\"   Cluster : {cluster_id}\")\n",
    "    print(f\"   Danceability: {track['danceability']:.2f} | Energy: {track['energy']:.2f} | Valence: {track['valence']:.2f}\")\n",
    "    \n",
    "    # Trouver d'autres morceaux du même cluster\n",
    "    similar_songs = df_with_clusters[\n",
    "        (df_with_clusters['cluster'] == cluster_id) & \n",
    "        (df_with_clusters['track_id'] != track['track_id'])\n",
    "    ]\n",
    "    \n",
    "    # Calculer la distance dans l'espace PCA pour affiner\n",
    "    track_idx = track.name\n",
    "    track_pca = data_proj[track_idx].reshape(1, -1)\n",
    "    \n",
    "    similar_indices = similar_songs.index\n",
    "    if len(similar_indices) > 0:\n",
    "        distances = np.linalg.norm(data_proj[similar_indices] - track_pca, axis=1)\n",
    "        similar_songs = similar_songs.copy()\n",
    "        similar_songs['distance'] = distances\n",
    "        similar_songs = similar_songs.sort_values('distance')\n",
    "    \n",
    "    # Retourner les n recommandations\n",
    "    n_recommendations = min(n_recommendations, len(similar_songs))\n",
    "    recommendations = similar_songs.head(n_recommendations)\n",
    "    \n",
    "    print(f\"\\nTop {n_recommendations} recommandations similaires :\\n\")\n",
    "    print(recommendations[['track_name', 'artists', 'danceability', 'energy', 'valence']].to_string(index=False))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Exemple de recommandations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Choisir un morceau aléatoire pour la démonstration\n",
    "random_track = df['track_name'].sample(1, random_state=42).values[0]\n",
    "recommendations = recommend_similar_songs(random_track, n_recommendations=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Export des playlists\n",
    "\n",
    "Exportons les playlists pour chaque cluster dans des fichiers CSV séparés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un répertoire pour les playlists\n",
    "import os\n",
    "os.makedirs('playlists', exist_ok=True)\n",
    "\n",
    "# Exporter chaque cluster dans un fichier CSV\n",
    "for cluster_id in range(k_final):\n",
    "    cluster_songs = df_with_clusters[df_with_clusters['cluster'] == cluster_id]\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    description = characterize_cluster(cluster_id, profile)\n",
    "    \n",
    "    filename = f'playlists/cluster_{cluster_id}_{description.replace(\",\", \"\").replace(\" \", \"_\")}.csv'\n",
    "    cluster_songs.to_csv(filename, index=False)\n",
    "    print(f\"Playlist du Cluster {cluster_id} exportée : {filename}\")\n",
    "\n",
    "print(f\"\\n{k_final} playlists exportées dans le dossier 'playlists/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion et perspectives\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **K-means sur données brutes** : Performance limitée car les features ont des échelles différentes\n",
    "\n",
    "2. **Mise à l'échelle** : Amélioration modeste mais importante pour égaliser l'influence des features\n",
    "\n",
    "3. **PCA + K-means** : Combinaison puissante qui :\n",
    "   - Réduit la dimensionnalité tout en conservant l'information essentielle\n",
    "   - Révèle la structure sous-jacente des données\n",
    "   - Améliore significativement la qualité du clustering\n",
    "\n",
    "4. **Méthode du coude** : Permet de déterminer objectivement le nombre optimal de clusters\n",
    "\n",
    "### Applications pratiques\n",
    "\n",
    "Ce système de recommandation peut être utilisé pour :\n",
    "- Générer des playlists automatiques cohérentes\n",
    "- Suggérer de nouveaux morceaux similaires aux goûts d'un utilisateur\n",
    "- Organiser une bibliothèque musicale par style/ambiance\n",
    "- Créer des transitions fluides dans des DJ sets\n",
    "\n",
    "### Pistes d'amélioration\n",
    "\n",
    "1. **Tester d'autres algorithmes** : DBSCAN, Hierarchical Clustering, GMM\n",
    "2. **Incorporer des données supplémentaires** : genre musical, année de sortie, paroles\n",
    "3. **Utiliser des métriques d'évaluation** : Silhouette Score, Davies-Bouldin Index\n",
    "4. **Créer un système hybride** : combiner clustering et collaborative filtering\n",
    "5. **Interface utilisateur** : développer une application web interactive\n",
    "\n",
    "### Exercices complémentaires\n",
    "\n",
    "1. Comparez les résultats avec différents scalers (StandardScaler, MinMaxScaler)\n",
    "2. Testez la PCA avec un nombre différent de composantes\n",
    "3. Analysez les morceaux mal classés (outliers)\n",
    "4. Créez des playlists thématiques (sport, détente, fête, travail)\n",
    "5. Implémentez une fonction de \"découverte\" qui suggère des morceaux de clusters adjacents\n",
    "6. Cherchez une base de données et créez un système de recommandation pour les films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. À vous de jouer !\n",
    "\n",
    "Utilisez les cellules ci-dessous pour expérimenter et créer vos propres playlists personnalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule d'expérimentation libre\n",
    "# Testez vos propres recommandations ici !\n",
    "\n",
    "# Exemple : recommander_similar_songs(\"votre_morceau_préféré\", n_recommendations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez votre propre playlist personnalisée en mélangeant plusieurs clusters\n",
    "# Exemple : combiner des morceaux énergiques et joyeux\n",
    "\n",
    "# custom_playlist = pd.concat([\n",
    "#     create_playlist(cluster_1, n_songs=5),\n",
    "#     create_playlist(cluster_2, n_songs=5)\n",
    "# ])\n",
    "# print(custom_playlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
