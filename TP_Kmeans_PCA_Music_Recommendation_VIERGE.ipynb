{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Système de recommandation musicale avec K-means et PCA\n",
    "\n",
    "Dans ce TP, vous allez découvrir comment combiner deux techniques fondamentales du machine learning :\n",
    "- **K-means** : algorithme de clustering pour regrouper des morceaux similaires\n",
    "- **PCA (Analyse en Composantes Principales)** : technique de réduction de dimensionnalité pour visualiser et améliorer le clustering\n",
    "\n",
    "Vous allez construire un système de recommandation musicale en analysant des caractéristiques audio de morceaux Spotify.\n",
    "\n",
    "## Plan du TP\n",
    "\n",
    "1. Exploration des données\n",
    "2. Première visualisation 3D de features sélectionnées\n",
    "3. Premier clustering K-means sur données brutes\n",
    "4. Amélioration avec mise à l'échelle des données\n",
    "5. Application de la PCA pour réduction de dimensionnalité\n",
    "6. Clustering optimisé avec K-means sur les composantes principales\n",
    "7. Détermination du nombre optimal de clusters\n",
    "8. Création de playlists personnalisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn==0.13.2\n",
    "#!pip install plotly==6.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configuration pour de meilleurs graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et exploration des données\n",
    "\n",
    "### 1.1 Chargement du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lien pour télécharger le dataset sur Kaggle](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset - adapter en fonction du dossier où vous avez téléchargé les données\n",
    "df = pd.read_csv(os.path.join('data','SpotifyTracksDataset','dataset.csv'))\n",
    "\n",
    "print(f\"Dimensions du dataset : {df.shape}\")\n",
    "print(f\"Nombre de morceaux : {df.shape[0]}\")\n",
    "print(f\"Nombre de colonnes : {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procédez dans un premier temps à une exploration minimal classique des données (EDA).\n",
    "Il faudra certainement faire un peu de nettoyage, comme beaucoup de données téléchargées sur Kaggle, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Aperçu des premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a une colonne `unnamed` qui correspond à l’index visiblement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Unnamed: 0',drop=True, inplace=True)\n",
    "df.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Informations sur le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Vérification des valeurs manquantes et doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Création d'un DataFrame avec uniquement les données numériques\n",
    "\n",
    "Pour notre analyse, nous allons nous concentrer sur les features audio quantitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes numériques pertinentes pour l'analyse audio\n",
    "\n",
    "data_num = # VOTRE CODE\n",
    "\n",
    "print(f\"Dimensions des données numériques : {data_num.shape}\")\n",
    "print(f\"\\nFeatures disponibles :\")\n",
    "for i, col in enumerate(data_num.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Matrice de corrélation\n",
    "\n",
    "Analysons les corrélations entre les différentes features audio pour comprendre leurs relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de corrélation\n",
    "correlation_matrix = # VOTRE CODE\n",
    "\n",
    "# Visualisation avec une heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matrice de corrélation des features audio Spotify', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations à noter :\")\n",
    "print(\"- Quelles variables sont fortement corrélées ?\")\n",
    "print(\"- Y a-t-il des corrélations négatives intéressantes ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lisez bien la description de chaque feature sur la page Kaggle pour d’une part comprendre ces corrélations, et d’autre part sélectionnez trois features (pas forcément corrélées) qu’il vous semble intéressant d’analyser de plus près afin de voir si elles permettraient de regrouper des morceaux ressemblant. Faisons-en une visualisation 3D pour voir comment ces features sont partagées.\n",
    "\n",
    "Par exemple danceability, energy, valence ou acousticness, instrumentalness, speechiness ou tout autre combinaison qui vous inspire/interpelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Première visualisation 3D avec trois features\n",
    "\n",
    "Décrivez les 3 features que vous avez choisies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D des données brutes\n",
    "fig = px.scatter_3d(data_num, \n",
    "                    x= # VOTRE CODE \n",
    "                    y=# VOTRE CODE\n",
    "                    z=# VOTRE CODE\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title='Visualisation 3D : Danceability, Energy et Valence (données brutes)')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3, color='steelblue'))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : .\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Premier clustering K-means sur données brutes\n",
    "\n",
    "### 3.1 Choix du nombre de clusters\n",
    "\n",
    "Utilisons une règle empirique simple : pour N observations, on peut estimer le nombre de clusters optimal à environ √(N/2). Mais on va éviter d’avoir à considérer plus de 10 clusters (on se pose une limite dans notre exercice, pour se simplifier la vie, on pourrait aussi limiter le nombre d’observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du nombre de clusters selon la règle empirique\n",
    "n_samples = len(data_num)\n",
    "k_empirical = int(np.sqrt(n_samples / 2))\n",
    "k_empirical = max(5, min(k_empirical, 10))  # Contraindre entre 5 et 10\n",
    "\n",
    "print(f\"Nombre d'échantillons : {n_samples}\")\n",
    "print(f\"Nombre de clusters suggéré (règle empirique) : {k_empirical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Application de K-means sur les données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données brutes\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualisation du clustering sur données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame temporaire pour la visualisation\n",
    "df_viz_raw = # VOTRE CODE\n",
    "\n",
    "# Visualisation 3D avec les clusters\n",
    "fig = px.scatter_3d(df_viz_raw,\n",
    "                    x=# VOTRE CODE\n",
    "                    y=# VOTRE CODE\n",
    "                    z=# VOTRE CODE\n",
    "                    color='cluster',\n",
    "                    opacity=0.7,\n",
    "                    width=800,\n",
    "                    height=700,\n",
    "                    title=f'K-means sur données brutes ({k_empirical} clusters)')\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nConstat : .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Amélioration avec mise à l'échelle des données\n",
    "\n",
    "### 4.1 Pourquoi scaler les données ?\n",
    "\n",
    "Les features ont des échelles différentes (ex: duration_ms en millisecondes vs danceability entre 0 et 1). \n",
    "Le K-means utilise la distance euclidienne, donc les features avec de grandes valeurs dominent le calcul.\n",
    "\n",
    "Nous utilisons **RobustScaler** qui est résistant aux outliers (utilise la médiane et l'IQR plutôt que la moyenne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à l'échelle avec RobustScaler\n",
    "scaler = # VOTRE CODE\n",
    "data_scaled = # VOTRE CODE\n",
    "\n",
    "# Création d'un DataFrame pour faciliter l'analyse\n",
    "data_scaled_df = pd.DataFrame(data_scaled, columns=data_num.columns)\n",
    "\n",
    "print(\"Données mises à l'échelle avec RobustScaler\")\n",
    "print(f\"\\nAperçu des données scalées :\")\n",
    "print(data_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 K-means sur les données scalées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données scalées\n",
    "kmeans_scaled = # VOTRE CODE\n",
    "labels_scaled = # VOTRE CODE\n",
    "\n",
    "print(f\"Clustering effectué avec {k_empirical} clusters sur données scalées\")\n",
    "print(f\"Inertie : {kmeans_scaled.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_scaled, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_scaled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un gros déséquilibre dans la taille (nombre de morceaux) des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualisation avec données scalées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D avec les données scalées\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "print(\"\\nObservation : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse en Composantes Principales (PCA)\n",
    "\n",
    "### 5.1 PCA complète pour analyser la variance expliquée\n",
    "\n",
    "Commençons par faire une PCA sur toutes les composantes possibles pour voir combien de variance est capturée par chacune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA complète\n",
    "pca_full = # VOTRE CODE\n",
    "# Variance expliquée par chaque composante\n",
    "variance_explained = # VOTRE CODE\n",
    "cumulative_variance = # VOTRE CODE\n",
    "\n",
    "print(\"Variance expliquée par les premières composantes :\")\n",
    "for i in range(min(10, len(variance_explained))):\n",
    "    print(f\"  PC{i+1} : {variance_explained[i]*100:.2f}% (cumulé : {cumulative_variance[i]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Graphique de la variance cumulée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la variance expliquée\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Variance par composante (fig ax1)\n",
    "# VOTRE CODE\n",
    "\n",
    "# Variance cumulée (fig ax2)\n",
    "# VOTRE CODE\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trouver le nombre de composantes pour 80% et 90% de variance\n",
    "n_components_80 = # VOTRE CODE\n",
    "n_components_90 = # VOTRE CODE\n",
    "\n",
    "print(f\"\\nRésultats :\")\n",
    "print(f\"  • Les 3 premières composantes expliquent {cumulative_variance[2]*100:.2f}% de la variance\")\n",
    "print(f\"  • Il faut {n_components_80} composantes pour expliquer >80% de la variance\")\n",
    "print(f\"  • Il faut {n_components_90} composantes pour expliquer >90% de la variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 PCA à 3 composantes pour visualisation\n",
    "\n",
    "Nous allons projeter nos données sur 3 composantes principales pour pouvoir les visualiser en 3D, de plus elles expliquent quasiment 90% de la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA à 3 composantes\n",
    "pca_3d = # VOTRE CODE\n",
    "data_proj = # VOTRE CODE\n",
    "\n",
    "print(f\"PCA effectuée : {data_scaled.shape[1]} dimensions → 3 composantes principales\")\n",
    "print(f\"\\nVariance expliquée par les 3 composantes :\")\n",
    "for i, var in enumerate(pca_3d.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i} : {var*100:.2f}%\")\n",
    "print(f\"\\nVariance totale expliquée : {pca_3d.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Création d'un DataFrame pour faciliter l'analyse\n",
    "df_pca = pd.DataFrame(data_proj, columns=['PC1', 'PC2', 'PC3'])\n",
    "print(f\"\\nDimensions des données projetées : {df_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualisation des données dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D dans l'espace PCA\n",
    "fig = # VOTRE CODE\n",
    "fig.update_traces(marker=dict(size=3, color='steelblue'))\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nObservation : La projection PCA révèle une structure plus étalée des données.\")\n",
    "print(\"Les composantes principales capturent les directions de variance maximale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Analyse des composantes principales\n",
    "\n",
    "Voyons quelles features originales contribuent le plus à chaque composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame des composantes\n",
    "components_df = pd.DataFrame(\n",
    "    pca_3d.components_.T,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=data_num.columns\n",
    ")\n",
    "\n",
    "# Visualisation des contributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, pc in enumerate(['PC1', 'PC2', 'PC3']):\n",
    "    # Trier par valeur absolue pour voir les contributions importantes\n",
    "    sorted_features = components_df[pc].abs().sort_values(ascending=True)\n",
    "    colors = ['red' if x < 0 else 'steelblue' for x in components_df.loc[sorted_features.index, pc]]\n",
    "    \n",
    "    axes[i].barh(range(len(sorted_features)), \n",
    "                 components_df.loc[sorted_features.index, pc],\n",
    "                 color=colors, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(sorted_features)))\n",
    "    axes[i].set_yticklabels(sorted_features.index)\n",
    "    axes[i].set_xlabel('Contribution', fontsize=11)\n",
    "    axes[i].set_title(f'{pc}\\n({pca_3d.explained_variance_ratio_[i]*100:.1f}% variance)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[i].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterprétation des composantes principales :\")\n",
    "print(\"\\nPC1 - Principales contributions :\")\n",
    "top_pc1 = components_df['PC1'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc1.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC1']:.3f}\")\n",
    "\n",
    "print(\"\\nPC2 - Principales contributions :\")\n",
    "top_pc2 = components_df['PC2'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc2.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC2']:.3f}\")\n",
    "\n",
    "print(\"\\nPC3 - Principales contributions :\")\n",
    "top_pc3 = components_df['PC3'].abs().sort_values(ascending=False).head(3)\n",
    "for feat, val in top_pc3.items():\n",
    "    print(f\"  • {feat}: {components_df.loc[feat, 'PC3']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-means optimisé sur les composantes principales\n",
    "\n",
    "### 6.1 Clustering dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les données projetées PCA\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualisation du clustering dans l'espace PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation avec les clusters dans l'espace PCA\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Retour aux features originales\n",
    "\n",
    "Visualisons maintenant ces clusters dans l'espace des features originales que vous avez choisies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des clusters PCA dans l'espace original\n",
    "df_original_clustered = data_num[# les features que vous avez choisies au départ ].copy()\n",
    "df_original_clustered['cluster'] = labels_pca.astype(str)\n",
    "\n",
    "# VOTRE CODE (visualisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Détermination du nombre optimal de clusters\n",
    "\n",
    "### 7.1 Méthode du coude (Elbow method)\n",
    "\n",
    "Testons différents nombres de clusters et analysons l'inertie (somme des distances au carré aux centroïdes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de différents nombres de clusters\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visualisation de la courbe du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de l'inertie\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(K_range, inertias, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xlabel('Nombre de clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertie (somme des distances²)', fontsize=12)\n",
    "plt.title('Méthode du coude pour déterminer le nombre optimal de clusters', fontsize=14, pad=20)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(K_range)\n",
    "\n",
    "# Marquer la position que nous avions choisie arbitrairement\n",
    "plt.axvline(x=k_empirical, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Suggestion arbitraire (k={k_empirical})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse de la courbe du coude :\")\n",
    "print(\"Le 'coude' représente le point où ajouter des clusters supplémentaires\")\n",
    "print(\"n'apporte plus d'amélioration significative.\")\n",
    "print(\"\\nQuestion : Quel nombre de clusters vous semble optimal ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Calcul du taux de décroissance de l'inertie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Clustering final avec le nombre optimal de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir le nombre final de clusters (vous pouvez modifier cette valeur)\n",
    "k_final = # VOTRE CHOIX\n",
    "\n",
    "print(f\"Nombre final de clusters choisi : {k_final}\")\n",
    "print(\"\\nEntraînement du modèle K-means final...\")\n",
    "\n",
    "# K-means final\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
    "labels_final = kmeans_final.fit_predict(data_proj)\n",
    "\n",
    "print(f\"\\nClustering final effectué avec {k_final} clusters\")\n",
    "print(f\"Inertie finale : {kmeans_final.inertia_:.2f}\")\n",
    "print(f\"\\nRépartition finale des morceaux par cluster :\")\n",
    "unique, counts = np.unique(labels_final, return_counts=True)\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster_id} : {count} morceaux ({count/len(labels_final)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Visualisation finale du clustering optimisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation dans l'espace PCA avec le clustering final\n",
    "\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Création de playlists personnalisées\n",
    "\n",
    "### 8.1 Analyse des caractéristiques de chaque cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les labels de cluster au DataFrame original (nettoyé)\n",
    "\n",
    "# VOTRE CODE\n",
    "\n",
    "# Calculer les moyennes des features pour chaque cluster (faire un groupby)\n",
    "# VOTRE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualisation des profils de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de features clés pour la visualisation\n",
    "key_features = ['danceability', 'energy', 'valence', 'acousticness', 'instrumentalness', 'tempo']\n",
    "\n",
    "# Normalisation pour la visualisation en radar\n",
    "cluster_profiles_norm = cluster_profiles[key_features].copy()\n",
    "for col in key_features:\n",
    "    if col == 'tempo':\n",
    "        # Normaliser tempo sur [0, 1]\n",
    "        cluster_profiles_norm[col] = (cluster_profiles_norm[col] - cluster_profiles_norm[col].min()) / \\\n",
    "                                      (cluster_profiles_norm[col].max() - cluster_profiles_norm[col].min())\n",
    "\n",
    "# Heatmap des profils de clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_profiles_norm.T, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Valeur normalisée'}, linewidths=0.5)\n",
    "plt.title(f'Profils des {k_final} clusters musicaux', fontsize=14, pad=20)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Features audio', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterprétation : Chaque cluster a un profil musical distinct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Caractérisation automatique des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour caractériser un cluster\n",
    "def characterize_cluster(cluster_id, profile):\n",
    "    \"\"\"Génère une description textuelle d'un cluster basée sur ses caractéristiques.\"\"\"\n",
    "    characteristics = []\n",
    "    \n",
    "    # Danceability\n",
    "    if profile['danceability'] > 0.7:\n",
    "        characteristics.append(\"très dansant\")\n",
    "    elif profile['danceability'] > 0.5:\n",
    "        characteristics.append(\"dansant\")\n",
    "    else:\n",
    "        characteristics.append(\"peu dansant\")\n",
    "    \n",
    "    # Energy\n",
    "    if profile['energy'] > 0.7:\n",
    "        characteristics.append(\"énergique\")\n",
    "    elif profile['energy'] > 0.5:\n",
    "        characteristics.append(\"modérément énergique\")\n",
    "    else:\n",
    "        characteristics.append(\"calme\")\n",
    "    \n",
    "    # Valence\n",
    "    if profile['valence'] > 0.6:\n",
    "        characteristics.append(\"joyeux\")\n",
    "    elif profile['valence'] > 0.4:\n",
    "        characteristics.append(\"neutre\")\n",
    "    else:\n",
    "        characteristics.append(\"mélancolique\")\n",
    "    \n",
    "    # Acousticness\n",
    "    if profile['acousticness'] > 0.6:\n",
    "        characteristics.append(\"acoustique\")\n",
    "    elif profile['acousticness'] < 0.3:\n",
    "        characteristics.append(\"électronique\")\n",
    "    \n",
    "    # Instrumentalness\n",
    "    if profile['instrumentalness'] > 0.5:\n",
    "        characteristics.append(\"instrumental\")\n",
    "    \n",
    "    # Tempo\n",
    "    if profile['tempo'] > 140:\n",
    "        characteristics.append(\"rapide\")\n",
    "    elif profile['tempo'] < 90:\n",
    "        characteristics.append(\"lent\")\n",
    "    \n",
    "    return \", \".join(characteristics)\n",
    "\n",
    "# Générer les descriptions pour chaque cluster\n",
    "print(\"\\nCaractérisation des clusters musicaux :\\n\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in range(k_final):\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    description = characterize_cluster(cluster_id, profile)\n",
    "    n_songs = (labels_final == cluster_id).sum()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({n_songs} morceaux)\")\n",
    "    print(f\"   Style : {description.capitalize()}\")\n",
    "    print(f\"   Caractéristiques :\")\n",
    "    print(f\"     • Danceability : {profile['danceability']:.2f}\")\n",
    "    print(f\"     • Energy       : {profile['energy']:.2f}\")\n",
    "    print(f\"     • Valence      : {profile['valence']:.2f}\")\n",
    "    print(f\"     • Tempo        : {profile['tempo']:.0f} BPM\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Génération de playlists thématiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer une playlist à partir d'un cluster\n",
    "def create_playlist(cluster_id, n_songs=10, seed=None):\n",
    "    \"\"\"Crée une playlist de n_songs morceaux du cluster spécifié.\"\"\"\n",
    "    cluster_songs = df_with_clusters[df_with_clusters['cluster'] == cluster_id]\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_songs = min(n_songs, len(cluster_songs))\n",
    "    playlist = # VOTRE CODE\n",
    "    \n",
    "    return playlist[['track_name', 'artists', 'danceability', 'energy', 'valence', 'tempo']]\n",
    "\n",
    "# Créer des playlists pour quelques clusters\n",
    "print(\"\\nExemples de playlists générées :\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in range(min(3, k_final)):  # Afficher les 3 premiers clusters\n",
    "    print(f\"\\nPlaylist du Cluster {cluster_id}\")\n",
    "    playlist = create_playlist(cluster_id, n_songs=5, seed=42)\n",
    "    print(playlist.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Système de recommandation basé sur un morceau\n",
    "\n",
    "Créons une fonction qui recommande des morceaux similaires à un morceau donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_songs(track_name, n_recommendations=10):\n",
    "    \"\"\"Recommande des morceaux similaires basés sur le même cluster.\"\"\"\n",
    "    # Trouver le morceau\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Prendre le premier match\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Trouver d'autres morceaux du même cluster\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Calculer la distance dans l'espace PCA et les ordonner pour affiner\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    # Retourner les n recommandations\n",
    "    # VOTRE CODE\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Exemple de recommandations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Choisir un morceau aléatoire pour la démonstration\n",
    "random_track = df['track_name'].sample(1, random_state=42).values[0]\n",
    "recommendations = recommend_similar_songs(random_track, n_recommendations=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Export des playlists\n",
    "\n",
    "Exportons les playlists pour chaque cluster dans des fichiers CSV séparés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un répertoire pour les playlists\n",
    "import os\n",
    "os.makedirs('playlists', exist_ok=True)\n",
    "\n",
    "# Exporter chaque cluster dans un fichier CSV\n",
    "for cluster_id in range(k_final):\n",
    "    cluster_songs = df_with_clusters[df_with_clusters['cluster'] == cluster_id]\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    description = characterize_cluster(cluster_id, profile)\n",
    "    \n",
    "    filename = f'playlists/cluster_{cluster_id}_{description.replace(\",\", \"\").replace(\" \", \"_\")}.csv'\n",
    "    cluster_songs.to_csv(filename, index=False)\n",
    "    print(f\"Playlist du Cluster {cluster_id} exportée : {filename}\")\n",
    "\n",
    "print(f\"\\n{k_final} playlists exportées dans le dossier 'playlists/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion et perspectives\n",
    "\n",
    "Bien sûr nous ne sommes pas au niveau des équipes qui ont conçu le système de recommandation de Spotify, Deezer ou de Youtube. Notre échantillon était réduit et nous nous sommes volontairement limité dans notre clustering etc. mais la logique était là.\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **K-means sur données brutes** : Performance limitée car les features ont des échelles différentes\n",
    "\n",
    "2. **Mise à l'échelle** : Amélioration modeste mais importante pour égaliser l'influence des features\n",
    "\n",
    "3. **PCA + K-means** : Combinaison puissante qui :\n",
    "   - Réduit la dimensionnalité tout en conservant l'information essentielle\n",
    "   - Révèle la structure sous-jacente des données\n",
    "   - Améliore significativement la qualité du clustering\n",
    "\n",
    "4. **Méthode du coude** : Permet de déterminer (plus ou moins) objectivement le nombre optimal de clusters. Notre tentative d’automatiser complètement le processus de sélection via le taux d’accroissement et le calcul de la dérivé seconde ne fonctionnait pas totalement dans ce contexte limité, mais elle est à connaître.\n",
    "\n",
    "### Applications pratiques\n",
    "\n",
    "Ce système de recommandation peut être utilisé pour :\n",
    "- Générer des playlists automatiques cohérentes\n",
    "- Suggérer de nouveaux morceaux similaires aux goûts d'un utilisateur\n",
    "- Organiser une bibliothèque musicale par style/ambiance\n",
    "- Créer des transitions fluides dans des DJ sets\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "1. **Tester d'autres algorithmes** : DBSCAN, Hierarchical Clustering, GMM\n",
    "2. **Incorporer des données supplémentaires** : genre musical, année de sortie, paroles\n",
    "3. **Utiliser des métriques d'évaluation** : Silhouette Score, Davies-Bouldin Index\n",
    "4. **Créer un système hybride** : combiner clustering et collaborative filtering\n",
    "5. **Interface utilisateur** : développer une application web interactive\n",
    "\n",
    "### Exercices complémentaires\n",
    "\n",
    "1. Comparez les résultats avec différents scalers (StandardScaler, MinMaxScaler)\n",
    "2. Testez la PCA avec un nombre différent de composantes\n",
    "3. Analysez les morceaux mal classés (outliers)\n",
    "4. Créez des playlists thématiques (sport, détente, fête, travail)\n",
    "5. Implémentez une fonction de \"découverte\" qui suggère des morceaux de clusters adjacents\n",
    "6. Cherchez une base de données et créez un système de recommandation pour les films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. À vous de jouer !\n",
    "\n",
    "Utilisez les cellules ci-dessous pour expérimenter et créer vos propres playlists personnalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule d'expérimentation libre\n",
    "# Testez vos propres recommandations ici !\n",
    "\n",
    "# Exemple : recommander_similar_songs(\"votre_morceau_préféré\", n_recommendations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez votre propre playlist personnalisée en mélangeant plusieurs clusters\n",
    "# Exemple : combiner des morceaux énergiques et joyeux\n",
    "\n",
    "# custom_playlist = pd.concat([\n",
    "#     create_playlist(cluster_1, n_songs=5),\n",
    "#     create_playlist(cluster_2, n_songs=5)\n",
    "# ])\n",
    "# print(custom_playlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
